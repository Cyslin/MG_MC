-------------------- Hyperparams --------------------
time: 2021-01-04 13:25:57.008490
Dataset: amazon
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 64
use_cuda: True
2021-01-04 13:36:13.008306 Training: [1 epoch,  10 batch] loss: 29.78435, the best RMSE/MAE: inf / inf
2021-01-04 13:37:07.764314 Training: [1 epoch,  20 batch] loss: 27.29558, the best RMSE/MAE: inf / inf
2021-01-04 13:38:02.222330 Training: [1 epoch,  30 batch] loss: 25.39012, the best RMSE/MAE: inf / inf
2021-01-04 13:38:57.556523 Training: [1 epoch,  40 batch] loss: 23.86177, the best RMSE/MAE: inf / inf
2021-01-04 13:39:53.825842 Training: [1 epoch,  50 batch] loss: 22.33384, the best RMSE/MAE: inf / inf
2021-01-04 13:40:49.853177 Training: [1 epoch,  60 batch] loss: 20.87451, the best RMSE/MAE: inf / inf
2021-01-04 13:41:45.086835 Training: [1 epoch,  70 batch] loss: 19.92677, the best RMSE/MAE: inf / inf
2021-01-04 13:42:40.028703 Training: [1 epoch,  80 batch] loss: 18.88731, the best RMSE/MAE: inf / inf
2021-01-04 13:43:35.111640 Training: [1 epoch,  90 batch] loss: 18.17371, the best RMSE/MAE: inf / inf
2021-01-04 13:44:32.005477 Training: [1 epoch, 100 batch] loss: 17.58890, the best RMSE/MAE: inf / inf
2021-01-04 13:45:28.217388 Training: [1 epoch, 110 batch] loss: 17.20288, the best RMSE/MAE: inf / inf
2021-01-04 13:46:23.768996 Training: [1 epoch, 120 batch] loss: 16.83666, the best RMSE/MAE: inf / inf
2021-01-04 13:47:18.904021 Training: [1 epoch, 130 batch] loss: 16.53953, the best RMSE/MAE: inf / inf
2021-01-04 13:48:14.654668 Training: [1 epoch, 140 batch] loss: 16.38633, the best RMSE/MAE: inf / inf
2021-01-04 13:49:11.416207 Training: [1 epoch, 150 batch] loss: 16.27945, the best RMSE/MAE: inf / inf
2021-01-04 13:50:07.892325 Training: [1 epoch, 160 batch] loss: 16.06215, the best RMSE/MAE: inf / inf
2021-01-04 13:51:03.480728 Training: [1 epoch, 170 batch] loss: 16.37121, the best RMSE/MAE: inf / inf
2021-01-04 13:51:58.665136 Training: [1 epoch, 180 batch] loss: 16.08659, the best RMSE/MAE: inf / inf
2021-01-04 13:52:54.071347 Training: [1 epoch, 190 batch] loss: 16.05832, the best RMSE/MAE: inf / inf
2021-01-04 13:53:50.524508 Training: [1 epoch, 200 batch] loss: 16.03991, the best RMSE/MAE: inf / inf
<Test> RMSE：789158.87500,MAE：689114.00000
2021-01-04 13:57:05.361994 Training: [2 epoch,  10 batch] loss: 15.99656, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 13:58:00.903579 Training: [2 epoch,  20 batch] loss: 15.89637, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 13:58:57.342081 Training: [2 epoch,  30 batch] loss: 15.91141, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 13:59:54.406143 Training: [2 epoch,  40 batch] loss: 15.93175, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:00:51.151981 Training: [2 epoch,  50 batch] loss: 15.81402, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:01:47.607026 Training: [2 epoch,  60 batch] loss: 15.80367, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:02:43.897460 Training: [2 epoch,  70 batch] loss: 15.78532, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:03:41.080042 Training: [2 epoch,  80 batch] loss: 15.80212, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:04:38.109438 Training: [2 epoch,  90 batch] loss: 15.76520, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:05:34.603768 Training: [2 epoch, 100 batch] loss: 15.74799, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:06:31.076881 Training: [2 epoch, 110 batch] loss: 15.72312, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:07:27.148586 Training: [2 epoch, 120 batch] loss: 15.75173, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:08:22.869472 Training: [2 epoch, 130 batch] loss: 15.59410, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:09:18.291772 Training: [2 epoch, 140 batch] loss: 15.56795, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:10:13.845979 Training: [2 epoch, 150 batch] loss: 15.49416, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:11:09.880239 Training: [2 epoch, 160 batch] loss: 15.45111, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:12:06.239500 Training: [2 epoch, 170 batch] loss: 15.55383, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:13:02.901735 Training: [2 epoch, 180 batch] loss: 15.43015, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:13:59.047847 Training: [2 epoch, 190 batch] loss: 15.50269, the best RMSE/MAE: 789158.87500 / 689114.00000
2021-01-04 14:14:54.688090 Training: [2 epoch, 200 batch] loss: 15.41971, the best RMSE/MAE: 789158.87500 / 689114.00000
<Test> RMSE：2545.41187,MAE：2338.60693
2021-01-04 14:18:09.708833 Training: [3 epoch,  10 batch] loss: 15.37299, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:19:05.931471 Training: [3 epoch,  20 batch] loss: 15.32497, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:20:02.653305 Training: [3 epoch,  30 batch] loss: 15.30221, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:20:59.299760 Training: [3 epoch,  40 batch] loss: 15.27982, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:21:55.613019 Training: [3 epoch,  50 batch] loss: 15.24796, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:22:51.545500 Training: [3 epoch,  60 batch] loss: 15.23936, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:23:47.921097 Training: [3 epoch,  70 batch] loss: 15.22230, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:24:44.922388 Training: [3 epoch,  80 batch] loss: 15.17439, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:25:41.758560 Training: [3 epoch,  90 batch] loss: 15.08995, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:26:38.358558 Training: [3 epoch, 100 batch] loss: 15.09653, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:27:34.514088 Training: [3 epoch, 110 batch] loss: 15.03974, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:28:31.240630 Training: [3 epoch, 120 batch] loss: 15.11258, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:29:28.507281 Training: [3 epoch, 130 batch] loss: 14.95451, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:30:25.514121 Training: [3 epoch, 140 batch] loss: 15.01584, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:31:21.913374 Training: [3 epoch, 150 batch] loss: 14.91903, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:32:19.029614 Training: [3 epoch, 160 batch] loss: 14.94368, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:33:16.077889 Training: [3 epoch, 170 batch] loss: 14.88957, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:34:12.704026 Training: [3 epoch, 180 batch] loss: 14.91657, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:35:09.210437 Training: [3 epoch, 190 batch] loss: 14.81121, the best RMSE/MAE: 2545.41187 / 2338.60693
2021-01-04 14:36:05.559579 Training: [3 epoch, 200 batch] loss: 14.88102, the best RMSE/MAE: 2545.41187 / 2338.60693
<Test> RMSE：97.26270,MAE：88.73016
2021-01-04 14:39:22.353330 Training: [4 epoch,  10 batch] loss: 14.78543, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:40:18.989250 Training: [4 epoch,  20 batch] loss: 14.75401, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:41:15.161844 Training: [4 epoch,  30 batch] loss: 14.75618, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:42:11.637479 Training: [4 epoch,  40 batch] loss: 14.64451, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:43:08.894055 Training: [4 epoch,  50 batch] loss: 14.60208, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:44:05.189173 Training: [4 epoch,  60 batch] loss: 14.60276, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:45:02.488922 Training: [4 epoch,  70 batch] loss: 14.54611, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:45:59.392950 Training: [4 epoch,  80 batch] loss: 14.56352, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:46:56.020226 Training: [4 epoch,  90 batch] loss: 14.52276, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:47:53.411836 Training: [4 epoch, 100 batch] loss: 14.51299, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:48:50.143410 Training: [4 epoch, 110 batch] loss: 14.47768, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:49:47.113299 Training: [4 epoch, 120 batch] loss: 14.50016, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:50:44.319707 Training: [4 epoch, 130 batch] loss: 14.46705, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:51:41.016780 Training: [4 epoch, 140 batch] loss: 14.34850, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:52:38.321794 Training: [4 epoch, 150 batch] loss: 14.34647, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:53:35.426425 Training: [4 epoch, 160 batch] loss: 14.33953, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:54:32.171356 Training: [4 epoch, 170 batch] loss: 14.30235, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:55:29.641247 Training: [4 epoch, 180 batch] loss: 14.24150, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:56:25.848902 Training: [4 epoch, 190 batch] loss: 14.29803, the best RMSE/MAE: 97.26270 / 88.73016
2021-01-04 14:57:21.952915 Training: [4 epoch, 200 batch] loss: 14.28231, the best RMSE/MAE: 97.26270 / 88.73016
<Test> RMSE：13.26414,MAE：12.05194
2021-01-04 15:00:38.670874 Training: [5 epoch,  10 batch] loss: 14.16766, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:01:34.802767 Training: [5 epoch,  20 batch] loss: 14.14424, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:02:30.729294 Training: [5 epoch,  30 batch] loss: 14.08962, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:03:26.607788 Training: [5 epoch,  40 batch] loss: 14.03716, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:04:23.203799 Training: [5 epoch,  50 batch] loss: 14.02811, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:05:19.649187 Training: [5 epoch,  60 batch] loss: 14.01574, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:06:15.776223 Training: [5 epoch,  70 batch] loss: 14.03772, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:07:12.496052 Training: [5 epoch,  80 batch] loss: 13.95911, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:08:09.251103 Training: [5 epoch,  90 batch] loss: 13.92318, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:09:05.795929 Training: [5 epoch, 100 batch] loss: 13.87235, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:10:02.766506 Training: [5 epoch, 110 batch] loss: 13.93004, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:10:59.532392 Training: [5 epoch, 120 batch] loss: 13.92879, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:11:55.632278 Training: [5 epoch, 130 batch] loss: 13.77227, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:12:52.525916 Training: [5 epoch, 140 batch] loss: 13.82184, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:13:49.284730 Training: [5 epoch, 150 batch] loss: 13.81988, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:14:45.653388 Training: [5 epoch, 160 batch] loss: 13.71307, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:15:42.197068 Training: [5 epoch, 170 batch] loss: 13.69710, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:16:38.952888 Training: [5 epoch, 180 batch] loss: 13.60868, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:17:35.185800 Training: [5 epoch, 190 batch] loss: 13.65474, the best RMSE/MAE: 13.26414 / 12.05194
2021-01-04 15:18:31.594927 Training: [5 epoch, 200 batch] loss: 13.66006, the best RMSE/MAE: 13.26414 / 12.05194
<Test> RMSE：3.73613,MAE：3.20296
2021-01-04 15:21:51.489103 Training: [6 epoch,  10 batch] loss: 13.53333, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:22:53.405050 Training: [6 epoch,  20 batch] loss: 13.49260, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:23:55.497127 Training: [6 epoch,  30 batch] loss: 13.43958, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:24:57.706268 Training: [6 epoch,  40 batch] loss: 13.45629, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:26:00.227994 Training: [6 epoch,  50 batch] loss: 13.44134, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:27:02.911874 Training: [6 epoch,  60 batch] loss: 13.34273, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:28:02.763028 Training: [6 epoch,  70 batch] loss: 13.39286, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:29:02.604138 Training: [6 epoch,  80 batch] loss: 13.21296, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:30:02.105846 Training: [6 epoch,  90 batch] loss: 13.40888, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:31:01.735198 Training: [6 epoch, 100 batch] loss: 13.17404, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:32:01.684274 Training: [6 epoch, 110 batch] loss: 13.22587, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:33:00.951914 Training: [6 epoch, 120 batch] loss: 13.18269, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:34:00.846438 Training: [6 epoch, 130 batch] loss: 13.12438, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:35:00.972528 Training: [6 epoch, 140 batch] loss: 13.10155, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:36:00.338588 Training: [6 epoch, 150 batch] loss: 13.11816, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:37:00.127684 Training: [6 epoch, 160 batch] loss: 13.13273, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:37:58.738909 Training: [6 epoch, 170 batch] loss: 13.07441, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:38:56.742651 Training: [6 epoch, 180 batch] loss: 12.99843, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:39:55.979151 Training: [6 epoch, 190 batch] loss: 12.96163, the best RMSE/MAE: 3.73613 / 3.20296
2021-01-04 15:40:56.171245 Training: [6 epoch, 200 batch] loss: 12.93871, the best RMSE/MAE: 3.73613 / 3.20296
<Test> RMSE：1.62777,MAE：1.32257
2021-01-04 15:44:16.184458 Training: [7 epoch,  10 batch] loss: 12.81310, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:45:13.310463 Training: [7 epoch,  20 batch] loss: 12.77642, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:46:11.380295 Training: [7 epoch,  30 batch] loss: 12.79008, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:47:09.078915 Training: [7 epoch,  40 batch] loss: 12.78997, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:48:08.516666 Training: [7 epoch,  50 batch] loss: 12.73677, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:49:08.530896 Training: [7 epoch,  60 batch] loss: 12.64050, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:50:08.057892 Training: [7 epoch,  70 batch] loss: 12.61965, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:51:07.835190 Training: [7 epoch,  80 batch] loss: 12.63122, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:52:07.690320 Training: [7 epoch,  90 batch] loss: 12.56570, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:53:07.118358 Training: [7 epoch, 100 batch] loss: 12.54665, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:54:07.418316 Training: [7 epoch, 110 batch] loss: 12.44410, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:55:06.831030 Training: [7 epoch, 120 batch] loss: 12.43919, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:56:03.937527 Training: [7 epoch, 130 batch] loss: 12.44387, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:57:00.288600 Training: [7 epoch, 140 batch] loss: 12.36761, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:57:55.765885 Training: [7 epoch, 150 batch] loss: 12.35356, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:58:51.173181 Training: [7 epoch, 160 batch] loss: 12.31865, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 15:59:46.330932 Training: [7 epoch, 170 batch] loss: 12.28972, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 16:00:42.191457 Training: [7 epoch, 180 batch] loss: 12.27732, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 16:01:38.711893 Training: [7 epoch, 190 batch] loss: 12.17097, the best RMSE/MAE: 1.62777 / 1.32257
2021-01-04 16:02:34.794342 Training: [7 epoch, 200 batch] loss: 12.25135, the best RMSE/MAE: 1.62777 / 1.32257
<Test> RMSE：1.13510,MAE：0.86547
2021-01-04 16:05:54.643857 Training: [8 epoch,  10 batch] loss: 12.15842, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:06:49.499913 Training: [8 epoch,  20 batch] loss: 12.10167, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:07:45.263345 Training: [8 epoch,  30 batch] loss: 12.09353, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:08:40.859904 Training: [8 epoch,  40 batch] loss: 12.00622, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:09:35.664302 Training: [8 epoch,  50 batch] loss: 11.94270, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:10:30.247334 Training: [8 epoch,  60 batch] loss: 12.00132, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:11:25.226848 Training: [8 epoch,  70 batch] loss: 11.94953, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:12:21.805158 Training: [8 epoch,  80 batch] loss: 11.86311, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:13:18.350235 Training: [8 epoch,  90 batch] loss: 11.85127, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:14:15.504477 Training: [8 epoch, 100 batch] loss: 11.85756, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:15:12.369363 Training: [8 epoch, 110 batch] loss: 11.72731, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:16:08.986177 Training: [8 epoch, 120 batch] loss: 11.69341, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:17:05.073530 Training: [8 epoch, 130 batch] loss: 11.64750, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:18:00.705319 Training: [8 epoch, 140 batch] loss: 11.66388, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:18:55.980915 Training: [8 epoch, 150 batch] loss: 11.70064, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:19:51.353605 Training: [8 epoch, 160 batch] loss: 11.57546, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:20:46.430204 Training: [8 epoch, 170 batch] loss: 11.56415, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:21:42.576407 Training: [8 epoch, 180 batch] loss: 11.50525, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:22:39.044724 Training: [8 epoch, 190 batch] loss: 11.45585, the best RMSE/MAE: 1.13510 / 0.86547
2021-01-04 16:23:34.988252 Training: [8 epoch, 200 batch] loss: 11.49659, the best RMSE/MAE: 1.13510 / 0.86547
<Test> RMSE：0.97857,MAE：0.71286
2021-01-04 16:27:10.091224 Training: [9 epoch,  10 batch] loss: 11.38430, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:28:06.704671 Training: [9 epoch,  20 batch] loss: 11.33731, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:29:02.889566 Training: [9 epoch,  30 batch] loss: 11.29290, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:29:59.099837 Training: [9 epoch,  40 batch] loss: 11.28388, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:30:56.043111 Training: [9 epoch,  50 batch] loss: 11.26634, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:31:55.215120 Training: [9 epoch,  60 batch] loss: 11.15861, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:32:54.467368 Training: [9 epoch,  70 batch] loss: 11.16866, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:33:52.638237 Training: [9 epoch,  80 batch] loss: 11.00587, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:34:50.838914 Training: [9 epoch,  90 batch] loss: 11.09578, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:35:48.587458 Training: [9 epoch, 100 batch] loss: 11.01343, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:36:47.602445 Training: [9 epoch, 110 batch] loss: 10.91449, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:37:47.302317 Training: [9 epoch, 120 batch] loss: 10.97065, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:38:46.539822 Training: [9 epoch, 130 batch] loss: 10.94070, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:39:45.508051 Training: [9 epoch, 140 batch] loss: 10.93137, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:40:45.016125 Training: [9 epoch, 150 batch] loss: 10.77212, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:41:45.132819 Training: [9 epoch, 160 batch] loss: 10.84202, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:42:45.876254 Training: [9 epoch, 170 batch] loss: 10.77654, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:43:46.740983 Training: [9 epoch, 180 batch] loss: 10.75110, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:44:47.264600 Training: [9 epoch, 190 batch] loss: 10.69538, the best RMSE/MAE: 0.97857 / 0.71286
2021-01-04 16:45:47.767747 Training: [9 epoch, 200 batch] loss: 10.71257, the best RMSE/MAE: 0.97857 / 0.71286
<Test> RMSE：0.94359,MAE：0.68318
2021-01-04 16:49:28.149701 Training: [10 epoch,  10 batch] loss: 10.61662, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:50:25.681084 Training: [10 epoch,  20 batch] loss: 10.56302, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:51:23.103829 Training: [10 epoch,  30 batch] loss: 10.57732, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:52:20.092908 Training: [10 epoch,  40 batch] loss: 10.44184, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:53:18.249058 Training: [10 epoch,  50 batch] loss: 10.44638, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:54:15.252923 Training: [10 epoch,  60 batch] loss: 10.40787, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:55:11.921775 Training: [10 epoch,  70 batch] loss: 10.38740, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:56:08.180916 Training: [10 epoch,  80 batch] loss: 10.34656, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:57:04.043682 Training: [10 epoch,  90 batch] loss: 10.30129, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:58:00.633599 Training: [10 epoch, 100 batch] loss: 10.22743, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:58:57.986782 Training: [10 epoch, 110 batch] loss: 10.19486, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 16:59:55.173930 Training: [10 epoch, 120 batch] loss: 10.14308, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:00:53.338687 Training: [10 epoch, 130 batch] loss: 10.17874, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:01:50.098060 Training: [10 epoch, 140 batch] loss: 10.15719, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:02:47.208688 Training: [10 epoch, 150 batch] loss: 10.10094, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:03:44.036884 Training: [10 epoch, 160 batch] loss: 10.08489, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:04:40.245237 Training: [10 epoch, 170 batch] loss: 10.00808, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:05:37.360466 Training: [10 epoch, 180 batch] loss: 9.95504, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:06:34.575100 Training: [10 epoch, 190 batch] loss: 9.90081, the best RMSE/MAE: 0.94359 / 0.68318
2021-01-04 17:07:31.092544 Training: [10 epoch, 200 batch] loss: 9.95825, the best RMSE/MAE: 0.94359 / 0.68318
<Test> RMSE：0.94347,MAE：0.69777
2021-01-04 17:11:00.862639 Training: [11 epoch,  10 batch] loss: 9.87160, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:11:54.716450 Training: [11 epoch,  20 batch] loss: 9.80855, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:12:52.787806 Training: [11 epoch,  30 batch] loss: 9.72756, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:13:54.542950 Training: [11 epoch,  40 batch] loss: 9.71527, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:14:55.995892 Training: [11 epoch,  50 batch] loss: 9.67747, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:15:53.571932 Training: [11 epoch,  60 batch] loss: 9.61786, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:16:49.983528 Training: [11 epoch,  70 batch] loss: 9.60389, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:17:46.541984 Training: [11 epoch,  80 batch] loss: 9.63128, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:18:43.129824 Training: [11 epoch,  90 batch] loss: 9.58247, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:19:39.691641 Training: [11 epoch, 100 batch] loss: 9.48701, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:20:36.207781 Training: [11 epoch, 110 batch] loss: 9.43873, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:21:31.631626 Training: [11 epoch, 120 batch] loss: 9.45156, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:22:27.481023 Training: [11 epoch, 130 batch] loss: 9.44442, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:23:23.930737 Training: [11 epoch, 140 batch] loss: 9.38544, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:24:19.983266 Training: [11 epoch, 150 batch] loss: 9.26507, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:25:14.760033 Training: [11 epoch, 160 batch] loss: 9.23773, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:26:10.398942 Training: [11 epoch, 170 batch] loss: 9.28143, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:27:06.656278 Training: [11 epoch, 180 batch] loss: 9.16167, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:28:03.353167 Training: [11 epoch, 190 batch] loss: 9.22302, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:29:00.167399 Training: [11 epoch, 200 batch] loss: 9.13042, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.94400,MAE：0.70697
2021-01-04 17:32:22.720151 Training: [12 epoch,  10 batch] loss: 9.09113, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:33:21.502038 Training: [12 epoch,  20 batch] loss: 9.01928, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:34:16.316954 Training: [12 epoch,  30 batch] loss: 9.00630, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:35:11.192760 Training: [12 epoch,  40 batch] loss: 8.97173, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:36:05.983345 Training: [12 epoch,  50 batch] loss: 8.86559, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:37:00.863815 Training: [12 epoch,  60 batch] loss: 8.95572, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:37:55.303202 Training: [12 epoch,  70 batch] loss: 8.85130, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:38:50.135296 Training: [12 epoch,  80 batch] loss: 8.87197, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:39:46.098831 Training: [12 epoch,  90 batch] loss: 8.84787, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:40:42.416090 Training: [12 epoch, 100 batch] loss: 8.74483, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:41:38.063473 Training: [12 epoch, 110 batch] loss: 8.68995, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:42:33.235942 Training: [12 epoch, 120 batch] loss: 8.68838, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:43:28.271339 Training: [12 epoch, 130 batch] loss: 8.62330, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:44:23.216867 Training: [12 epoch, 140 batch] loss: 8.57515, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:45:18.744181 Training: [12 epoch, 150 batch] loss: 8.59466, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:46:15.065264 Training: [12 epoch, 160 batch] loss: 8.56164, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:47:10.755436 Training: [12 epoch, 170 batch] loss: 8.49341, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:48:05.687874 Training: [12 epoch, 180 batch] loss: 8.49533, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:49:01.203389 Training: [12 epoch, 190 batch] loss: 8.42033, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:49:56.999487 Training: [12 epoch, 200 batch] loss: 8.35295, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.96265,MAE：0.72650
2021-01-04 17:53:15.332023 Training: [13 epoch,  10 batch] loss: 8.34865, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:54:11.195081 Training: [13 epoch,  20 batch] loss: 8.35589, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:55:06.787146 Training: [13 epoch,  30 batch] loss: 8.28656, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:56:02.934551 Training: [13 epoch,  40 batch] loss: 8.25669, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:56:59.404880 Training: [13 epoch,  50 batch] loss: 8.19520, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:57:55.999677 Training: [13 epoch,  60 batch] loss: 8.20819, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:58:52.436663 Training: [13 epoch,  70 batch] loss: 8.19774, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 17:59:48.910984 Training: [13 epoch,  80 batch] loss: 8.04852, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:00:45.239495 Training: [13 epoch,  90 batch] loss: 8.05318, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:01:41.660222 Training: [13 epoch, 100 batch] loss: 8.08364, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:02:37.652283 Training: [13 epoch, 110 batch] loss: 8.02273, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:03:32.867127 Training: [13 epoch, 120 batch] loss: 8.01425, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:04:28.277010 Training: [13 epoch, 130 batch] loss: 8.02559, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:05:23.717755 Training: [13 epoch, 140 batch] loss: 7.93331, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:06:18.604104 Training: [13 epoch, 150 batch] loss: 7.91094, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:07:13.885270 Training: [13 epoch, 160 batch] loss: 7.84192, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:08:09.972640 Training: [13 epoch, 170 batch] loss: 7.82287, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:09:06.720680 Training: [13 epoch, 180 batch] loss: 7.85367, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:10:03.367200 Training: [13 epoch, 190 batch] loss: 7.76393, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:10:59.222882 Training: [13 epoch, 200 batch] loss: 7.74645, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.95849,MAE：0.75681
2021-01-04 18:14:29.023316 Training: [14 epoch,  10 batch] loss: 7.66298, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:15:24.873299 Training: [14 epoch,  20 batch] loss: 7.65710, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:16:21.028814 Training: [14 epoch,  30 batch] loss: 7.62038, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:17:17.116813 Training: [14 epoch,  40 batch] loss: 7.57778, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:18:12.982644 Training: [14 epoch,  50 batch] loss: 7.57206, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:19:09.053135 Training: [14 epoch,  60 batch] loss: 7.52851, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:20:05.475386 Training: [14 epoch,  70 batch] loss: 7.46221, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:21:02.088578 Training: [14 epoch,  80 batch] loss: 7.46223, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:21:58.335030 Training: [14 epoch,  90 batch] loss: 7.44732, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:22:54.263289 Training: [14 epoch, 100 batch] loss: 7.42226, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:23:49.266890 Training: [14 epoch, 110 batch] loss: 7.38844, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:24:44.089001 Training: [14 epoch, 120 batch] loss: 7.32074, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:25:39.220260 Training: [14 epoch, 130 batch] loss: 7.27723, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:26:34.796006 Training: [14 epoch, 140 batch] loss: 7.28309, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:27:29.676610 Training: [14 epoch, 150 batch] loss: 7.24936, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:28:24.649818 Training: [14 epoch, 160 batch] loss: 7.19917, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:29:19.408208 Training: [14 epoch, 170 batch] loss: 7.12408, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:30:14.099777 Training: [14 epoch, 180 batch] loss: 7.13295, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:31:08.671303 Training: [14 epoch, 190 batch] loss: 7.18754, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:32:03.586757 Training: [14 epoch, 200 batch] loss: 7.14547, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.98097,MAE：0.77939
2021-01-04 18:35:20.872086 Training: [15 epoch,  10 batch] loss: 7.07250, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:36:14.987667 Training: [15 epoch,  20 batch] loss: 7.00256, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:37:09.312827 Training: [15 epoch,  30 batch] loss: 6.97525, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:38:03.770442 Training: [15 epoch,  40 batch] loss: 7.00336, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:38:58.577498 Training: [15 epoch,  50 batch] loss: 6.94152, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:39:53.347124 Training: [15 epoch,  60 batch] loss: 6.93941, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:40:48.341454 Training: [15 epoch,  70 batch] loss: 6.95592, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:41:43.671166 Training: [15 epoch,  80 batch] loss: 6.83973, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:42:38.972828 Training: [15 epoch,  90 batch] loss: 6.78690, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:43:34.755845 Training: [15 epoch, 100 batch] loss: 6.83699, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:44:29.750837 Training: [15 epoch, 110 batch] loss: 6.75695, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:45:24.414836 Training: [15 epoch, 120 batch] loss: 6.74102, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:46:19.707556 Training: [15 epoch, 130 batch] loss: 6.73976, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:47:15.211278 Training: [15 epoch, 140 batch] loss: 6.73166, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:48:09.787835 Training: [15 epoch, 150 batch] loss: 6.66417, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:49:05.596969 Training: [15 epoch, 160 batch] loss: 6.67444, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:50:02.159781 Training: [15 epoch, 170 batch] loss: 6.58008, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:50:58.022432 Training: [15 epoch, 180 batch] loss: 6.54276, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:51:54.579560 Training: [15 epoch, 190 batch] loss: 6.52289, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:52:50.911744 Training: [15 epoch, 200 batch] loss: 6.49742, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.96967,MAE：0.77663
2021-01-04 18:56:20.393356 Training: [16 epoch,  10 batch] loss: 6.42862, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:57:15.971690 Training: [16 epoch,  20 batch] loss: 6.46659, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:58:12.263657 Training: [16 epoch,  30 batch] loss: 6.34856, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 18:59:08.492559 Training: [16 epoch,  40 batch] loss: 6.41509, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:00:04.536606 Training: [16 epoch,  50 batch] loss: 6.38255, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:01:00.645195 Training: [16 epoch,  60 batch] loss: 6.34262, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:01:57.095134 Training: [16 epoch,  70 batch] loss: 6.32238, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:02:53.793495 Training: [16 epoch,  80 batch] loss: 6.30628, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:03:50.373039 Training: [16 epoch,  90 batch] loss: 6.21511, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:04:46.195437 Training: [16 epoch, 100 batch] loss: 6.28914, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:05:42.032886 Training: [16 epoch, 110 batch] loss: 6.21250, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:06:37.620790 Training: [16 epoch, 120 batch] loss: 6.22365, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:07:34.331666 Training: [16 epoch, 130 batch] loss: 6.11298, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:08:30.632784 Training: [16 epoch, 140 batch] loss: 6.16890, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:09:27.146209 Training: [16 epoch, 150 batch] loss: 6.22663, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:10:23.672325 Training: [16 epoch, 160 batch] loss: 6.02571, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:11:19.561274 Training: [16 epoch, 170 batch] loss: 6.04681, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:12:16.315504 Training: [16 epoch, 180 batch] loss: 6.02007, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:13:13.210742 Training: [16 epoch, 190 batch] loss: 6.00319, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:14:09.605170 Training: [16 epoch, 200 batch] loss: 5.99560, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：0.98099,MAE：0.77342
2021-01-04 19:17:27.656720 Training: [17 epoch,  10 batch] loss: 5.95292, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:18:22.988984 Training: [17 epoch,  20 batch] loss: 5.89379, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:19:18.444216 Training: [17 epoch,  30 batch] loss: 5.95075, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:20:14.165007 Training: [17 epoch,  40 batch] loss: 5.90765, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:21:09.736976 Training: [17 epoch,  50 batch] loss: 5.89265, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:22:05.528792 Training: [17 epoch,  60 batch] loss: 5.83149, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:23:01.972268 Training: [17 epoch,  70 batch] loss: 5.82695, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:23:58.564139 Training: [17 epoch,  80 batch] loss: 5.76317, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:24:54.767492 Training: [17 epoch,  90 batch] loss: 5.70427, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:25:50.804734 Training: [17 epoch, 100 batch] loss: 5.79766, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:26:46.356851 Training: [17 epoch, 110 batch] loss: 5.79922, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:27:42.697150 Training: [17 epoch, 120 batch] loss: 5.69576, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:28:39.039485 Training: [17 epoch, 130 batch] loss: 5.68572, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:29:35.783723 Training: [17 epoch, 140 batch] loss: 5.62405, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:30:31.939038 Training: [17 epoch, 150 batch] loss: 5.63269, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:31:28.164420 Training: [17 epoch, 160 batch] loss: 5.56546, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:32:25.103834 Training: [17 epoch, 170 batch] loss: 5.59248, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:33:21.384869 Training: [17 epoch, 180 batch] loss: 5.60875, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:34:17.839817 Training: [17 epoch, 190 batch] loss: 5.54665, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:35:14.828362 Training: [17 epoch, 200 batch] loss: 5.53269, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.01241,MAE：0.83690
2021-01-04 19:38:45.124413 Training: [18 epoch,  10 batch] loss: 5.53969, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:39:41.655297 Training: [18 epoch,  20 batch] loss: 5.43599, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:40:38.027754 Training: [18 epoch,  30 batch] loss: 5.39566, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:41:34.710844 Training: [18 epoch,  40 batch] loss: 5.33470, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:42:31.513635 Training: [18 epoch,  50 batch] loss: 5.44915, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:43:28.440501 Training: [18 epoch,  60 batch] loss: 5.33395, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:44:25.621383 Training: [18 epoch,  70 batch] loss: 5.34593, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:45:22.048596 Training: [18 epoch,  80 batch] loss: 5.33160, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:46:17.577642 Training: [18 epoch,  90 batch] loss: 5.28264, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:47:14.955393 Training: [18 epoch, 100 batch] loss: 5.29739, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:48:12.030747 Training: [18 epoch, 110 batch] loss: 5.32730, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:49:09.137565 Training: [18 epoch, 120 batch] loss: 5.26700, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:50:06.877214 Training: [18 epoch, 130 batch] loss: 5.22767, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:51:02.965814 Training: [18 epoch, 140 batch] loss: 5.20230, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:51:59.643481 Training: [18 epoch, 150 batch] loss: 5.14868, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:52:56.745098 Training: [18 epoch, 160 batch] loss: 5.15878, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:53:54.347553 Training: [18 epoch, 170 batch] loss: 5.10462, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:54:50.883607 Training: [18 epoch, 180 batch] loss: 5.15112, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:55:47.763667 Training: [18 epoch, 190 batch] loss: 5.16039, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 19:56:44.159783 Training: [18 epoch, 200 batch] loss: 5.07698, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.04886,MAE：0.87970
2021-01-04 20:00:13.823063 Training: [19 epoch,  10 batch] loss: 5.10029, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:01:09.184275 Training: [19 epoch,  20 batch] loss: 5.06131, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:02:04.757375 Training: [19 epoch,  30 batch] loss: 5.02028, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:03:01.924075 Training: [19 epoch,  40 batch] loss: 5.01964, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:03:58.683812 Training: [19 epoch,  50 batch] loss: 4.97413, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:04:55.508990 Training: [19 epoch,  60 batch] loss: 4.95492, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:05:52.347584 Training: [19 epoch,  70 batch] loss: 5.03765, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:06:48.695700 Training: [19 epoch,  80 batch] loss: 4.87334, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:07:44.664036 Training: [19 epoch,  90 batch] loss: 4.91022, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:08:41.431275 Training: [19 epoch, 100 batch] loss: 4.85214, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:09:41.241359 Training: [19 epoch, 110 batch] loss: 4.83951, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:10:39.585117 Training: [19 epoch, 120 batch] loss: 4.82319, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:11:35.845087 Training: [19 epoch, 130 batch] loss: 4.85781, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:12:34.175429 Training: [19 epoch, 140 batch] loss: 4.84707, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:13:36.106500 Training: [19 epoch, 150 batch] loss: 4.78229, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:14:47.475864 Training: [19 epoch, 160 batch] loss: 4.72030, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:16:01.489951 Training: [19 epoch, 170 batch] loss: 4.81183, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:17:10.879883 Training: [19 epoch, 180 batch] loss: 4.75292, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:18:19.835077 Training: [19 epoch, 190 batch] loss: 4.71152, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:19:34.898910 Training: [19 epoch, 200 batch] loss: 4.64895, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.02531,MAE：0.84774
2021-01-04 20:23:27.777071 Training: [20 epoch,  10 batch] loss: 4.63437, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:24:34.907208 Training: [20 epoch,  20 batch] loss: 4.66974, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:25:42.960679 Training: [20 epoch,  30 batch] loss: 4.67220, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:26:53.970669 Training: [20 epoch,  40 batch] loss: 4.61489, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:28:00.370613 Training: [20 epoch,  50 batch] loss: 4.57861, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:29:12.707770 Training: [20 epoch,  60 batch] loss: 4.57678, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:30:21.702424 Training: [20 epoch,  70 batch] loss: 4.60046, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:31:28.032522 Training: [20 epoch,  80 batch] loss: 4.58554, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:32:38.084941 Training: [20 epoch,  90 batch] loss: 4.49514, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:33:49.930991 Training: [20 epoch, 100 batch] loss: 4.55523, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:34:59.648820 Training: [20 epoch, 110 batch] loss: 4.54516, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:36:08.236752 Training: [20 epoch, 120 batch] loss: 4.48548, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:37:19.291624 Training: [20 epoch, 130 batch] loss: 4.44535, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:38:26.952901 Training: [20 epoch, 140 batch] loss: 4.43022, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:39:37.462486 Training: [20 epoch, 150 batch] loss: 4.41139, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:40:46.375331 Training: [20 epoch, 160 batch] loss: 4.38820, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:41:52.212017 Training: [20 epoch, 170 batch] loss: 4.46223, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:42:58.805417 Training: [20 epoch, 180 batch] loss: 4.35084, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:44:06.603448 Training: [20 epoch, 190 batch] loss: 4.38970, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:45:14.722581 Training: [20 epoch, 200 batch] loss: 4.36067, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.04014,MAE：0.86216
2021-01-04 20:49:00.824241 Training: [21 epoch,  10 batch] loss: 4.32317, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:50:03.892976 Training: [21 epoch,  20 batch] loss: 4.29535, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:51:07.275251 Training: [21 epoch,  30 batch] loss: 4.26523, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:52:10.816469 Training: [21 epoch,  40 batch] loss: 4.24325, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:53:14.007964 Training: [21 epoch,  50 batch] loss: 4.28972, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:54:17.261197 Training: [21 epoch,  60 batch] loss: 4.24969, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:55:20.705566 Training: [21 epoch,  70 batch] loss: 4.28395, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:56:24.364832 Training: [21 epoch,  80 batch] loss: 4.18943, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:57:27.828957 Training: [21 epoch,  90 batch] loss: 4.22339, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:58:31.340371 Training: [21 epoch, 100 batch] loss: 4.20944, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 20:59:35.243782 Training: [21 epoch, 110 batch] loss: 4.16752, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:00:39.200789 Training: [21 epoch, 120 batch] loss: 4.18683, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:01:43.197285 Training: [21 epoch, 130 batch] loss: 4.14388, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:02:47.227725 Training: [21 epoch, 140 batch] loss: 4.12168, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:03:51.301670 Training: [21 epoch, 150 batch] loss: 4.18248, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:04:54.727178 Training: [21 epoch, 160 batch] loss: 4.11321, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:05:57.807569 Training: [21 epoch, 170 batch] loss: 4.05730, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:07:00.945350 Training: [21 epoch, 180 batch] loss: 4.07653, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:08:04.875535 Training: [21 epoch, 190 batch] loss: 4.04500, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:09:08.845221 Training: [21 epoch, 200 batch] loss: 4.03526, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.01549,MAE：0.82136
2021-01-04 21:12:48.991361 Training: [22 epoch,  10 batch] loss: 4.04748, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:13:51.621753 Training: [22 epoch,  20 batch] loss: 4.01380, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:14:54.364193 Training: [22 epoch,  30 batch] loss: 3.97916, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:15:56.712388 Training: [22 epoch,  40 batch] loss: 4.03046, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:16:59.865979 Training: [22 epoch,  50 batch] loss: 4.00910, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:18:03.809437 Training: [22 epoch,  60 batch] loss: 3.94449, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:19:07.857251 Training: [22 epoch,  70 batch] loss: 3.95797, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:20:12.348250 Training: [22 epoch,  80 batch] loss: 3.95600, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:21:16.816451 Training: [22 epoch,  90 batch] loss: 3.94242, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:22:21.204499 Training: [22 epoch, 100 batch] loss: 3.88987, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:23:25.648154 Training: [22 epoch, 110 batch] loss: 3.86486, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:24:30.155355 Training: [22 epoch, 120 batch] loss: 3.84439, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:25:34.943055 Training: [22 epoch, 130 batch] loss: 3.88902, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:26:46.124628 Training: [22 epoch, 140 batch] loss: 3.85867, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:27:56.984836 Training: [22 epoch, 150 batch] loss: 3.86241, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:29:08.045040 Training: [22 epoch, 160 batch] loss: 3.83017, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:30:17.848345 Training: [22 epoch, 170 batch] loss: 3.78362, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:31:27.418050 Training: [22 epoch, 180 batch] loss: 3.77801, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:32:37.032570 Training: [22 epoch, 190 batch] loss: 3.77796, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:33:47.712700 Training: [22 epoch, 200 batch] loss: 3.75592, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.02761,MAE：0.83090
2021-01-04 21:38:04.806551 Training: [23 epoch,  10 batch] loss: 3.74020, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:39:15.234677 Training: [23 epoch,  20 batch] loss: 3.73043, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:40:25.663844 Training: [23 epoch,  30 batch] loss: 3.68838, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:41:36.400035 Training: [23 epoch,  40 batch] loss: 3.69653, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:42:47.078257 Training: [23 epoch,  50 batch] loss: 3.66967, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:43:57.666158 Training: [23 epoch,  60 batch] loss: 3.72286, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:45:08.545820 Training: [23 epoch,  70 batch] loss: 3.68176, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:46:19.401782 Training: [23 epoch,  80 batch] loss: 3.68600, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:47:30.037304 Training: [23 epoch,  90 batch] loss: 3.62227, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:48:40.665800 Training: [23 epoch, 100 batch] loss: 3.60669, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:49:51.479042 Training: [23 epoch, 110 batch] loss: 3.58636, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:51:01.968946 Training: [23 epoch, 120 batch] loss: 3.61170, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:52:12.817710 Training: [23 epoch, 130 batch] loss: 3.58036, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:53:23.554711 Training: [23 epoch, 140 batch] loss: 3.63774, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:54:34.241698 Training: [23 epoch, 150 batch] loss: 3.58793, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:55:45.150561 Training: [23 epoch, 160 batch] loss: 3.59861, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:56:55.961216 Training: [23 epoch, 170 batch] loss: 3.59118, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:58:06.703888 Training: [23 epoch, 180 batch] loss: 3.54891, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 21:59:15.180380 Training: [23 epoch, 190 batch] loss: 3.55351, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:00:21.199397 Training: [23 epoch, 200 batch] loss: 3.56752, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.05889,MAE：0.88289
2021-01-04 22:04:33.207127 Training: [24 epoch,  10 batch] loss: 3.50175, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:05:43.540346 Training: [24 epoch,  20 batch] loss: 3.50007, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:06:54.108554 Training: [24 epoch,  30 batch] loss: 3.47760, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:08:04.692739 Training: [24 epoch,  40 batch] loss: 3.50780, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:09:15.433582 Training: [24 epoch,  50 batch] loss: 3.43381, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:10:25.852556 Training: [24 epoch,  60 batch] loss: 3.46015, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:11:36.472908 Training: [24 epoch,  70 batch] loss: 3.45353, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:12:47.211255 Training: [24 epoch,  80 batch] loss: 3.35268, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:13:58.028644 Training: [24 epoch,  90 batch] loss: 3.41289, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:15:08.640871 Training: [24 epoch, 100 batch] loss: 3.42450, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:16:19.205869 Training: [24 epoch, 110 batch] loss: 3.47521, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:17:30.083900 Training: [24 epoch, 120 batch] loss: 3.43384, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:18:40.900093 Training: [24 epoch, 130 batch] loss: 3.39436, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:19:51.748647 Training: [24 epoch, 140 batch] loss: 3.37734, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:21:02.673438 Training: [24 epoch, 150 batch] loss: 3.32510, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:22:13.473286 Training: [24 epoch, 160 batch] loss: 3.39582, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:23:24.290890 Training: [24 epoch, 170 batch] loss: 3.29745, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:24:33.553040 Training: [24 epoch, 180 batch] loss: 3.28527, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:25:42.745970 Training: [24 epoch, 190 batch] loss: 3.27802, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:26:52.888417 Training: [24 epoch, 200 batch] loss: 3.30387, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.04417,MAE：0.85099
2021-01-04 22:31:05.322694 Training: [25 epoch,  10 batch] loss: 3.27648, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:32:15.725288 Training: [25 epoch,  20 batch] loss: 3.28887, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:33:26.389821 Training: [25 epoch,  30 batch] loss: 3.23836, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:34:33.384733 Training: [25 epoch,  40 batch] loss: 3.23579, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:35:38.540952 Training: [25 epoch,  50 batch] loss: 3.26437, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:36:42.769854 Training: [25 epoch,  60 batch] loss: 3.23958, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:37:47.253310 Training: [25 epoch,  70 batch] loss: 3.24137, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:38:51.322630 Training: [25 epoch,  80 batch] loss: 3.24432, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:39:55.772191 Training: [25 epoch,  90 batch] loss: 3.19407, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:41:02.378699 Training: [25 epoch, 100 batch] loss: 3.13291, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:42:08.731776 Training: [25 epoch, 110 batch] loss: 3.20756, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:43:14.041288 Training: [25 epoch, 120 batch] loss: 3.16589, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:44:20.074758 Training: [25 epoch, 130 batch] loss: 3.18776, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:45:26.372027 Training: [25 epoch, 140 batch] loss: 3.17921, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:46:31.647148 Training: [25 epoch, 150 batch] loss: 3.15953, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:47:37.924146 Training: [25 epoch, 160 batch] loss: 3.11149, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:48:43.979893 Training: [25 epoch, 170 batch] loss: 3.20375, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:49:48.411366 Training: [25 epoch, 180 batch] loss: 3.15691, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:50:53.008821 Training: [25 epoch, 190 batch] loss: 3.03582, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:51:58.248759 Training: [25 epoch, 200 batch] loss: 3.10237, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.09560,MAE：0.93644
2021-01-04 22:56:06.297552 Training: [26 epoch,  10 batch] loss: 3.10868, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:57:10.781104 Training: [26 epoch,  20 batch] loss: 3.07572, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:58:14.953650 Training: [26 epoch,  30 batch] loss: 3.06483, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 22:59:19.758026 Training: [26 epoch,  40 batch] loss: 3.08874, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:00:24.732373 Training: [26 epoch,  50 batch] loss: 3.01471, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:01:29.964235 Training: [26 epoch,  60 batch] loss: 3.05549, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:02:35.883812 Training: [26 epoch,  70 batch] loss: 3.01734, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:03:42.529331 Training: [26 epoch,  80 batch] loss: 3.00038, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:04:48.177444 Training: [26 epoch,  90 batch] loss: 3.01844, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:05:53.637869 Training: [26 epoch, 100 batch] loss: 3.06550, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:07:00.225062 Training: [26 epoch, 110 batch] loss: 3.04444, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:08:06.444793 Training: [26 epoch, 120 batch] loss: 3.02576, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:09:11.869853 Training: [26 epoch, 130 batch] loss: 3.00052, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:10:18.121615 Training: [26 epoch, 140 batch] loss: 3.02731, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:11:24.641140 Training: [26 epoch, 150 batch] loss: 2.94643, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:12:30.253251 Training: [26 epoch, 160 batch] loss: 2.96774, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:13:34.298241 Training: [26 epoch, 170 batch] loss: 2.92362, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:14:37.605740 Training: [26 epoch, 180 batch] loss: 2.88887, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:15:40.832251 Training: [26 epoch, 190 batch] loss: 2.96868, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:16:45.964983 Training: [26 epoch, 200 batch] loss: 2.90395, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.06370,MAE：0.88439
2021-01-04 23:20:53.580202 Training: [27 epoch,  10 batch] loss: 2.91032, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:21:59.187684 Training: [27 epoch,  20 batch] loss: 2.92095, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:23:04.900050 Training: [27 epoch,  30 batch] loss: 2.87799, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:24:09.893075 Training: [27 epoch,  40 batch] loss: 2.88674, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:25:15.359969 Training: [27 epoch,  50 batch] loss: 2.88744, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:26:20.561280 Training: [27 epoch,  60 batch] loss: 2.86598, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:27:25.176817 Training: [27 epoch,  70 batch] loss: 2.91485, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:28:29.573294 Training: [27 epoch,  80 batch] loss: 2.84419, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:29:34.999824 Training: [27 epoch,  90 batch] loss: 2.83878, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:30:40.704859 Training: [27 epoch, 100 batch] loss: 2.84105, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:31:45.952974 Training: [27 epoch, 110 batch] loss: 2.88004, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:32:52.158964 Training: [27 epoch, 120 batch] loss: 2.79914, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:33:58.188082 Training: [27 epoch, 130 batch] loss: 2.78438, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:35:03.460586 Training: [27 epoch, 140 batch] loss: 2.80342, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:36:09.040658 Training: [27 epoch, 150 batch] loss: 2.79453, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:37:14.682900 Training: [27 epoch, 160 batch] loss: 2.82895, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:38:19.020123 Training: [27 epoch, 170 batch] loss: 2.77822, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:39:23.388288 Training: [27 epoch, 180 batch] loss: 2.76822, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:40:28.363880 Training: [27 epoch, 190 batch] loss: 2.80166, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:41:33.860098 Training: [27 epoch, 200 batch] loss: 2.80655, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.11869,MAE：0.96271
2021-01-04 23:45:41.841515 Training: [28 epoch,  10 batch] loss: 2.77642, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:46:46.491895 Training: [28 epoch,  20 batch] loss: 2.77868, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:47:51.732326 Training: [28 epoch,  30 batch] loss: 2.73639, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:48:57.531488 Training: [28 epoch,  40 batch] loss: 2.73691, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:50:03.336242 Training: [28 epoch,  50 batch] loss: 2.69685, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:51:07.785166 Training: [28 epoch,  60 batch] loss: 2.78216, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:52:11.509185 Training: [28 epoch,  70 batch] loss: 2.70056, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:53:16.342839 Training: [28 epoch,  80 batch] loss: 2.75087, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:54:21.337902 Training: [28 epoch,  90 batch] loss: 2.69548, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:55:25.544861 Training: [28 epoch, 100 batch] loss: 2.64842, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:56:30.374975 Training: [28 epoch, 110 batch] loss: 2.64140, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:57:35.670166 Training: [28 epoch, 120 batch] loss: 2.64647, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:58:40.532719 Training: [28 epoch, 130 batch] loss: 2.83438, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-04 23:59:45.924993 Training: [28 epoch, 140 batch] loss: 2.65600, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:00:52.349717 Training: [28 epoch, 150 batch] loss: 2.64500, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:01:57.630361 Training: [28 epoch, 160 batch] loss: 2.66090, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:03:02.305880 Training: [28 epoch, 170 batch] loss: 2.66039, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:04:06.650464 Training: [28 epoch, 180 batch] loss: 2.61407, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:05:11.973775 Training: [28 epoch, 190 batch] loss: 2.57517, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:06:17.844248 Training: [28 epoch, 200 batch] loss: 2.64342, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.22412,MAE：1.07820
2021-01-05 00:10:25.238958 Training: [29 epoch,  10 batch] loss: 2.56554, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:11:29.649510 Training: [29 epoch,  20 batch] loss: 2.64085, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:12:34.653009 Training: [29 epoch,  30 batch] loss: 2.60218, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:13:40.276521 Training: [29 epoch,  40 batch] loss: 2.63380, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:14:46.238779 Training: [29 epoch,  50 batch] loss: 2.62873, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:15:51.867932 Training: [29 epoch,  60 batch] loss: 2.57062, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:16:57.055145 Training: [29 epoch,  70 batch] loss: 2.58188, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:18:02.400713 Training: [29 epoch,  80 batch] loss: 2.58971, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:19:08.688921 Training: [29 epoch,  90 batch] loss: 2.58741, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:20:14.625484 Training: [29 epoch, 100 batch] loss: 2.56778, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:21:19.849668 Training: [29 epoch, 110 batch] loss: 2.53683, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:22:25.477961 Training: [29 epoch, 120 batch] loss: 2.46923, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:23:31.573819 Training: [29 epoch, 130 batch] loss: 2.53003, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:24:37.165493 Training: [29 epoch, 140 batch] loss: 2.58636, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:25:41.497057 Training: [29 epoch, 150 batch] loss: 2.55199, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:26:44.916603 Training: [29 epoch, 160 batch] loss: 2.51908, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:27:47.524653 Training: [29 epoch, 170 batch] loss: 2.48824, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:28:51.005220 Training: [29 epoch, 180 batch] loss: 2.51124, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:29:55.806041 Training: [29 epoch, 190 batch] loss: 2.48962, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:31:00.181261 Training: [29 epoch, 200 batch] loss: 2.53193, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.07579,MAE：0.89620
2021-01-05 00:35:06.761384 Training: [30 epoch,  10 batch] loss: 2.50675, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:36:11.777768 Training: [30 epoch,  20 batch] loss: 2.47735, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:37:17.098517 Training: [30 epoch,  30 batch] loss: 2.45858, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:38:23.224865 Training: [30 epoch,  40 batch] loss: 2.55083, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:39:28.800344 Training: [30 epoch,  50 batch] loss: 2.45592, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:40:33.735016 Training: [30 epoch,  60 batch] loss: 2.47384, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:41:39.459475 Training: [30 epoch,  70 batch] loss: 2.47153, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:42:45.505364 Training: [30 epoch,  80 batch] loss: 2.46603, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:43:51.205126 Training: [30 epoch,  90 batch] loss: 2.40308, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:44:56.536041 Training: [30 epoch, 100 batch] loss: 2.40592, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:46:01.941616 Training: [30 epoch, 110 batch] loss: 2.40885, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:47:08.249606 Training: [30 epoch, 120 batch] loss: 2.42606, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:48:14.226592 Training: [30 epoch, 130 batch] loss: 2.44804, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:49:19.305608 Training: [30 epoch, 140 batch] loss: 2.43413, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:50:24.730599 Training: [30 epoch, 150 batch] loss: 2.43491, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:51:29.417229 Training: [30 epoch, 160 batch] loss: 2.43957, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:52:33.643467 Training: [30 epoch, 170 batch] loss: 2.44527, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:53:38.279898 Training: [30 epoch, 180 batch] loss: 2.40091, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:54:43.348859 Training: [30 epoch, 190 batch] loss: 2.41657, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 00:55:47.724997 Training: [30 epoch, 200 batch] loss: 2.44979, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.09815,MAE：0.93420
2021-01-05 00:59:16.700226 Training: [31 epoch,  10 batch] loss: 2.39212, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:00:11.415469 Training: [31 epoch,  20 batch] loss: 2.37774, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:01:06.357456 Training: [31 epoch,  30 batch] loss: 2.44647, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:02:01.524137 Training: [31 epoch,  40 batch] loss: 2.37989, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:02:56.903181 Training: [31 epoch,  50 batch] loss: 2.36372, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:03:52.503001 Training: [31 epoch,  60 batch] loss: 2.37033, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:04:48.676159 Training: [31 epoch,  70 batch] loss: 2.33541, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:05:44.536415 Training: [31 epoch,  80 batch] loss: 2.39283, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:06:39.960146 Training: [31 epoch,  90 batch] loss: 2.37400, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:07:35.942756 Training: [31 epoch, 100 batch] loss: 2.32674, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:08:32.227751 Training: [31 epoch, 110 batch] loss: 2.37619, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:09:27.849104 Training: [31 epoch, 120 batch] loss: 2.34815, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:10:23.560721 Training: [31 epoch, 130 batch] loss: 2.31171, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:11:19.453065 Training: [31 epoch, 140 batch] loss: 2.35944, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:12:15.324740 Training: [31 epoch, 150 batch] loss: 2.33069, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:13:09.996609 Training: [31 epoch, 160 batch] loss: 2.28663, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:14:04.600815 Training: [31 epoch, 170 batch] loss: 2.34433, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:14:59.923663 Training: [31 epoch, 180 batch] loss: 2.35552, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:15:55.611773 Training: [31 epoch, 190 batch] loss: 2.33162, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:16:51.668325 Training: [31 epoch, 200 batch] loss: 2.32192, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.11585,MAE：0.95399
2021-01-05 01:20:22.920028 Training: [32 epoch,  10 batch] loss: 2.31251, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:21:18.665250 Training: [32 epoch,  20 batch] loss: 2.27304, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:22:14.546049 Training: [32 epoch,  30 batch] loss: 2.24526, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:23:10.304580 Training: [32 epoch,  40 batch] loss: 2.28497, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:24:05.798942 Training: [32 epoch,  50 batch] loss: 2.32997, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:25:02.108668 Training: [32 epoch,  60 batch] loss: 2.30796, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:25:59.153675 Training: [32 epoch,  70 batch] loss: 2.31356, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:26:56.077091 Training: [32 epoch,  80 batch] loss: 2.27194, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:27:52.808555 Training: [32 epoch,  90 batch] loss: 2.29887, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:28:49.317065 Training: [32 epoch, 100 batch] loss: 2.26286, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:29:45.293403 Training: [32 epoch, 110 batch] loss: 2.28145, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:30:41.741506 Training: [32 epoch, 120 batch] loss: 2.30427, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:31:38.905853 Training: [32 epoch, 130 batch] loss: 2.23347, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:32:35.913099 Training: [32 epoch, 140 batch] loss: 2.25858, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:33:32.552249 Training: [32 epoch, 150 batch] loss: 2.21970, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:34:28.448041 Training: [32 epoch, 160 batch] loss: 2.33554, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:35:24.379763 Training: [32 epoch, 170 batch] loss: 2.22775, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:36:21.183632 Training: [32 epoch, 180 batch] loss: 2.25816, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:37:19.069278 Training: [32 epoch, 190 batch] loss: 2.22100, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:38:15.684960 Training: [32 epoch, 200 batch] loss: 2.25850, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.13851,MAE：0.98264
2021-01-05 01:41:34.884791 Training: [33 epoch,  10 batch] loss: 2.21662, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:42:31.296889 Training: [33 epoch,  20 batch] loss: 2.21342, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:43:27.903954 Training: [33 epoch,  30 batch] loss: 2.21745, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:44:23.977734 Training: [33 epoch,  40 batch] loss: 2.27253, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:45:19.865061 Training: [33 epoch,  50 batch] loss: 2.23409, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:46:16.724610 Training: [33 epoch,  60 batch] loss: 2.24701, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:47:13.560622 Training: [33 epoch,  70 batch] loss: 2.22259, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:48:09.605771 Training: [33 epoch,  80 batch] loss: 2.24564, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:49:06.171634 Training: [33 epoch,  90 batch] loss: 2.21396, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:50:03.252533 Training: [33 epoch, 100 batch] loss: 2.27377, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:50:59.636706 Training: [33 epoch, 110 batch] loss: 2.21501, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:51:56.113034 Training: [33 epoch, 120 batch] loss: 2.18612, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:52:53.416270 Training: [33 epoch, 130 batch] loss: 2.19470, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:53:50.004592 Training: [33 epoch, 140 batch] loss: 2.21492, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:54:46.204293 Training: [33 epoch, 150 batch] loss: 2.19859, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:55:42.221381 Training: [33 epoch, 160 batch] loss: 2.16462, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:56:38.476438 Training: [33 epoch, 170 batch] loss: 2.16620, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:57:34.907056 Training: [33 epoch, 180 batch] loss: 2.16702, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:58:32.292267 Training: [33 epoch, 190 batch] loss: 2.16065, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 01:59:29.261052 Training: [33 epoch, 200 batch] loss: 2.12229, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.09453,MAE：0.92731
2021-01-05 02:03:00.346767 Training: [34 epoch,  10 batch] loss: 2.17895, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:03:55.611617 Training: [34 epoch,  20 batch] loss: 2.13357, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:04:51.228816 Training: [34 epoch,  30 batch] loss: 2.13542, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:05:46.592197 Training: [34 epoch,  40 batch] loss: 2.18001, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:06:41.755158 Training: [34 epoch,  50 batch] loss: 2.16341, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:07:37.172150 Training: [34 epoch,  60 batch] loss: 2.15435, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:08:32.852431 Training: [34 epoch,  70 batch] loss: 2.22165, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:09:28.568772 Training: [34 epoch,  80 batch] loss: 2.12626, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:10:24.353767 Training: [34 epoch,  90 batch] loss: 2.12626, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:11:20.099481 Training: [34 epoch, 100 batch] loss: 2.18346, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:12:15.783684 Training: [34 epoch, 110 batch] loss: 2.14407, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:13:11.336391 Training: [34 epoch, 120 batch] loss: 2.14136, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:14:06.825220 Training: [34 epoch, 130 batch] loss: 2.14881, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:15:02.194322 Training: [34 epoch, 140 batch] loss: 2.11657, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:16:04.619748 Training: [34 epoch, 150 batch] loss: 2.16524, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:17:02.061736 Training: [34 epoch, 160 batch] loss: 2.09508, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:17:57.552333 Training: [34 epoch, 170 batch] loss: 2.11717, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:18:54.379889 Training: [34 epoch, 180 batch] loss: 2.10059, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:19:50.938968 Training: [34 epoch, 190 batch] loss: 2.16129, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:20:46.801428 Training: [34 epoch, 200 batch] loss: 2.07386, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.11464,MAE：0.95597
2021-01-05 02:24:07.846293 Training: [35 epoch,  10 batch] loss: 2.16853, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:25:04.235850 Training: [35 epoch,  20 batch] loss: 2.13304, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:26:00.907481 Training: [35 epoch,  30 batch] loss: 2.14259, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:26:57.176113 Training: [35 epoch,  40 batch] loss: 2.06669, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:27:53.085598 Training: [35 epoch,  50 batch] loss: 2.08554, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:28:50.011953 Training: [35 epoch,  60 batch] loss: 2.09231, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:29:46.856510 Training: [35 epoch,  70 batch] loss: 2.07906, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:30:42.912584 Training: [35 epoch,  80 batch] loss: 2.06836, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:31:39.904012 Training: [35 epoch,  90 batch] loss: 2.13611, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:32:37.030234 Training: [35 epoch, 100 batch] loss: 2.05607, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:33:33.298509 Training: [35 epoch, 110 batch] loss: 2.05625, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:34:29.583720 Training: [35 epoch, 120 batch] loss: 2.04066, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:35:26.169606 Training: [35 epoch, 130 batch] loss: 2.03754, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:36:22.129023 Training: [35 epoch, 140 batch] loss: 2.13117, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:37:18.062160 Training: [35 epoch, 150 batch] loss: 2.08779, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:38:14.114734 Training: [35 epoch, 160 batch] loss: 2.13528, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:39:11.097436 Training: [35 epoch, 170 batch] loss: 2.08129, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:40:07.656112 Training: [35 epoch, 180 batch] loss: 2.06265, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:41:04.815164 Training: [35 epoch, 190 batch] loss: 2.07980, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:42:01.225544 Training: [35 epoch, 200 batch] loss: 2.05115, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.07102,MAE：0.88654
2021-01-05 02:45:34.404283 Training: [36 epoch,  10 batch] loss: 2.07763, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:46:29.717382 Training: [36 epoch,  20 batch] loss: 2.04678, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:47:25.646880 Training: [36 epoch,  30 batch] loss: 2.09704, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:48:22.770109 Training: [36 epoch,  40 batch] loss: 2.04976, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:49:19.731667 Training: [36 epoch,  50 batch] loss: 2.07439, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:50:15.963737 Training: [36 epoch,  60 batch] loss: 2.02832, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:51:12.412992 Training: [36 epoch,  70 batch] loss: 2.02359, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:52:09.759483 Training: [36 epoch,  80 batch] loss: 2.04501, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:53:07.273777 Training: [36 epoch,  90 batch] loss: 2.02349, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:54:04.579141 Training: [36 epoch, 100 batch] loss: 2.02577, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:55:01.717044 Training: [36 epoch, 110 batch] loss: 2.07220, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:55:58.201949 Training: [36 epoch, 120 batch] loss: 2.06172, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:56:55.009369 Training: [36 epoch, 130 batch] loss: 2.07422, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:57:51.242593 Training: [36 epoch, 140 batch] loss: 2.00746, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:58:47.531377 Training: [36 epoch, 150 batch] loss: 1.98723, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 02:59:44.293217 Training: [36 epoch, 160 batch] loss: 2.05381, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:00:41.546832 Training: [36 epoch, 170 batch] loss: 2.03158, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:01:38.140870 Training: [36 epoch, 180 batch] loss: 2.05819, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:02:35.348611 Training: [36 epoch, 190 batch] loss: 2.04504, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:03:31.944952 Training: [36 epoch, 200 batch] loss: 2.05305, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.15907,MAE：1.00510
2021-01-05 03:06:46.413036 Training: [37 epoch,  10 batch] loss: 2.00673, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:07:41.331468 Training: [37 epoch,  20 batch] loss: 1.99649, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:08:36.971228 Training: [37 epoch,  30 batch] loss: 2.01486, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:09:32.868983 Training: [37 epoch,  40 batch] loss: 2.01077, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:10:28.397077 Training: [37 epoch,  50 batch] loss: 2.01972, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:11:23.902722 Training: [37 epoch,  60 batch] loss: 2.04496, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:12:19.188409 Training: [37 epoch,  70 batch] loss: 2.07322, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:13:14.909603 Training: [37 epoch,  80 batch] loss: 2.00991, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:14:10.774895 Training: [37 epoch,  90 batch] loss: 1.99092, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:15:06.396209 Training: [37 epoch, 100 batch] loss: 1.97510, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:16:01.861646 Training: [37 epoch, 110 batch] loss: 2.05799, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:16:57.753731 Training: [37 epoch, 120 batch] loss: 1.99625, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:17:53.831270 Training: [37 epoch, 130 batch] loss: 1.96945, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:18:49.240491 Training: [37 epoch, 140 batch] loss: 1.97738, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:19:44.468196 Training: [37 epoch, 150 batch] loss: 1.98103, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:20:39.517035 Training: [37 epoch, 160 batch] loss: 1.97401, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:21:35.561053 Training: [37 epoch, 170 batch] loss: 1.96936, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:22:31.460540 Training: [37 epoch, 180 batch] loss: 1.96769, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:23:27.521038 Training: [37 epoch, 190 batch] loss: 2.02400, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:24:23.611575 Training: [37 epoch, 200 batch] loss: 2.00807, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.10153,MAE：0.94123
2021-01-05 03:27:41.727187 Training: [38 epoch,  10 batch] loss: 2.00512, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:28:37.989068 Training: [38 epoch,  20 batch] loss: 1.93563, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:29:34.531530 Training: [38 epoch,  30 batch] loss: 2.04982, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:30:31.599995 Training: [38 epoch,  40 batch] loss: 1.96755, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:31:28.372601 Training: [38 epoch,  50 batch] loss: 1.95575, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:32:25.160126 Training: [38 epoch,  60 batch] loss: 1.97385, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:33:22.393597 Training: [38 epoch,  70 batch] loss: 2.01292, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:34:19.065700 Training: [38 epoch,  80 batch] loss: 1.90244, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:35:16.222386 Training: [38 epoch,  90 batch] loss: 1.99623, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:36:13.324518 Training: [38 epoch, 100 batch] loss: 1.94929, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:37:10.507981 Training: [38 epoch, 110 batch] loss: 1.96360, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:38:07.373103 Training: [38 epoch, 120 batch] loss: 1.91201, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:39:04.468976 Training: [38 epoch, 130 batch] loss: 1.96479, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:40:02.009038 Training: [38 epoch, 140 batch] loss: 1.98138, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:40:59.328578 Training: [38 epoch, 150 batch] loss: 1.93907, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:41:55.642091 Training: [38 epoch, 160 batch] loss: 2.00871, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:42:51.841727 Training: [38 epoch, 170 batch] loss: 1.93184, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:43:48.366603 Training: [38 epoch, 180 batch] loss: 1.97467, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:44:45.583244 Training: [38 epoch, 190 batch] loss: 1.93948, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:45:43.043831 Training: [38 epoch, 200 batch] loss: 1.96011, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.12967,MAE：0.97216
2021-01-05 03:49:18.679393 Training: [39 epoch,  10 batch] loss: 1.98412, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:50:17.976209 Training: [39 epoch,  20 batch] loss: 1.94810, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:51:17.258812 Training: [39 epoch,  30 batch] loss: 1.92622, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:52:16.866764 Training: [39 epoch,  40 batch] loss: 1.96178, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:53:16.232273 Training: [39 epoch,  50 batch] loss: 1.94304, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:54:15.874657 Training: [39 epoch,  60 batch] loss: 1.94552, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:55:15.073371 Training: [39 epoch,  70 batch] loss: 1.94105, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:56:12.203817 Training: [39 epoch,  80 batch] loss: 1.94189, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:57:08.352436 Training: [39 epoch,  90 batch] loss: 1.94083, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:58:05.404331 Training: [39 epoch, 100 batch] loss: 1.88722, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:59:02.334065 Training: [39 epoch, 110 batch] loss: 1.97846, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 03:59:58.720690 Training: [39 epoch, 120 batch] loss: 1.92797, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:00:55.912082 Training: [39 epoch, 130 batch] loss: 1.89905, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:01:52.333762 Training: [39 epoch, 140 batch] loss: 1.94676, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:02:48.377299 Training: [39 epoch, 150 batch] loss: 1.93989, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:03:44.420037 Training: [39 epoch, 160 batch] loss: 1.96199, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:04:40.888537 Training: [39 epoch, 170 batch] loss: 1.91009, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:05:37.268383 Training: [39 epoch, 180 batch] loss: 1.94526, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:06:34.106080 Training: [39 epoch, 190 batch] loss: 1.93718, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:07:30.616008 Training: [39 epoch, 200 batch] loss: 1.90459, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.06819,MAE：0.89094
2021-01-05 04:10:48.182739 Training: [40 epoch,  10 batch] loss: 1.92893, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:11:43.065067 Training: [40 epoch,  20 batch] loss: 1.92569, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:12:38.269845 Training: [40 epoch,  30 batch] loss: 1.91576, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:13:34.391449 Training: [40 epoch,  40 batch] loss: 1.85176, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:14:30.355083 Training: [40 epoch,  50 batch] loss: 1.88595, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:15:26.115362 Training: [40 epoch,  60 batch] loss: 1.95574, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:16:21.659863 Training: [40 epoch,  70 batch] loss: 1.92895, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:17:16.937635 Training: [40 epoch,  80 batch] loss: 1.89019, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:18:11.573058 Training: [40 epoch,  90 batch] loss: 1.97077, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:19:06.808273 Training: [40 epoch, 100 batch] loss: 1.92185, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:20:03.317113 Training: [40 epoch, 110 batch] loss: 1.90425, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:20:59.468067 Training: [40 epoch, 120 batch] loss: 1.89834, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:21:55.069341 Training: [40 epoch, 130 batch] loss: 1.87900, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:22:50.313423 Training: [40 epoch, 140 batch] loss: 1.90577, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:23:45.392381 Training: [40 epoch, 150 batch] loss: 1.90094, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:24:40.312834 Training: [40 epoch, 160 batch] loss: 1.91628, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:25:35.695348 Training: [40 epoch, 170 batch] loss: 1.97349, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:26:32.227885 Training: [40 epoch, 180 batch] loss: 1.86347, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:27:28.405315 Training: [40 epoch, 190 batch] loss: 1.87490, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:28:23.935781 Training: [40 epoch, 200 batch] loss: 1.93048, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.12786,MAE：0.97041
2021-01-05 04:31:45.947951 Training: [41 epoch,  10 batch] loss: 1.93401, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:32:42.474085 Training: [41 epoch,  20 batch] loss: 1.86275, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:33:38.950151 Training: [41 epoch,  30 batch] loss: 1.85372, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:34:35.621753 Training: [41 epoch,  40 batch] loss: 1.85662, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:35:32.321607 Training: [41 epoch,  50 batch] loss: 1.80664, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:36:29.220501 Training: [41 epoch,  60 batch] loss: 1.93120, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:37:26.347190 Training: [41 epoch,  70 batch] loss: 1.86325, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:38:23.399922 Training: [41 epoch,  80 batch] loss: 1.89242, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:39:20.513096 Training: [41 epoch,  90 batch] loss: 1.91152, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:40:17.972603 Training: [41 epoch, 100 batch] loss: 1.87613, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:41:15.141175 Training: [41 epoch, 110 batch] loss: 1.90102, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:42:12.742847 Training: [41 epoch, 120 batch] loss: 1.92349, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:43:09.927360 Training: [41 epoch, 130 batch] loss: 1.87071, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:44:07.549231 Training: [41 epoch, 140 batch] loss: 1.88230, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:45:03.753603 Training: [41 epoch, 150 batch] loss: 1.94123, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:45:59.981054 Training: [41 epoch, 160 batch] loss: 1.88874, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:46:56.780002 Training: [41 epoch, 170 batch] loss: 1.89674, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:47:54.142680 Training: [41 epoch, 180 batch] loss: 1.92851, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:48:51.268318 Training: [41 epoch, 190 batch] loss: 1.83800, the best RMSE/MAE: 0.94347 / 0.69777
2021-01-05 04:49:48.504144 Training: [41 epoch, 200 batch] loss: 1.86827, the best RMSE/MAE: 0.94347 / 0.69777
<Test> RMSE：1.08613,MAE：0.92435
The best RMSE/MAE：0.94347/0.69777
