-------------------- Hyperparams --------------------
time: 2021-01-10 13:45:42.695314
Dataset: amazon
N: 30000
weight decay: 0.1
dropout rate: 0.5
learning rate: 0.0005
dimension of embedding: 64
use_cuda: True
2021-01-10 14:03:30.058256 Training: [1 epoch,  10 batch] loss: 16.46950, the best RMSE/MAE: inf / inf
2021-01-10 14:06:24.639327 Training: [1 epoch,  20 batch] loss: 15.15633, the best RMSE/MAE: inf / inf
2021-01-10 14:09:09.433416 Training: [1 epoch,  30 batch] loss: 13.98539, the best RMSE/MAE: inf / inf
2021-01-10 14:12:02.166307 Training: [1 epoch,  40 batch] loss: 13.02172, the best RMSE/MAE: inf / inf
2021-01-10 14:14:54.394756 Training: [1 epoch,  50 batch] loss: 11.96207, the best RMSE/MAE: inf / inf
2021-01-10 14:17:48.612143 Training: [1 epoch,  60 batch] loss: 10.68538, the best RMSE/MAE: inf / inf
2021-01-10 14:20:42.486366 Training: [1 epoch,  70 batch] loss: 10.00419, the best RMSE/MAE: inf / inf
2021-01-10 14:23:27.671683 Training: [1 epoch,  80 batch] loss: 9.07467, the best RMSE/MAE: inf / inf
2021-01-10 14:26:20.744681 Training: [1 epoch,  90 batch] loss: 8.14973, the best RMSE/MAE: inf / inf
2021-01-10 14:29:11.301917 Training: [1 epoch, 100 batch] loss: 7.53827, the best RMSE/MAE: inf / inf
2021-01-10 14:32:05.435514 Training: [1 epoch, 110 batch] loss: 6.82389, the best RMSE/MAE: inf / inf
2021-01-10 14:34:59.725279 Training: [1 epoch, 120 batch] loss: 6.41525, the best RMSE/MAE: inf / inf
2021-01-10 14:37:44.115310 Training: [1 epoch, 130 batch] loss: 5.75684, the best RMSE/MAE: inf / inf
2021-01-10 14:40:35.474370 Training: [1 epoch, 140 batch] loss: 5.45438, the best RMSE/MAE: inf / inf
2021-01-10 14:43:27.038502 Training: [1 epoch, 150 batch] loss: 5.11373, the best RMSE/MAE: inf / inf
2021-01-10 14:46:22.123813 Training: [1 epoch, 160 batch] loss: 4.56388, the best RMSE/MAE: inf / inf
2021-01-10 14:49:16.493885 Training: [1 epoch, 170 batch] loss: 4.20689, the best RMSE/MAE: inf / inf
2021-01-10 14:52:02.411742 Training: [1 epoch, 180 batch] loss: 3.88971, the best RMSE/MAE: inf / inf
2021-01-10 14:54:54.486396 Training: [1 epoch, 190 batch] loss: 3.56494, the best RMSE/MAE: inf / inf
2021-01-10 14:57:46.139742 Training: [1 epoch, 200 batch] loss: 3.27432, the best RMSE/MAE: inf / inf
<Test> RMSE：225534.68750,MAE：170967.75000
2021-01-10 15:08:58.421028 Training: [2 epoch,  10 batch] loss: 2.91931, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:11:48.538776 Training: [2 epoch,  20 batch] loss: 2.79033, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:14:42.554323 Training: [2 epoch,  30 batch] loss: 2.61392, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:17:35.568662 Training: [2 epoch,  40 batch] loss: 2.46978, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:20:21.221057 Training: [2 epoch,  50 batch] loss: 2.40568, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:23:12.385979 Training: [2 epoch,  60 batch] loss: 2.25608, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:26:04.024236 Training: [2 epoch,  70 batch] loss: 2.25064, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:28:59.683696 Training: [2 epoch,  80 batch] loss: 2.13173, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:31:53.999062 Training: [2 epoch,  90 batch] loss: 2.09794, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:34:40.339068 Training: [2 epoch, 100 batch] loss: 2.02146, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:37:30.708853 Training: [2 epoch, 110 batch] loss: 1.93791, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:40:22.265445 Training: [2 epoch, 120 batch] loss: 1.93121, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:43:17.293751 Training: [2 epoch, 130 batch] loss: 2.05230, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:46:12.589742 Training: [2 epoch, 140 batch] loss: 1.92893, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:48:59.190678 Training: [2 epoch, 150 batch] loss: 1.90410, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:51:50.011156 Training: [2 epoch, 160 batch] loss: 1.84496, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:54:40.943560 Training: [2 epoch, 170 batch] loss: 1.87939, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 15:57:34.517018 Training: [2 epoch, 180 batch] loss: 1.85870, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 16:00:28.315672 Training: [2 epoch, 190 batch] loss: 1.88539, the best RMSE/MAE: 225534.68750 / 170967.75000
2021-01-10 16:03:13.886200 Training: [2 epoch, 200 batch] loss: 1.84189, the best RMSE/MAE: 225534.68750 / 170967.75000
<Test> RMSE：6824.68994,MAE：3918.96948
2021-01-10 16:14:34.794298 Training: [3 epoch,  10 batch] loss: 1.86937, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:17:20.427426 Training: [3 epoch,  20 batch] loss: 1.83087, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:20:09.162971 Training: [3 epoch,  30 batch] loss: 1.90517, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:23:00.561501 Training: [3 epoch,  40 batch] loss: 1.83415, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:25:53.668632 Training: [3 epoch,  50 batch] loss: 1.75089, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:28:48.392345 Training: [3 epoch,  60 batch] loss: 1.82658, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:31:34.876145 Training: [3 epoch,  70 batch] loss: 1.76128, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:34:24.673917 Training: [3 epoch,  80 batch] loss: 1.71848, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:37:18.050775 Training: [3 epoch,  90 batch] loss: 1.80683, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:40:10.553897 Training: [3 epoch, 100 batch] loss: 1.79362, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:43:04.591769 Training: [3 epoch, 110 batch] loss: 1.83256, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:45:50.917829 Training: [3 epoch, 120 batch] loss: 1.83745, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:48:40.821738 Training: [3 epoch, 130 batch] loss: 1.82962, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:51:34.341648 Training: [3 epoch, 140 batch] loss: 1.78887, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:54:25.572268 Training: [3 epoch, 150 batch] loss: 1.73962, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 16:57:19.124940 Training: [3 epoch, 160 batch] loss: 1.78377, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 17:00:05.717392 Training: [3 epoch, 170 batch] loss: 1.79660, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 17:02:55.370783 Training: [3 epoch, 180 batch] loss: 1.75143, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 17:05:49.246353 Training: [3 epoch, 190 batch] loss: 1.76263, the best RMSE/MAE: 6824.68994 / 3918.96948
2021-01-10 17:08:40.943201 Training: [3 epoch, 200 batch] loss: 1.76585, the best RMSE/MAE: 6824.68994 / 3918.96948
<Test> RMSE：1382.80200,MAE：474.38702
2021-01-10 17:19:50.851999 Training: [4 epoch,  10 batch] loss: 1.79095, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:22:42.551580 Training: [4 epoch,  20 batch] loss: 1.73141, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:25:35.980329 Training: [4 epoch,  30 batch] loss: 1.66229, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:28:23.118526 Training: [4 epoch,  40 batch] loss: 1.74459, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:31:11.164764 Training: [4 epoch,  50 batch] loss: 1.76239, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:34:06.371919 Training: [4 epoch,  60 batch] loss: 1.69812, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:36:58.971589 Training: [4 epoch,  70 batch] loss: 1.69472, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:39:55.864950 Training: [4 epoch,  80 batch] loss: 1.74942, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:42:43.584743 Training: [4 epoch,  90 batch] loss: 1.82546, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:45:31.500405 Training: [4 epoch, 100 batch] loss: 1.86680, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:48:25.652533 Training: [4 epoch, 110 batch] loss: 1.72190, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:51:16.624912 Training: [4 epoch, 120 batch] loss: 1.64239, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:54:10.669080 Training: [4 epoch, 130 batch] loss: 1.74711, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:56:59.073933 Training: [4 epoch, 140 batch] loss: 1.82894, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 17:59:48.293177 Training: [4 epoch, 150 batch] loss: 1.73309, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 18:02:42.957412 Training: [4 epoch, 160 batch] loss: 1.86018, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 18:05:35.446765 Training: [4 epoch, 170 batch] loss: 1.72078, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 18:08:30.561812 Training: [4 epoch, 180 batch] loss: 1.72465, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 18:11:19.262229 Training: [4 epoch, 190 batch] loss: 1.71457, the best RMSE/MAE: 1382.80200 / 474.38702
2021-01-10 18:14:09.148217 Training: [4 epoch, 200 batch] loss: 1.73732, the best RMSE/MAE: 1382.80200 / 474.38702
<Test> RMSE：1652.46094,MAE：373.91867
2021-01-10 18:25:26.858382 Training: [5 epoch,  10 batch] loss: 1.68581, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:28:14.493366 Training: [5 epoch,  20 batch] loss: 1.65844, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:31:06.335305 Training: [5 epoch,  30 batch] loss: 1.72085, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:33:58.077130 Training: [5 epoch,  40 batch] loss: 1.71226, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:36:51.104572 Training: [5 epoch,  50 batch] loss: 1.69121, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:39:38.724741 Training: [5 epoch,  60 batch] loss: 1.77667, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:42:27.569771 Training: [5 epoch,  70 batch] loss: 1.69379, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:45:19.864197 Training: [5 epoch,  80 batch] loss: 1.73843, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:48:11.363500 Training: [5 epoch,  90 batch] loss: 1.65595, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:51:04.483104 Training: [5 epoch, 100 batch] loss: 1.75382, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:53:54.024823 Training: [5 epoch, 110 batch] loss: 1.68372, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:56:43.612373 Training: [5 epoch, 120 batch] loss: 1.64849, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 18:59:35.949573 Training: [5 epoch, 130 batch] loss: 1.60133, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:02:29.098221 Training: [5 epoch, 140 batch] loss: 1.64854, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:05:20.830907 Training: [5 epoch, 150 batch] loss: 1.67230, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:08:11.088145 Training: [5 epoch, 160 batch] loss: 1.68766, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:10:58.895592 Training: [5 epoch, 170 batch] loss: 1.58820, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:13:50.200348 Training: [5 epoch, 180 batch] loss: 1.53490, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:16:43.425297 Training: [5 epoch, 190 batch] loss: 1.60088, the best RMSE/MAE: 1652.46094 / 373.91867
2021-01-10 19:19:36.226949 Training: [5 epoch, 200 batch] loss: 1.66839, the best RMSE/MAE: 1652.46094 / 373.91867
<Test> RMSE：209.10677,MAE：78.61806
2021-01-10 19:30:49.072515 Training: [6 epoch,  10 batch] loss: 1.61554, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:33:41.137368 Training: [6 epoch,  20 batch] loss: 1.62891, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:36:32.320314 Training: [6 epoch,  30 batch] loss: 1.62276, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:39:19.706441 Training: [6 epoch,  40 batch] loss: 1.63588, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:42:11.064478 Training: [6 epoch,  50 batch] loss: 1.64670, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:45:07.179473 Training: [6 epoch,  60 batch] loss: 1.62364, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:48:00.609368 Training: [6 epoch,  70 batch] loss: 1.65154, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:50:52.704832 Training: [6 epoch,  80 batch] loss: 1.66671, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:53:42.556870 Training: [6 epoch,  90 batch] loss: 1.58895, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:56:34.466611 Training: [6 epoch, 100 batch] loss: 1.58965, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 19:59:28.950238 Training: [6 epoch, 110 batch] loss: 1.55031, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:02:19.044651 Training: [6 epoch, 120 batch] loss: 1.60643, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:05:10.362001 Training: [6 epoch, 130 batch] loss: 1.55715, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:07:57.975838 Training: [6 epoch, 140 batch] loss: 1.58810, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:10:47.180152 Training: [6 epoch, 150 batch] loss: 1.57895, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:13:41.060866 Training: [6 epoch, 160 batch] loss: 1.59277, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:16:32.604613 Training: [6 epoch, 170 batch] loss: 1.56608, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:19:25.968467 Training: [6 epoch, 180 batch] loss: 1.56470, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:22:27.874240 Training: [6 epoch, 190 batch] loss: 1.54942, the best RMSE/MAE: 209.10677 / 78.61806
2021-01-10 20:25:29.260088 Training: [6 epoch, 200 batch] loss: 1.54096, the best RMSE/MAE: 209.10677 / 78.61806
<Test> RMSE：71.19416,MAE：64.53033
2021-01-10 20:36:43.795022 Training: [7 epoch,  10 batch] loss: 1.59118, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:39:35.020578 Training: [7 epoch,  20 batch] loss: 1.48240, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:42:29.920204 Training: [7 epoch,  30 batch] loss: 1.58400, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:45:21.988611 Training: [7 epoch,  40 batch] loss: 1.48535, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:48:12.328952 Training: [7 epoch,  50 batch] loss: 1.48285, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:50:57.673402 Training: [7 epoch,  60 batch] loss: 1.55500, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:53:46.506908 Training: [7 epoch,  70 batch] loss: 1.49459, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:56:37.720171 Training: [7 epoch,  80 batch] loss: 1.57516, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 20:59:26.128794 Training: [7 epoch,  90 batch] loss: 1.55833, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:02:23.126609 Training: [7 epoch, 100 batch] loss: 1.53759, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:05:06.680583 Training: [7 epoch, 110 batch] loss: 1.55849, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:08:04.572322 Training: [7 epoch, 120 batch] loss: 1.47546, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:10:57.983122 Training: [7 epoch, 130 batch] loss: 1.51943, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:13:55.772499 Training: [7 epoch, 140 batch] loss: 1.52484, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:16:56.186680 Training: [7 epoch, 150 batch] loss: 1.47457, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:19:43.509976 Training: [7 epoch, 160 batch] loss: 1.51964, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:22:32.635835 Training: [7 epoch, 170 batch] loss: 1.52557, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:25:31.282085 Training: [7 epoch, 180 batch] loss: 1.54398, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:28:21.177978 Training: [7 epoch, 190 batch] loss: 1.52681, the best RMSE/MAE: 71.19416 / 64.53033
2021-01-10 21:31:13.954057 Training: [7 epoch, 200 batch] loss: 1.44654, the best RMSE/MAE: 71.19416 / 64.53033
<Test> RMSE：51.97126,MAE：46.46028
2021-01-10 21:42:31.033142 Training: [8 epoch,  10 batch] loss: 1.49726, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:45:26.573195 Training: [8 epoch,  20 batch] loss: 1.43333, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:48:14.025828 Training: [8 epoch,  30 batch] loss: 1.39068, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:51:03.284483 Training: [8 epoch,  40 batch] loss: 1.42229, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:53:56.694825 Training: [8 epoch,  50 batch] loss: 1.53407, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:56:47.159130 Training: [8 epoch,  60 batch] loss: 1.43600, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 21:59:49.404653 Training: [8 epoch,  70 batch] loss: 1.46119, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:02:46.833202 Training: [8 epoch,  80 batch] loss: 1.44799, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:05:38.536114 Training: [8 epoch,  90 batch] loss: 1.45990, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:08:39.046963 Training: [8 epoch, 100 batch] loss: 1.45740, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:11:31.129927 Training: [8 epoch, 110 batch] loss: 1.41378, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:14:24.305458 Training: [8 epoch, 120 batch] loss: 1.45316, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:17:18.827106 Training: [8 epoch, 130 batch] loss: 1.36349, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:20:38.199268 Training: [8 epoch, 140 batch] loss: 1.38443, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:23:55.422840 Training: [8 epoch, 150 batch] loss: 1.55361, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:27:12.578182 Training: [8 epoch, 160 batch] loss: 1.43400, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:30:31.355371 Training: [8 epoch, 170 batch] loss: 1.40710, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:33:47.152314 Training: [8 epoch, 180 batch] loss: 1.44896, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:37:01.468066 Training: [8 epoch, 190 batch] loss: 1.46539, the best RMSE/MAE: 51.97126 / 46.46028
2021-01-10 22:40:19.240575 Training: [8 epoch, 200 batch] loss: 1.42548, the best RMSE/MAE: 51.97126 / 46.46028
<Test> RMSE：29.93081,MAE：26.17866
2021-01-10 22:52:08.149842 Training: [9 epoch,  10 batch] loss: 1.46406, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 22:54:58.973989 Training: [9 epoch,  20 batch] loss: 1.33220, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 22:57:51.952086 Training: [9 epoch,  30 batch] loss: 1.40729, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:00:45.513467 Training: [9 epoch,  40 batch] loss: 1.32556, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:03:31.399999 Training: [9 epoch,  50 batch] loss: 1.42144, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:06:23.440809 Training: [9 epoch,  60 batch] loss: 1.38489, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:09:16.221713 Training: [9 epoch,  70 batch] loss: 1.40144, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:12:17.550345 Training: [9 epoch,  80 batch] loss: 1.39926, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:15:08.436288 Training: [9 epoch,  90 batch] loss: 1.36222, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:17:56.220047 Training: [9 epoch, 100 batch] loss: 1.37203, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:20:48.043150 Training: [9 epoch, 110 batch] loss: 1.38293, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:23:39.930507 Training: [9 epoch, 120 batch] loss: 1.39307, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:26:32.856199 Training: [9 epoch, 130 batch] loss: 1.35723, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:29:23.261002 Training: [9 epoch, 140 batch] loss: 1.33811, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:32:11.679269 Training: [9 epoch, 150 batch] loss: 1.38210, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:35:05.451835 Training: [9 epoch, 160 batch] loss: 1.36525, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:37:55.847661 Training: [9 epoch, 170 batch] loss: 1.42728, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:40:49.572900 Training: [9 epoch, 180 batch] loss: 1.36102, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:43:39.634441 Training: [9 epoch, 190 batch] loss: 1.37761, the best RMSE/MAE: 29.93081 / 26.17866
2021-01-10 23:46:26.203912 Training: [9 epoch, 200 batch] loss: 1.45354, the best RMSE/MAE: 29.93081 / 26.17866
<Test> RMSE：24.75475,MAE：20.00545
2021-01-10 23:57:59.346419 Training: [10 epoch,  10 batch] loss: 1.31919, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:00:50.841376 Training: [10 epoch,  20 batch] loss: 1.32267, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:03:44.506423 Training: [10 epoch,  30 batch] loss: 1.30224, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:06:46.543958 Training: [10 epoch,  40 batch] loss: 1.35141, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:09:47.097096 Training: [10 epoch,  50 batch] loss: 1.27722, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:12:40.445353 Training: [10 epoch,  60 batch] loss: 1.34371, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:15:39.722133 Training: [10 epoch,  70 batch] loss: 1.44758, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:18:31.624931 Training: [10 epoch,  80 batch] loss: 1.30903, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:21:25.293118 Training: [10 epoch,  90 batch] loss: 1.32749, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:24:20.547555 Training: [10 epoch, 100 batch] loss: 1.35547, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:27:19.589525 Training: [10 epoch, 110 batch] loss: 1.30754, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:30:05.647484 Training: [10 epoch, 120 batch] loss: 1.38095, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:33:05.177532 Training: [10 epoch, 130 batch] loss: 1.29219, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:35:58.812805 Training: [10 epoch, 140 batch] loss: 1.26036, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:38:58.051760 Training: [10 epoch, 150 batch] loss: 1.32622, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:41:46.915979 Training: [10 epoch, 160 batch] loss: 1.29405, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:44:35.368747 Training: [10 epoch, 170 batch] loss: 1.30538, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:47:25.745117 Training: [10 epoch, 180 batch] loss: 1.29102, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:50:19.618592 Training: [10 epoch, 190 batch] loss: 1.30239, the best RMSE/MAE: 24.75475 / 20.00545
2021-01-11 00:53:14.852335 Training: [10 epoch, 200 batch] loss: 1.31780, the best RMSE/MAE: 24.75475 / 20.00545
<Test> RMSE：14.58966,MAE：11.47424
2021-01-11 01:04:25.436270 Training: [11 epoch,  10 batch] loss: 1.30389, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:07:17.688628 Training: [11 epoch,  20 batch] loss: 1.21272, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:10:07.856605 Training: [11 epoch,  30 batch] loss: 1.25427, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:12:54.330344 Training: [11 epoch,  40 batch] loss: 1.27819, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:15:43.583448 Training: [11 epoch,  50 batch] loss: 1.31229, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:18:35.298823 Training: [11 epoch,  60 batch] loss: 1.26832, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:21:28.215777 Training: [11 epoch,  70 batch] loss: 1.31064, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:24:15.891036 Training: [11 epoch,  80 batch] loss: 1.26640, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:27:01.577838 Training: [11 epoch,  90 batch] loss: 1.30767, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:29:51.866710 Training: [11 epoch, 100 batch] loss: 1.30252, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:32:44.691288 Training: [11 epoch, 110 batch] loss: 1.21385, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:35:42.215262 Training: [11 epoch, 120 batch] loss: 1.23265, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:38:34.888405 Training: [11 epoch, 130 batch] loss: 1.21422, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:41:21.010406 Training: [11 epoch, 140 batch] loss: 1.25913, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:44:10.801674 Training: [11 epoch, 150 batch] loss: 1.21772, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:47:02.708354 Training: [11 epoch, 160 batch] loss: 1.32178, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:49:55.230615 Training: [11 epoch, 170 batch] loss: 1.25049, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:52:45.197716 Training: [11 epoch, 180 batch] loss: 1.25302, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:55:32.503963 Training: [11 epoch, 190 batch] loss: 1.28459, the best RMSE/MAE: 14.58966 / 11.47424
2021-01-11 01:58:21.996970 Training: [11 epoch, 200 batch] loss: 1.30313, the best RMSE/MAE: 14.58966 / 11.47424
<Test> RMSE：14.42114,MAE：11.15759
2021-01-11 02:09:39.661774 Training: [12 epoch,  10 batch] loss: 1.15566, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:12:30.832149 Training: [12 epoch,  20 batch] loss: 1.19752, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:15:21.722817 Training: [12 epoch,  30 batch] loss: 1.21765, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:18:14.745120 Training: [12 epoch,  40 batch] loss: 1.26284, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:21:03.361099 Training: [12 epoch,  50 batch] loss: 1.18543, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:23:48.413047 Training: [12 epoch,  60 batch] loss: 1.23866, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:26:52.001162 Training: [12 epoch,  70 batch] loss: 1.29984, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:29:44.282687 Training: [12 epoch,  80 batch] loss: 1.19052, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:32:38.286624 Training: [12 epoch,  90 batch] loss: 1.20762, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:35:27.456092 Training: [12 epoch, 100 batch] loss: 1.18415, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:38:13.800431 Training: [12 epoch, 110 batch] loss: 1.25011, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:41:03.065831 Training: [12 epoch, 120 batch] loss: 1.20310, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:44:05.978903 Training: [12 epoch, 130 batch] loss: 1.24564, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:47:02.808578 Training: [12 epoch, 140 batch] loss: 1.20206, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:49:50.695988 Training: [12 epoch, 150 batch] loss: 1.27199, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:52:36.697944 Training: [12 epoch, 160 batch] loss: 1.21451, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:55:28.204663 Training: [12 epoch, 170 batch] loss: 1.15456, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 02:58:21.527227 Training: [12 epoch, 180 batch] loss: 1.18647, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 03:01:15.273576 Training: [12 epoch, 190 batch] loss: 1.24642, the best RMSE/MAE: 14.42114 / 11.15759
2021-01-11 03:04:12.062876 Training: [12 epoch, 200 batch] loss: 1.21322, the best RMSE/MAE: 14.42114 / 11.15759
<Test> RMSE：11.31558,MAE：8.94128
2021-01-11 03:15:27.967054 Training: [13 epoch,  10 batch] loss: 1.19291, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:18:17.230935 Training: [13 epoch,  20 batch] loss: 1.14941, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:21:03.591773 Training: [13 epoch,  30 batch] loss: 1.18921, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:23:52.819803 Training: [13 epoch,  40 batch] loss: 1.14217, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:26:42.161726 Training: [13 epoch,  50 batch] loss: 1.16270, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:29:45.512480 Training: [13 epoch,  60 batch] loss: 1.14907, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:32:34.813390 Training: [13 epoch,  70 batch] loss: 1.10856, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:35:21.729570 Training: [13 epoch,  80 batch] loss: 1.25136, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:38:19.037385 Training: [13 epoch,  90 batch] loss: 1.15928, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:41:11.566030 Training: [13 epoch, 100 batch] loss: 1.19360, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:44:06.518164 Training: [13 epoch, 110 batch] loss: 1.18018, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:46:56.968534 Training: [13 epoch, 120 batch] loss: 1.20803, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:49:43.167451 Training: [13 epoch, 130 batch] loss: 1.21954, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:52:32.694667 Training: [13 epoch, 140 batch] loss: 1.21439, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:55:30.265860 Training: [13 epoch, 150 batch] loss: 1.20384, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 03:58:25.007565 Training: [13 epoch, 160 batch] loss: 1.16857, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 04:01:14.509496 Training: [13 epoch, 170 batch] loss: 1.22928, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 04:04:01.039894 Training: [13 epoch, 180 batch] loss: 1.11201, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 04:06:51.676141 Training: [13 epoch, 190 batch] loss: 1.16604, the best RMSE/MAE: 11.31558 / 8.94128
2021-01-11 04:09:42.913477 Training: [13 epoch, 200 batch] loss: 1.16895, the best RMSE/MAE: 11.31558 / 8.94128
<Test> RMSE：9.04201,MAE：7.19650
2021-01-11 04:20:53.134101 Training: [14 epoch,  10 batch] loss: 1.10112, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:23:43.301345 Training: [14 epoch,  20 batch] loss: 1.11968, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:26:37.723104 Training: [14 epoch,  30 batch] loss: 1.18741, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:29:26.263244 Training: [14 epoch,  40 batch] loss: 1.16322, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:32:13.833355 Training: [14 epoch,  50 batch] loss: 1.11248, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:35:02.438367 Training: [14 epoch,  60 batch] loss: 1.13602, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:37:56.311036 Training: [14 epoch,  70 batch] loss: 1.09841, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:40:50.830667 Training: [14 epoch,  80 batch] loss: 1.17373, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:43:42.185011 Training: [14 epoch,  90 batch] loss: 1.10362, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:46:28.795210 Training: [14 epoch, 100 batch] loss: 1.19319, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:49:16.182512 Training: [14 epoch, 110 batch] loss: 1.17347, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:52:06.898494 Training: [14 epoch, 120 batch] loss: 1.16006, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:55:00.603015 Training: [14 epoch, 130 batch] loss: 1.22898, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 04:57:51.065638 Training: [14 epoch, 140 batch] loss: 1.11409, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:00:38.875660 Training: [14 epoch, 150 batch] loss: 1.13708, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:03:26.305072 Training: [14 epoch, 160 batch] loss: 1.17218, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:06:18.078017 Training: [14 epoch, 170 batch] loss: 1.15659, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:09:11.150361 Training: [14 epoch, 180 batch] loss: 1.15376, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:12:04.975519 Training: [14 epoch, 190 batch] loss: 1.13053, the best RMSE/MAE: 9.04201 / 7.19650
2021-01-11 05:14:53.771163 Training: [14 epoch, 200 batch] loss: 1.13957, the best RMSE/MAE: 9.04201 / 7.19650
<Test> RMSE：7.67632,MAE：6.17422
2021-01-11 05:26:12.583342 Training: [15 epoch,  10 batch] loss: 1.17609, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:28:58.852012 Training: [15 epoch,  20 batch] loss: 1.11883, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:31:46.915188 Training: [15 epoch,  30 batch] loss: 1.06746, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:34:36.250042 Training: [15 epoch,  40 batch] loss: 1.08869, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:37:33.253087 Training: [15 epoch,  50 batch] loss: 1.10055, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:40:22.605592 Training: [15 epoch,  60 batch] loss: 1.13936, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:43:10.748880 Training: [15 epoch,  70 batch] loss: 1.17505, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:45:58.364988 Training: [15 epoch,  80 batch] loss: 1.13324, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:48:52.252550 Training: [15 epoch,  90 batch] loss: 1.07856, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:51:49.270842 Training: [15 epoch, 100 batch] loss: 1.10424, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:54:43.485149 Training: [15 epoch, 110 batch] loss: 1.16443, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 05:57:33.130405 Training: [15 epoch, 120 batch] loss: 1.07058, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:00:06.134564 Training: [15 epoch, 130 batch] loss: 1.13962, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:02:36.072069 Training: [15 epoch, 140 batch] loss: 1.22336, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:05:08.620787 Training: [15 epoch, 150 batch] loss: 1.14524, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:07:39.763099 Training: [15 epoch, 160 batch] loss: 1.06555, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:10:09.214078 Training: [15 epoch, 170 batch] loss: 1.04599, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:12:37.428886 Training: [15 epoch, 180 batch] loss: 1.11495, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:15:07.031391 Training: [15 epoch, 190 batch] loss: 1.11254, the best RMSE/MAE: 7.67632 / 6.17422
2021-01-11 06:17:39.906354 Training: [15 epoch, 200 batch] loss: 1.14632, the best RMSE/MAE: 7.67632 / 6.17422
<Test> RMSE：6.28882,MAE：5.05515
2021-01-11 06:27:02.605693 Training: [16 epoch,  10 batch] loss: 1.02939, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:29:33.474252 Training: [16 epoch,  20 batch] loss: 1.06881, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:32:03.847504 Training: [16 epoch,  30 batch] loss: 1.15638, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:34:32.486949 Training: [16 epoch,  40 batch] loss: 1.14672, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:36:59.698769 Training: [16 epoch,  50 batch] loss: 1.13614, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:39:27.543403 Training: [16 epoch,  60 batch] loss: 1.07725, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:41:57.479804 Training: [16 epoch,  70 batch] loss: 1.10669, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:44:27.386097 Training: [16 epoch,  80 batch] loss: 1.11137, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:46:55.997933 Training: [16 epoch,  90 batch] loss: 1.09106, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:49:22.220367 Training: [16 epoch, 100 batch] loss: 1.05393, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:51:51.286905 Training: [16 epoch, 110 batch] loss: 1.12582, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:54:22.492055 Training: [16 epoch, 120 batch] loss: 1.08347, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:56:52.202560 Training: [16 epoch, 130 batch] loss: 1.07280, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 06:59:07.232624 Training: [16 epoch, 140 batch] loss: 1.08553, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:01:14.771357 Training: [16 epoch, 150 batch] loss: 1.09817, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:03:25.908523 Training: [16 epoch, 160 batch] loss: 1.08928, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:05:38.178464 Training: [16 epoch, 170 batch] loss: 1.07785, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:07:50.904710 Training: [16 epoch, 180 batch] loss: 1.03917, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:10:02.510723 Training: [16 epoch, 190 batch] loss: 1.15191, the best RMSE/MAE: 6.28882 / 5.05515
2021-01-11 07:11:52.299235 Training: [16 epoch, 200 batch] loss: 1.11533, the best RMSE/MAE: 6.28882 / 5.05515
<Test> RMSE：4.07314,MAE：3.30551
2021-01-11 07:18:23.054702 Training: [17 epoch,  10 batch] loss: 1.10263, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:20:04.274127 Training: [17 epoch,  20 batch] loss: 1.05071, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:21:30.208725 Training: [17 epoch,  30 batch] loss: 1.07086, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:22:58.751026 Training: [17 epoch,  40 batch] loss: 1.03642, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:24:26.118310 Training: [17 epoch,  50 batch] loss: 1.09896, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:25:54.028648 Training: [17 epoch,  60 batch] loss: 1.03226, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:27:21.777431 Training: [17 epoch,  70 batch] loss: 1.09809, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:28:47.843090 Training: [17 epoch,  80 batch] loss: 1.10174, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:30:15.823667 Training: [17 epoch,  90 batch] loss: 1.16288, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:31:43.404602 Training: [17 epoch, 100 batch] loss: 1.04987, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:33:11.509199 Training: [17 epoch, 110 batch] loss: 1.13316, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:34:36.988845 Training: [17 epoch, 120 batch] loss: 1.12923, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:36:03.576553 Training: [17 epoch, 130 batch] loss: 1.08490, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:37:30.849063 Training: [17 epoch, 140 batch] loss: 1.02599, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:38:58.375063 Training: [17 epoch, 150 batch] loss: 1.07502, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:40:26.231283 Training: [17 epoch, 160 batch] loss: 1.12000, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:41:52.476777 Training: [17 epoch, 170 batch] loss: 1.06438, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:43:20.524747 Training: [17 epoch, 180 batch] loss: 1.15255, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:44:48.527325 Training: [17 epoch, 190 batch] loss: 1.11596, the best RMSE/MAE: 4.07314 / 3.30551
2021-01-11 07:46:17.859426 Training: [17 epoch, 200 batch] loss: 1.01517, the best RMSE/MAE: 4.07314 / 3.30551
<Test> RMSE：2.33527,MAE：1.88256
2021-01-11 07:51:11.205335 Training: [18 epoch,  10 batch] loss: 1.07365, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:52:38.138321 Training: [18 epoch,  20 batch] loss: 1.04082, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:54:04.330057 Training: [18 epoch,  30 batch] loss: 1.01007, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:55:31.014548 Training: [18 epoch,  40 batch] loss: 1.13848, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:56:58.011161 Training: [18 epoch,  50 batch] loss: 1.12108, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:58:25.273317 Training: [18 epoch,  60 batch] loss: 1.05208, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 07:59:53.456219 Training: [18 epoch,  70 batch] loss: 1.09574, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:01:19.644000 Training: [18 epoch,  80 batch] loss: 1.08572, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:02:47.797261 Training: [18 epoch,  90 batch] loss: 1.05813, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:04:16.134852 Training: [18 epoch, 100 batch] loss: 1.04917, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:05:43.949726 Training: [18 epoch, 110 batch] loss: 1.04225, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:07:11.702422 Training: [18 epoch, 120 batch] loss: 1.10057, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:08:37.737544 Training: [18 epoch, 130 batch] loss: 1.08055, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:10:06.068220 Training: [18 epoch, 140 batch] loss: 1.12075, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:11:33.548327 Training: [18 epoch, 150 batch] loss: 1.05300, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:13:01.054151 Training: [18 epoch, 160 batch] loss: 1.09877, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:14:27.244130 Training: [18 epoch, 170 batch] loss: 1.02158, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:15:55.567256 Training: [18 epoch, 180 batch] loss: 1.01520, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:17:24.207619 Training: [18 epoch, 190 batch] loss: 1.08002, the best RMSE/MAE: 2.33527 / 1.88256
2021-01-11 08:18:51.568578 Training: [18 epoch, 200 batch] loss: 1.06838, the best RMSE/MAE: 2.33527 / 1.88256
<Test> RMSE：1.78947,MAE：1.46660
2021-01-11 08:23:44.510238 Training: [19 epoch,  10 batch] loss: 1.04554, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:25:11.170359 Training: [19 epoch,  20 batch] loss: 1.07936, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:26:37.458682 Training: [19 epoch,  30 batch] loss: 1.06860, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:27:55.100887 Training: [19 epoch,  40 batch] loss: 1.02973, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:28:57.875743 Training: [19 epoch,  50 batch] loss: 1.07969, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:29:59.055321 Training: [19 epoch,  60 batch] loss: 1.04326, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:31:00.443174 Training: [19 epoch,  70 batch] loss: 1.09626, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:32:02.044268 Training: [19 epoch,  80 batch] loss: 1.08541, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:33:04.850840 Training: [19 epoch,  90 batch] loss: 1.05999, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:34:07.151340 Training: [19 epoch, 100 batch] loss: 1.06401, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:35:09.310212 Training: [19 epoch, 110 batch] loss: 1.02522, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:36:11.799142 Training: [19 epoch, 120 batch] loss: 1.05881, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:37:13.633030 Training: [19 epoch, 130 batch] loss: 1.02744, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:38:15.898781 Training: [19 epoch, 140 batch] loss: 1.06666, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:39:17.688724 Training: [19 epoch, 150 batch] loss: 1.08579, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:40:21.052821 Training: [19 epoch, 160 batch] loss: 1.03844, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:41:23.385730 Training: [19 epoch, 170 batch] loss: 1.03820, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:42:25.247639 Training: [19 epoch, 180 batch] loss: 1.09826, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:43:27.323966 Training: [19 epoch, 190 batch] loss: 1.07676, the best RMSE/MAE: 1.78947 / 1.46660
2021-01-11 08:44:29.306868 Training: [19 epoch, 200 batch] loss: 1.08078, the best RMSE/MAE: 1.78947 / 1.46660
<Test> RMSE：1.44709,MAE：1.17484
2021-01-11 08:47:30.312141 Training: [20 epoch,  10 batch] loss: 1.06473, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:48:31.492472 Training: [20 epoch,  20 batch] loss: 1.04607, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:49:32.998316 Training: [20 epoch,  30 batch] loss: 1.02509, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:50:35.390011 Training: [20 epoch,  40 batch] loss: 1.01068, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:51:36.724974 Training: [20 epoch,  50 batch] loss: 1.03666, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:52:38.333194 Training: [20 epoch,  60 batch] loss: 1.02658, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:53:40.127264 Training: [20 epoch,  70 batch] loss: 1.01997, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:54:42.337292 Training: [20 epoch,  80 batch] loss: 1.11865, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:55:45.184735 Training: [20 epoch,  90 batch] loss: 1.09136, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:56:48.085292 Training: [20 epoch, 100 batch] loss: 1.01974, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:57:50.619145 Training: [20 epoch, 110 batch] loss: 1.11502, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:58:52.551721 Training: [20 epoch, 120 batch] loss: 1.06310, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 08:59:54.788213 Training: [20 epoch, 130 batch] loss: 1.04747, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:00:56.940424 Training: [20 epoch, 140 batch] loss: 1.06333, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:01:59.082093 Training: [20 epoch, 150 batch] loss: 1.03326, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:03:01.045765 Training: [20 epoch, 160 batch] loss: 1.08842, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:04:04.064112 Training: [20 epoch, 170 batch] loss: 1.04938, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:05:06.773127 Training: [20 epoch, 180 batch] loss: 1.07377, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:06:09.911684 Training: [20 epoch, 190 batch] loss: 1.04533, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:07:12.422583 Training: [20 epoch, 200 batch] loss: 1.06224, the best RMSE/MAE: 1.44709 / 1.17484
<Test> RMSE：1.46281,MAE：1.20068
2021-01-11 09:10:13.300791 Training: [21 epoch,  10 batch] loss: 1.04058, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:11:15.005457 Training: [21 epoch,  20 batch] loss: 1.04705, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:12:16.557910 Training: [21 epoch,  30 batch] loss: 1.00652, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:13:18.459224 Training: [21 epoch,  40 batch] loss: 1.07144, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:14:21.059269 Training: [21 epoch,  50 batch] loss: 1.02751, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:15:23.931819 Training: [21 epoch,  60 batch] loss: 1.05083, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:16:26.899561 Training: [21 epoch,  70 batch] loss: 1.06057, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:17:29.874479 Training: [21 epoch,  80 batch] loss: 1.05507, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:18:32.246316 Training: [21 epoch,  90 batch] loss: 1.04806, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:19:34.782280 Training: [21 epoch, 100 batch] loss: 1.04579, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:20:36.566512 Training: [21 epoch, 110 batch] loss: 1.11497, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:21:38.256026 Training: [21 epoch, 120 batch] loss: 1.04070, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:22:40.777997 Training: [21 epoch, 130 batch] loss: 1.00741, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:23:42.940645 Training: [21 epoch, 140 batch] loss: 1.02468, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:24:45.136580 Training: [21 epoch, 150 batch] loss: 1.04047, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:25:48.251787 Training: [21 epoch, 160 batch] loss: 1.02877, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:26:51.277311 Training: [21 epoch, 170 batch] loss: 1.04223, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:27:53.952432 Training: [21 epoch, 180 batch] loss: 1.05726, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:28:56.549345 Training: [21 epoch, 190 batch] loss: 1.06279, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:29:59.007618 Training: [21 epoch, 200 batch] loss: 1.05609, the best RMSE/MAE: 1.44709 / 1.17484
<Test> RMSE：1.49731,MAE：1.21900
2021-01-11 09:33:00.484635 Training: [22 epoch,  10 batch] loss: 1.04276, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:34:01.530072 Training: [22 epoch,  20 batch] loss: 1.01836, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:35:02.998580 Training: [22 epoch,  30 batch] loss: 1.10266, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:36:04.619983 Training: [22 epoch,  40 batch] loss: 1.03206, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:37:06.349807 Training: [22 epoch,  50 batch] loss: 1.02244, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:38:08.266884 Training: [22 epoch,  60 batch] loss: 1.00378, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:39:10.070707 Training: [22 epoch,  70 batch] loss: 1.03242, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:40:12.292814 Training: [22 epoch,  80 batch] loss: 1.04737, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:41:14.074003 Training: [22 epoch,  90 batch] loss: 1.04506, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:42:16.859438 Training: [22 epoch, 100 batch] loss: 1.01318, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:43:19.275688 Training: [22 epoch, 110 batch] loss: 1.06168, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:44:21.400459 Training: [22 epoch, 120 batch] loss: 1.07044, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:45:23.796953 Training: [22 epoch, 130 batch] loss: 1.04776, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:46:26.220510 Training: [22 epoch, 140 batch] loss: 1.07873, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:47:28.941237 Training: [22 epoch, 150 batch] loss: 1.04453, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:48:31.413440 Training: [22 epoch, 160 batch] loss: 1.05541, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:49:35.611255 Training: [22 epoch, 170 batch] loss: 1.08019, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:50:37.799278 Training: [22 epoch, 180 batch] loss: 0.98317, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:51:39.734975 Training: [22 epoch, 190 batch] loss: 1.02501, the best RMSE/MAE: 1.44709 / 1.17484
2021-01-11 09:52:42.394855 Training: [22 epoch, 200 batch] loss: 1.06201, the best RMSE/MAE: 1.44709 / 1.17484
<Test> RMSE：2.10954,MAE：1.82752
