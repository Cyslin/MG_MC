-------------------- Hyperparams --------------------
time: 2021-01-04 13:23:59.685201
Dataset: amazon
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 64
use_cuda: True
2021-01-04 13:36:11.772569 Training: [1 epoch,  10 batch] loss: 29.51175, the best RMSE/MAE: inf / inf
2021-01-04 13:37:05.743370 Training: [1 epoch,  20 batch] loss: 27.28878, the best RMSE/MAE: inf / inf
2021-01-04 13:37:58.862874 Training: [1 epoch,  30 batch] loss: 25.33729, the best RMSE/MAE: inf / inf
2021-01-04 13:38:53.004912 Training: [1 epoch,  40 batch] loss: 23.76982, the best RMSE/MAE: inf / inf
2021-01-04 13:39:47.821371 Training: [1 epoch,  50 batch] loss: 22.45723, the best RMSE/MAE: inf / inf
2021-01-04 13:40:43.231881 Training: [1 epoch,  60 batch] loss: 20.79451, the best RMSE/MAE: inf / inf
2021-01-04 13:41:37.734073 Training: [1 epoch,  70 batch] loss: 19.73432, the best RMSE/MAE: inf / inf
2021-01-04 13:42:32.298004 Training: [1 epoch,  80 batch] loss: 18.89829, the best RMSE/MAE: inf / inf
2021-01-04 13:43:26.114977 Training: [1 epoch,  90 batch] loss: 18.19809, the best RMSE/MAE: inf / inf
2021-01-04 13:44:20.592625 Training: [1 epoch, 100 batch] loss: 17.66718, the best RMSE/MAE: inf / inf
2021-01-04 13:45:16.118982 Training: [1 epoch, 110 batch] loss: 17.24133, the best RMSE/MAE: inf / inf
2021-01-04 13:46:10.930654 Training: [1 epoch, 120 batch] loss: 16.81444, the best RMSE/MAE: inf / inf
2021-01-04 13:47:05.167373 Training: [1 epoch, 130 batch] loss: 16.50150, the best RMSE/MAE: inf / inf
2021-01-04 13:47:59.519632 Training: [1 epoch, 140 batch] loss: 16.48906, the best RMSE/MAE: inf / inf
2021-01-04 13:48:54.462728 Training: [1 epoch, 150 batch] loss: 16.29625, the best RMSE/MAE: inf / inf
2021-01-04 13:49:50.081746 Training: [1 epoch, 160 batch] loss: 16.24358, the best RMSE/MAE: inf / inf
2021-01-04 13:50:45.453560 Training: [1 epoch, 170 batch] loss: 16.18191, the best RMSE/MAE: inf / inf
2021-01-04 13:51:39.891080 Training: [1 epoch, 180 batch] loss: 16.11747, the best RMSE/MAE: inf / inf
2021-01-04 13:52:34.117654 Training: [1 epoch, 190 batch] loss: 16.10158, the best RMSE/MAE: inf / inf
2021-01-04 13:53:28.750088 Training: [1 epoch, 200 batch] loss: 16.06934, the best RMSE/MAE: inf / inf
<Test> RMSE：1407559.87500,MAE：1277171.25000
2021-01-04 13:56:41.286731 Training: [2 epoch,  10 batch] loss: 16.08960, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 13:57:35.807627 Training: [2 epoch,  20 batch] loss: 15.90749, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 13:58:30.001883 Training: [2 epoch,  30 batch] loss: 15.85855, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 13:59:25.934008 Training: [2 epoch,  40 batch] loss: 15.99075, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:00:22.361575 Training: [2 epoch,  50 batch] loss: 15.88080, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:01:17.922603 Training: [2 epoch,  60 batch] loss: 15.92155, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:02:12.795573 Training: [2 epoch,  70 batch] loss: 15.83936, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:03:07.889708 Training: [2 epoch,  80 batch] loss: 15.81530, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:04:03.926550 Training: [2 epoch,  90 batch] loss: 15.75738, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:05:00.147233 Training: [2 epoch, 100 batch] loss: 15.72919, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:05:56.530614 Training: [2 epoch, 110 batch] loss: 15.75453, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:06:52.462300 Training: [2 epoch, 120 batch] loss: 15.68442, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:07:47.705272 Training: [2 epoch, 130 batch] loss: 15.75993, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:08:42.755364 Training: [2 epoch, 140 batch] loss: 15.52646, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:09:37.730171 Training: [2 epoch, 150 batch] loss: 15.69080, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:10:32.796590 Training: [2 epoch, 160 batch] loss: 15.60219, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:11:27.847469 Training: [2 epoch, 170 batch] loss: 15.59316, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:12:23.055897 Training: [2 epoch, 180 batch] loss: 15.53592, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:13:18.925172 Training: [2 epoch, 190 batch] loss: 15.46169, the best RMSE/MAE: 1407559.87500 / 1277171.25000
2021-01-04 14:14:14.373782 Training: [2 epoch, 200 batch] loss: 15.42289, the best RMSE/MAE: 1407559.87500 / 1277171.25000
<Test> RMSE：4460.30273,MAE：4081.11279
2021-01-04 14:17:27.522475 Training: [3 epoch,  10 batch] loss: 15.38911, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:18:21.781673 Training: [3 epoch,  20 batch] loss: 15.34079, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:19:17.520951 Training: [3 epoch,  30 batch] loss: 15.36774, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:20:13.776294 Training: [3 epoch,  40 batch] loss: 15.38675, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:21:09.444591 Training: [3 epoch,  50 batch] loss: 15.29531, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:22:04.490470 Training: [3 epoch,  60 batch] loss: 15.24170, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:22:59.317034 Training: [3 epoch,  70 batch] loss: 15.35567, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:23:54.373630 Training: [3 epoch,  80 batch] loss: 15.24495, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:24:50.341653 Training: [3 epoch,  90 batch] loss: 15.23273, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:25:46.610431 Training: [3 epoch, 100 batch] loss: 15.14024, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:26:42.521768 Training: [3 epoch, 110 batch] loss: 15.17323, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:27:37.137664 Training: [3 epoch, 120 batch] loss: 15.11769, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:28:31.982873 Training: [3 epoch, 130 batch] loss: 15.09266, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:29:28.387480 Training: [3 epoch, 140 batch] loss: 15.07568, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:30:24.698358 Training: [3 epoch, 150 batch] loss: 14.99416, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:31:19.626015 Training: [3 epoch, 160 batch] loss: 15.01586, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:32:14.478540 Training: [3 epoch, 170 batch] loss: 14.98648, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:33:09.880819 Training: [3 epoch, 180 batch] loss: 14.99321, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:34:05.555099 Training: [3 epoch, 190 batch] loss: 14.92666, the best RMSE/MAE: 4460.30273 / 4081.11279
2021-01-04 14:35:00.982370 Training: [3 epoch, 200 batch] loss: 14.96852, the best RMSE/MAE: 4460.30273 / 4081.11279
<Test> RMSE：148.75865,MAE：136.17444
2021-01-04 14:38:13.831291 Training: [4 epoch,  10 batch] loss: 14.85117, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:39:06.732069 Training: [4 epoch,  20 batch] loss: 14.85640, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:40:01.849141 Training: [4 epoch,  30 batch] loss: 14.75261, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:40:56.515579 Training: [4 epoch,  40 batch] loss: 14.75500, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:41:50.210035 Training: [4 epoch,  50 batch] loss: 14.78266, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:42:45.640109 Training: [4 epoch,  60 batch] loss: 14.69262, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:43:40.725843 Training: [4 epoch,  70 batch] loss: 14.82137, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:44:34.771124 Training: [4 epoch,  80 batch] loss: 14.61675, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:45:30.274552 Training: [4 epoch,  90 batch] loss: 14.60702, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:46:24.942703 Training: [4 epoch, 100 batch] loss: 14.55400, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:47:19.280133 Training: [4 epoch, 110 batch] loss: 14.49254, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:48:15.337810 Training: [4 epoch, 120 batch] loss: 14.57900, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:49:09.500281 Training: [4 epoch, 130 batch] loss: 14.59915, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:50:04.186880 Training: [4 epoch, 140 batch] loss: 14.47671, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:50:59.875751 Training: [4 epoch, 150 batch] loss: 14.42078, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:51:53.936863 Training: [4 epoch, 160 batch] loss: 14.43871, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:52:49.399208 Training: [4 epoch, 170 batch] loss: 14.36031, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:53:44.209641 Training: [4 epoch, 180 batch] loss: 14.36863, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:54:38.446973 Training: [4 epoch, 190 batch] loss: 14.37607, the best RMSE/MAE: 148.75865 / 136.17444
2021-01-04 14:55:34.270363 Training: [4 epoch, 200 batch] loss: 14.33046, the best RMSE/MAE: 148.75865 / 136.17444
<Test> RMSE：19.46413,MAE：18.14550
2021-01-04 14:58:44.386309 Training: [5 epoch,  10 batch] loss: 14.25720, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 14:59:37.107557 Training: [5 epoch,  20 batch] loss: 14.25655, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:00:32.806815 Training: [5 epoch,  30 batch] loss: 14.13367, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:01:28.535026 Training: [5 epoch,  40 batch] loss: 14.18523, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:02:23.073301 Training: [5 epoch,  50 batch] loss: 14.10794, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:03:16.971264 Training: [5 epoch,  60 batch] loss: 14.22490, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:04:12.033585 Training: [5 epoch,  70 batch] loss: 14.10346, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:05:07.429367 Training: [5 epoch,  80 batch] loss: 14.10704, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:06:02.341443 Training: [5 epoch,  90 batch] loss: 13.95970, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:06:56.097340 Training: [5 epoch, 100 batch] loss: 14.00767, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:07:51.719196 Training: [5 epoch, 110 batch] loss: 14.00771, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:08:46.701008 Training: [5 epoch, 120 batch] loss: 13.96439, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:09:40.542933 Training: [5 epoch, 130 batch] loss: 13.91757, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:10:36.419001 Training: [5 epoch, 140 batch] loss: 13.87597, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:11:31.186226 Training: [5 epoch, 150 batch] loss: 13.81167, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:12:25.137085 Training: [5 epoch, 160 batch] loss: 13.78435, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:13:20.917651 Training: [5 epoch, 170 batch] loss: 13.82606, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:14:16.033682 Training: [5 epoch, 180 batch] loss: 13.73184, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:15:10.270229 Training: [5 epoch, 190 batch] loss: 13.75492, the best RMSE/MAE: 19.46413 / 18.14550
2021-01-04 15:16:05.183896 Training: [5 epoch, 200 batch] loss: 13.70759, the best RMSE/MAE: 19.46413 / 18.14550
<Test> RMSE：5.08804,MAE：4.66318
2021-01-04 15:19:22.120898 Training: [6 epoch,  10 batch] loss: 13.62159, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:20:16.168379 Training: [6 epoch,  20 batch] loss: 13.59170, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:21:09.933590 Training: [6 epoch,  30 batch] loss: 13.55108, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:22:04.778907 Training: [6 epoch,  40 batch] loss: 13.56841, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:22:59.574447 Training: [6 epoch,  50 batch] loss: 13.48465, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:23:55.166870 Training: [6 epoch,  60 batch] loss: 13.51443, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:24:50.580825 Training: [6 epoch,  70 batch] loss: 13.40651, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:25:46.161127 Training: [6 epoch,  80 batch] loss: 13.52061, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:26:41.877617 Training: [6 epoch,  90 batch] loss: 13.34583, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:27:37.717049 Training: [6 epoch, 100 batch] loss: 13.40478, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:28:32.675920 Training: [6 epoch, 110 batch] loss: 13.39552, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:29:27.606911 Training: [6 epoch, 120 batch] loss: 13.28542, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:30:23.558187 Training: [6 epoch, 130 batch] loss: 13.24816, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:31:18.861404 Training: [6 epoch, 140 batch] loss: 13.24039, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:32:14.053073 Training: [6 epoch, 150 batch] loss: 13.14550, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:33:10.052558 Training: [6 epoch, 160 batch] loss: 13.18194, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:34:05.968549 Training: [6 epoch, 170 batch] loss: 13.08218, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:35:01.155645 Training: [6 epoch, 180 batch] loss: 13.05758, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:35:56.366140 Training: [6 epoch, 190 batch] loss: 13.06645, the best RMSE/MAE: 5.08804 / 4.66318
2021-01-04 15:36:52.554067 Training: [6 epoch, 200 batch] loss: 12.99499, the best RMSE/MAE: 5.08804 / 4.66318
<Test> RMSE：2.02274,MAE：1.71252
2021-01-04 15:40:07.840917 Training: [7 epoch,  10 batch] loss: 12.98266, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:41:01.165022 Training: [7 epoch,  20 batch] loss: 12.91636, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:41:54.457973 Training: [7 epoch,  30 batch] loss: 12.97011, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:42:47.677881 Training: [7 epoch,  40 batch] loss: 12.85855, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:43:40.794804 Training: [7 epoch,  50 batch] loss: 12.81064, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:44:34.097627 Training: [7 epoch,  60 batch] loss: 12.73886, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:45:28.976893 Training: [7 epoch,  70 batch] loss: 12.76363, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:46:22.781606 Training: [7 epoch,  80 batch] loss: 12.66812, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:47:16.689478 Training: [7 epoch,  90 batch] loss: 12.72252, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:48:11.412935 Training: [7 epoch, 100 batch] loss: 12.66215, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:49:05.933928 Training: [7 epoch, 110 batch] loss: 12.60174, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:50:00.174067 Training: [7 epoch, 120 batch] loss: 12.57630, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:50:54.747598 Training: [7 epoch, 130 batch] loss: 12.55562, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:51:48.976577 Training: [7 epoch, 140 batch] loss: 12.50346, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:52:43.357965 Training: [7 epoch, 150 batch] loss: 12.49254, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:53:37.800230 Training: [7 epoch, 160 batch] loss: 12.51385, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:54:31.874292 Training: [7 epoch, 170 batch] loss: 12.43059, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:55:26.628927 Training: [7 epoch, 180 batch] loss: 12.45742, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:56:20.794125 Training: [7 epoch, 190 batch] loss: 12.45761, the best RMSE/MAE: 2.02274 / 1.71252
2021-01-04 15:57:16.341340 Training: [7 epoch, 200 batch] loss: 12.29698, the best RMSE/MAE: 2.02274 / 1.71252
<Test> RMSE：1.20894,MAE：0.93743
2021-01-04 16:00:31.986030 Training: [8 epoch,  10 batch] loss: 12.22197, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:01:27.058894 Training: [8 epoch,  20 batch] loss: 12.17898, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:02:22.129050 Training: [8 epoch,  30 batch] loss: 12.21359, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:03:16.338042 Training: [8 epoch,  40 batch] loss: 12.11812, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:04:10.050987 Training: [8 epoch,  50 batch] loss: 12.06826, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:05:04.131428 Training: [8 epoch,  60 batch] loss: 12.01616, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:05:58.813971 Training: [8 epoch,  70 batch] loss: 12.03982, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:06:54.450700 Training: [8 epoch,  80 batch] loss: 11.91814, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:07:50.134332 Training: [8 epoch,  90 batch] loss: 12.02154, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:08:44.976926 Training: [8 epoch, 100 batch] loss: 11.92890, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:09:39.476076 Training: [8 epoch, 110 batch] loss: 11.93517, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:10:33.253511 Training: [8 epoch, 120 batch] loss: 11.82724, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:11:26.478601 Training: [8 epoch, 130 batch] loss: 11.83846, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:12:21.753038 Training: [8 epoch, 140 batch] loss: 11.73951, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:13:17.130645 Training: [8 epoch, 150 batch] loss: 11.77487, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:14:16.082418 Training: [8 epoch, 160 batch] loss: 11.69953, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:15:12.209826 Training: [8 epoch, 170 batch] loss: 11.69500, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:16:08.401444 Training: [8 epoch, 180 batch] loss: 11.69149, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:17:04.002524 Training: [8 epoch, 190 batch] loss: 11.62854, the best RMSE/MAE: 1.20894 / 0.93743
2021-01-04 16:17:59.337710 Training: [8 epoch, 200 batch] loss: 11.63606, the best RMSE/MAE: 1.20894 / 0.93743
<Test> RMSE：1.00704,MAE：0.74610
2021-01-04 16:21:15.211125 Training: [9 epoch,  10 batch] loss: 11.52321, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:22:10.737621 Training: [9 epoch,  20 batch] loss: 11.48141, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:23:05.654074 Training: [9 epoch,  30 batch] loss: 11.52904, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:24:00.213783 Training: [9 epoch,  40 batch] loss: 11.33989, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:24:54.142554 Training: [9 epoch,  50 batch] loss: 11.36626, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:25:48.036779 Training: [9 epoch,  60 batch] loss: 11.27215, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:26:42.452912 Training: [9 epoch,  70 batch] loss: 11.31452, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:27:38.174878 Training: [9 epoch,  80 batch] loss: 11.31155, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:28:34.369863 Training: [9 epoch,  90 batch] loss: 11.24578, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:29:30.771206 Training: [9 epoch, 100 batch] loss: 11.18690, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:30:26.697317 Training: [9 epoch, 110 batch] loss: 11.14748, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:31:21.409219 Training: [9 epoch, 120 batch] loss: 11.13142, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:32:17.515462 Training: [9 epoch, 130 batch] loss: 11.05623, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:33:12.762101 Training: [9 epoch, 140 batch] loss: 11.03729, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:34:08.473648 Training: [9 epoch, 150 batch] loss: 10.94329, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:35:03.903694 Training: [9 epoch, 160 batch] loss: 10.94759, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:35:59.357769 Training: [9 epoch, 170 batch] loss: 10.90721, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:36:54.764525 Training: [9 epoch, 180 batch] loss: 10.92343, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:37:51.047410 Training: [9 epoch, 190 batch] loss: 10.79080, the best RMSE/MAE: 1.00704 / 0.74610
2021-01-04 16:38:46.483267 Training: [9 epoch, 200 batch] loss: 10.72069, the best RMSE/MAE: 1.00704 / 0.74610
<Test> RMSE：0.99950,MAE：0.73946
2021-01-04 16:42:03.525127 Training: [10 epoch,  10 batch] loss: 10.72737, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:42:58.567156 Training: [10 epoch,  20 batch] loss: 10.79506, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:43:53.629442 Training: [10 epoch,  30 batch] loss: 10.62949, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:44:49.059293 Training: [10 epoch,  40 batch] loss: 10.64464, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:45:44.242173 Training: [10 epoch,  50 batch] loss: 10.59172, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:46:39.122199 Training: [10 epoch,  60 batch] loss: 10.51944, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:47:33.290397 Training: [10 epoch,  70 batch] loss: 10.46157, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:48:27.882018 Training: [10 epoch,  80 batch] loss: 10.48846, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:49:22.889416 Training: [10 epoch,  90 batch] loss: 10.42995, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:50:19.431055 Training: [10 epoch, 100 batch] loss: 10.40987, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:51:15.555478 Training: [10 epoch, 110 batch] loss: 10.42119, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:52:11.015592 Training: [10 epoch, 120 batch] loss: 10.20660, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:53:06.911408 Training: [10 epoch, 130 batch] loss: 10.29883, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:54:03.249890 Training: [10 epoch, 140 batch] loss: 10.21221, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:54:59.545430 Training: [10 epoch, 150 batch] loss: 10.19826, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:55:55.078810 Training: [10 epoch, 160 batch] loss: 10.17522, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:56:49.923780 Training: [10 epoch, 170 batch] loss: 10.15830, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:57:44.829535 Training: [10 epoch, 180 batch] loss: 10.10627, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:58:40.247521 Training: [10 epoch, 190 batch] loss: 10.11583, the best RMSE/MAE: 0.99950 / 0.73946
2021-01-04 16:59:35.919125 Training: [10 epoch, 200 batch] loss: 10.04836, the best RMSE/MAE: 0.99950 / 0.73946
<Test> RMSE：0.97075,MAE：0.72069
2021-01-04 17:02:52.426793 Training: [11 epoch,  10 batch] loss: 9.98367, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:03:47.898129 Training: [11 epoch,  20 batch] loss: 9.98592, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:04:42.482210 Training: [11 epoch,  30 batch] loss: 9.94537, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:05:37.574675 Training: [11 epoch,  40 batch] loss: 9.90321, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:06:33.526681 Training: [11 epoch,  50 batch] loss: 9.86614, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:07:29.183219 Training: [11 epoch,  60 batch] loss: 9.76366, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:08:24.245429 Training: [11 epoch,  70 batch] loss: 9.73477, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:09:18.863484 Training: [11 epoch,  80 batch] loss: 9.65539, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:10:13.263428 Training: [11 epoch,  90 batch] loss: 9.70740, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:11:08.343292 Training: [11 epoch, 100 batch] loss: 9.61457, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:12:03.718591 Training: [11 epoch, 110 batch] loss: 9.65427, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:12:59.529705 Training: [11 epoch, 120 batch] loss: 9.54400, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:13:55.469054 Training: [11 epoch, 130 batch] loss: 9.51915, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:14:51.481692 Training: [11 epoch, 140 batch] loss: 9.50273, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:15:47.838078 Training: [11 epoch, 150 batch] loss: 9.44235, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:16:44.321807 Training: [11 epoch, 160 batch] loss: 9.41024, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:17:40.970584 Training: [11 epoch, 170 batch] loss: 9.33950, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:18:37.483662 Training: [11 epoch, 180 batch] loss: 9.34633, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:19:33.928165 Training: [11 epoch, 190 batch] loss: 9.41584, the best RMSE/MAE: 0.97075 / 0.72069
2021-01-04 17:20:30.398853 Training: [11 epoch, 200 batch] loss: 9.31749, the best RMSE/MAE: 0.97075 / 0.72069
<Test> RMSE：0.96964,MAE：0.75600
2021-01-04 17:23:45.762321 Training: [12 epoch,  10 batch] loss: 9.24806, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:24:39.784229 Training: [12 epoch,  20 batch] loss: 9.17581, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:25:33.201238 Training: [12 epoch,  30 batch] loss: 9.08345, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:26:27.511737 Training: [12 epoch,  40 batch] loss: 9.15385, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:27:23.133884 Training: [12 epoch,  50 batch] loss: 9.06253, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:28:18.070617 Training: [12 epoch,  60 batch] loss: 9.01426, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:29:12.094166 Training: [12 epoch,  70 batch] loss: 9.01254, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:30:06.169940 Training: [12 epoch,  80 batch] loss: 8.91956, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:31:00.143201 Training: [12 epoch,  90 batch] loss: 8.99038, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:31:53.923066 Training: [12 epoch, 100 batch] loss: 8.93293, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:32:51.329978 Training: [12 epoch, 110 batch] loss: 8.91702, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:33:47.537666 Training: [12 epoch, 120 batch] loss: 8.81669, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:34:42.043340 Training: [12 epoch, 130 batch] loss: 8.79964, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:35:37.186493 Training: [12 epoch, 140 batch] loss: 8.78516, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:36:31.338808 Training: [12 epoch, 150 batch] loss: 8.71674, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:37:25.849247 Training: [12 epoch, 160 batch] loss: 8.63465, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:38:19.360883 Training: [12 epoch, 170 batch] loss: 8.62621, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:39:13.833888 Training: [12 epoch, 180 batch] loss: 8.66656, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:40:08.578325 Training: [12 epoch, 190 batch] loss: 8.57436, the best RMSE/MAE: 0.96964 / 0.75600
2021-01-04 17:41:04.577278 Training: [12 epoch, 200 batch] loss: 8.59656, the best RMSE/MAE: 0.96964 / 0.75600
<Test> RMSE：0.96006,MAE：0.72475
2021-01-04 17:44:20.754744 Training: [13 epoch,  10 batch] loss: 8.48701, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:45:13.828871 Training: [13 epoch,  20 batch] loss: 8.46713, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:46:08.997959 Training: [13 epoch,  30 batch] loss: 8.43333, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:47:03.665894 Training: [13 epoch,  40 batch] loss: 8.38900, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:47:57.776332 Training: [13 epoch,  50 batch] loss: 8.36848, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:48:51.609524 Training: [13 epoch,  60 batch] loss: 8.32536, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:49:46.869014 Training: [13 epoch,  70 batch] loss: 8.27970, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:50:41.723534 Training: [13 epoch,  80 batch] loss: 8.35147, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:51:35.993596 Training: [13 epoch,  90 batch] loss: 8.25720, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:52:30.775141 Training: [13 epoch, 100 batch] loss: 8.17502, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:53:26.929218 Training: [13 epoch, 110 batch] loss: 8.12628, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:54:22.595366 Training: [13 epoch, 120 batch] loss: 8.12386, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:55:18.351790 Training: [13 epoch, 130 batch] loss: 8.11575, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:56:14.589690 Training: [13 epoch, 140 batch] loss: 8.08474, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:57:10.807085 Training: [13 epoch, 150 batch] loss: 8.07966, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:58:07.149647 Training: [13 epoch, 160 batch] loss: 7.98817, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:59:03.401784 Training: [13 epoch, 170 batch] loss: 7.96866, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 17:59:59.861636 Training: [13 epoch, 180 batch] loss: 7.97891, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 18:00:56.263299 Training: [13 epoch, 190 batch] loss: 7.90002, the best RMSE/MAE: 0.96006 / 0.72475
2021-01-04 18:01:52.785715 Training: [13 epoch, 200 batch] loss: 7.89738, the best RMSE/MAE: 0.96006 / 0.72475
<Test> RMSE：0.95721,MAE：0.74710
2021-01-04 18:05:10.973325 Training: [14 epoch,  10 batch] loss: 7.81180, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:06:05.142429 Training: [14 epoch,  20 batch] loss: 7.78591, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:06:59.634080 Training: [14 epoch,  30 batch] loss: 7.75451, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:07:54.225070 Training: [14 epoch,  40 batch] loss: 7.72465, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:08:49.571364 Training: [14 epoch,  50 batch] loss: 7.73332, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:09:45.950233 Training: [14 epoch,  60 batch] loss: 7.63827, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:10:41.551486 Training: [14 epoch,  70 batch] loss: 7.61971, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:11:35.962563 Training: [14 epoch,  80 batch] loss: 7.62666, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:12:29.634198 Training: [14 epoch,  90 batch] loss: 7.65004, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:13:23.098986 Training: [14 epoch, 100 batch] loss: 7.50570, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:14:18.017611 Training: [14 epoch, 110 batch] loss: 7.49981, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:15:13.610351 Training: [14 epoch, 120 batch] loss: 7.47413, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:16:09.499982 Training: [14 epoch, 130 batch] loss: 7.42641, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:17:05.645122 Training: [14 epoch, 140 batch] loss: 7.44651, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:18:01.914024 Training: [14 epoch, 150 batch] loss: 7.45381, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:18:57.710800 Training: [14 epoch, 160 batch] loss: 7.40126, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:19:53.861796 Training: [14 epoch, 170 batch] loss: 7.35039, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:20:50.581876 Training: [14 epoch, 180 batch] loss: 7.28737, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:21:47.063273 Training: [14 epoch, 190 batch] loss: 7.23638, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:22:42.629687 Training: [14 epoch, 200 batch] loss: 7.25497, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：0.96244,MAE：0.75600
2021-01-04 18:26:00.356973 Training: [15 epoch,  10 batch] loss: 7.16173, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:26:55.211501 Training: [15 epoch,  20 batch] loss: 7.13112, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:27:49.830927 Training: [15 epoch,  30 batch] loss: 7.17966, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:28:44.369605 Training: [15 epoch,  40 batch] loss: 7.02566, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:29:39.055436 Training: [15 epoch,  50 batch] loss: 7.10543, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:30:33.156370 Training: [15 epoch,  60 batch] loss: 7.03225, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:31:27.606497 Training: [15 epoch,  70 batch] loss: 7.02779, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:32:21.819217 Training: [15 epoch,  80 batch] loss: 7.02508, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:33:16.350994 Training: [15 epoch,  90 batch] loss: 6.97391, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:34:10.588297 Training: [15 epoch, 100 batch] loss: 6.89370, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:35:05.566141 Training: [15 epoch, 110 batch] loss: 6.84941, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:36:00.325785 Training: [15 epoch, 120 batch] loss: 6.89657, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:36:54.986892 Training: [15 epoch, 130 batch] loss: 6.83905, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:37:49.687641 Training: [15 epoch, 140 batch] loss: 6.83018, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:38:43.969356 Training: [15 epoch, 150 batch] loss: 6.81304, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:39:38.789969 Training: [15 epoch, 160 batch] loss: 6.78003, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:40:33.886745 Training: [15 epoch, 170 batch] loss: 6.76166, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:41:28.407451 Training: [15 epoch, 180 batch] loss: 6.64602, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:42:23.630514 Training: [15 epoch, 190 batch] loss: 6.66849, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:43:19.043386 Training: [15 epoch, 200 batch] loss: 6.66139, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：0.96559,MAE：0.77350
2021-01-04 18:46:35.912784 Training: [16 epoch,  10 batch] loss: 6.56283, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:47:29.631257 Training: [16 epoch,  20 batch] loss: 6.57960, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:48:22.370724 Training: [16 epoch,  30 batch] loss: 6.54756, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:49:17.009565 Training: [16 epoch,  40 batch] loss: 6.48386, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:50:11.439098 Training: [16 epoch,  50 batch] loss: 6.48268, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:51:05.143579 Training: [16 epoch,  60 batch] loss: 6.41913, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:51:59.808627 Training: [16 epoch,  70 batch] loss: 6.40545, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:52:54.448363 Training: [16 epoch,  80 batch] loss: 6.44354, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:53:47.573181 Training: [16 epoch,  90 batch] loss: 6.38412, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:54:41.235288 Training: [16 epoch, 100 batch] loss: 6.39545, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:55:34.558858 Training: [16 epoch, 110 batch] loss: 6.36700, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:56:28.529803 Training: [16 epoch, 120 batch] loss: 6.36701, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:57:22.697292 Training: [16 epoch, 130 batch] loss: 6.32788, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:58:17.893956 Training: [16 epoch, 140 batch] loss: 6.27251, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 18:59:13.791481 Training: [16 epoch, 150 batch] loss: 6.27051, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:00:09.214425 Training: [16 epoch, 160 batch] loss: 6.23624, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:01:04.039621 Training: [16 epoch, 170 batch] loss: 6.19912, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:01:58.102701 Training: [16 epoch, 180 batch] loss: 6.14414, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:02:53.829531 Training: [16 epoch, 190 batch] loss: 6.13839, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:03:49.910906 Training: [16 epoch, 200 batch] loss: 6.13215, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：0.97921,MAE：0.78801
2021-01-04 19:07:06.422933 Training: [17 epoch,  10 batch] loss: 6.04946, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:08:00.886499 Training: [17 epoch,  20 batch] loss: 6.01278, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:08:54.644337 Training: [17 epoch,  30 batch] loss: 6.00317, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:09:49.345802 Training: [17 epoch,  40 batch] loss: 5.98613, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:10:44.489728 Training: [17 epoch,  50 batch] loss: 5.91840, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:11:37.998978 Training: [17 epoch,  60 batch] loss: 5.97207, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:12:33.474859 Training: [17 epoch,  70 batch] loss: 5.95300, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:13:28.902858 Training: [17 epoch,  80 batch] loss: 5.92781, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:14:24.992084 Training: [17 epoch,  90 batch] loss: 5.85262, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:15:19.681591 Training: [17 epoch, 100 batch] loss: 5.79822, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:16:14.169886 Training: [17 epoch, 110 batch] loss: 5.83316, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:17:09.183220 Training: [17 epoch, 120 batch] loss: 5.83412, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:18:04.791357 Training: [17 epoch, 130 batch] loss: 5.73080, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:18:59.850410 Training: [17 epoch, 140 batch] loss: 5.76751, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:19:55.444969 Training: [17 epoch, 150 batch] loss: 5.74911, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:20:50.251982 Training: [17 epoch, 160 batch] loss: 5.74301, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:21:44.855841 Training: [17 epoch, 170 batch] loss: 5.71938, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:22:40.474352 Training: [17 epoch, 180 batch] loss: 5.66756, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:23:35.974952 Training: [17 epoch, 190 batch] loss: 5.62571, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:24:31.742213 Training: [17 epoch, 200 batch] loss: 5.68668, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.00395,MAE：0.80459
2021-01-04 19:27:46.359872 Training: [18 epoch,  10 batch] loss: 5.59057, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:28:39.995196 Training: [18 epoch,  20 batch] loss: 5.58101, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:29:35.127511 Training: [18 epoch,  30 batch] loss: 5.50120, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:30:29.389328 Training: [18 epoch,  40 batch] loss: 5.47320, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:31:23.494064 Training: [18 epoch,  50 batch] loss: 5.50910, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:32:18.721718 Training: [18 epoch,  60 batch] loss: 5.47763, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:33:13.865643 Training: [18 epoch,  70 batch] loss: 5.46409, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:34:07.611015 Training: [18 epoch,  80 batch] loss: 5.40581, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:35:03.209930 Training: [18 epoch,  90 batch] loss: 5.41886, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:35:58.244494 Training: [18 epoch, 100 batch] loss: 5.38205, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:36:52.661466 Training: [18 epoch, 110 batch] loss: 5.35262, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:37:47.077248 Training: [18 epoch, 120 batch] loss: 5.37946, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:38:41.807861 Training: [18 epoch, 130 batch] loss: 5.31087, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:39:37.154774 Training: [18 epoch, 140 batch] loss: 5.31795, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:40:32.867248 Training: [18 epoch, 150 batch] loss: 5.27699, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:41:29.189466 Training: [18 epoch, 160 batch] loss: 5.24920, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:42:25.739163 Training: [18 epoch, 170 batch] loss: 5.27040, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:43:22.703282 Training: [18 epoch, 180 batch] loss: 5.19084, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:44:19.683406 Training: [18 epoch, 190 batch] loss: 5.25888, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:45:16.151523 Training: [18 epoch, 200 batch] loss: 5.23332, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.01074,MAE：0.81504
2021-01-04 19:48:33.430446 Training: [19 epoch,  10 batch] loss: 5.15424, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:49:26.589506 Training: [19 epoch,  20 batch] loss: 5.09157, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:50:21.581125 Training: [19 epoch,  30 batch] loss: 5.19500, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:51:16.773997 Training: [19 epoch,  40 batch] loss: 5.12474, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:52:12.081200 Training: [19 epoch,  50 batch] loss: 5.04421, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:53:08.274966 Training: [19 epoch,  60 batch] loss: 5.03915, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:54:04.934934 Training: [19 epoch,  70 batch] loss: 5.05317, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:55:00.853862 Training: [19 epoch,  80 batch] loss: 5.00085, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:55:55.901554 Training: [19 epoch,  90 batch] loss: 5.00765, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:56:51.175387 Training: [19 epoch, 100 batch] loss: 5.00099, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:57:46.855844 Training: [19 epoch, 110 batch] loss: 4.94577, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:58:41.399322 Training: [19 epoch, 120 batch] loss: 4.90192, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 19:59:39.373390 Training: [19 epoch, 130 batch] loss: 4.95989, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:00:34.934599 Training: [19 epoch, 140 batch] loss: 4.84877, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:01:31.356511 Training: [19 epoch, 150 batch] loss: 4.86433, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:02:28.201172 Training: [19 epoch, 160 batch] loss: 4.83303, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:03:24.822359 Training: [19 epoch, 170 batch] loss: 4.90588, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:04:21.586379 Training: [19 epoch, 180 batch] loss: 4.81262, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:05:18.326350 Training: [19 epoch, 190 batch] loss: 4.77718, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:06:15.075011 Training: [19 epoch, 200 batch] loss: 4.79943, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.01806,MAE：0.82374
2021-01-04 20:09:32.321180 Training: [20 epoch,  10 batch] loss: 4.74926, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:10:27.456277 Training: [20 epoch,  20 batch] loss: 4.69761, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:11:21.567572 Training: [20 epoch,  30 batch] loss: 4.73996, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:12:16.377388 Training: [20 epoch,  40 batch] loss: 4.67946, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:13:12.704578 Training: [20 epoch,  50 batch] loss: 4.69431, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:14:13.911734 Training: [20 epoch,  60 batch] loss: 4.67479, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:15:16.510758 Training: [20 epoch,  70 batch] loss: 4.66335, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:16:19.854610 Training: [20 epoch,  80 batch] loss: 4.65262, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:17:23.229123 Training: [20 epoch,  90 batch] loss: 4.67667, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:18:27.217293 Training: [20 epoch, 100 batch] loss: 4.62189, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:19:32.021442 Training: [20 epoch, 110 batch] loss: 4.57886, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:20:35.711192 Training: [20 epoch, 120 batch] loss: 4.63469, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:21:39.412219 Training: [20 epoch, 130 batch] loss: 4.53358, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:22:43.615187 Training: [20 epoch, 140 batch] loss: 4.53027, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:23:45.544599 Training: [20 epoch, 150 batch] loss: 4.54649, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:24:50.783660 Training: [20 epoch, 160 batch] loss: 4.44086, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:25:57.375542 Training: [20 epoch, 170 batch] loss: 4.54765, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:27:01.827177 Training: [20 epoch, 180 batch] loss: 4.47426, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:28:06.129541 Training: [20 epoch, 190 batch] loss: 4.40849, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:29:09.386068 Training: [20 epoch, 200 batch] loss: 4.42793, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.01941,MAE：0.82176
2021-01-04 20:32:48.807246 Training: [21 epoch,  10 batch] loss: 4.36135, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:33:55.431017 Training: [21 epoch,  20 batch] loss: 4.44381, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:34:58.300576 Training: [21 epoch,  30 batch] loss: 4.40266, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:36:02.100953 Training: [21 epoch,  40 batch] loss: 4.30803, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:37:06.198194 Training: [21 epoch,  50 batch] loss: 4.36217, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:38:10.286207 Training: [21 epoch,  60 batch] loss: 4.30629, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:39:15.029435 Training: [21 epoch,  70 batch] loss: 4.33405, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:40:17.336592 Training: [21 epoch,  80 batch] loss: 4.31228, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:41:20.893537 Training: [21 epoch,  90 batch] loss: 4.26796, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:42:23.830601 Training: [21 epoch, 100 batch] loss: 4.30417, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:43:28.365154 Training: [21 epoch, 110 batch] loss: 4.31545, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:44:32.013637 Training: [21 epoch, 120 batch] loss: 4.23155, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:45:33.440080 Training: [21 epoch, 130 batch] loss: 4.29095, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:46:28.961834 Training: [21 epoch, 140 batch] loss: 4.20826, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:47:23.182635 Training: [21 epoch, 150 batch] loss: 4.18781, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:48:17.879362 Training: [21 epoch, 160 batch] loss: 4.19368, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:49:13.894711 Training: [21 epoch, 170 batch] loss: 4.17243, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:50:09.965855 Training: [21 epoch, 180 batch] loss: 4.15415, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:51:05.899364 Training: [21 epoch, 190 batch] loss: 4.15076, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:52:01.161578 Training: [21 epoch, 200 batch] loss: 4.09101, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.07155,MAE：0.90110
2021-01-04 20:55:16.714179 Training: [22 epoch,  10 batch] loss: 4.12260, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:56:11.136936 Training: [22 epoch,  20 batch] loss: 4.07671, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:57:06.170462 Training: [22 epoch,  30 batch] loss: 4.11912, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:58:01.493434 Training: [22 epoch,  40 batch] loss: 4.06274, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:58:56.950761 Training: [22 epoch,  50 batch] loss: 4.02780, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 20:59:52.631676 Training: [22 epoch,  60 batch] loss: 4.06617, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:00:48.111591 Training: [22 epoch,  70 batch] loss: 4.01217, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:01:43.671503 Training: [22 epoch,  80 batch] loss: 3.99097, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:02:39.616346 Training: [22 epoch,  90 batch] loss: 4.02619, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:03:35.275471 Training: [22 epoch, 100 batch] loss: 3.98711, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:04:29.968196 Training: [22 epoch, 110 batch] loss: 3.96841, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:05:25.117622 Training: [22 epoch, 120 batch] loss: 3.93205, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:06:20.272868 Training: [22 epoch, 130 batch] loss: 3.91127, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:07:14.999940 Training: [22 epoch, 140 batch] loss: 3.90763, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:08:10.391101 Training: [22 epoch, 150 batch] loss: 3.90049, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:09:05.398521 Training: [22 epoch, 160 batch] loss: 3.88329, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:09:59.697273 Training: [22 epoch, 170 batch] loss: 3.84003, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:10:53.858908 Training: [22 epoch, 180 batch] loss: 3.87025, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:11:47.549347 Training: [22 epoch, 190 batch] loss: 3.83031, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:12:43.991551 Training: [22 epoch, 200 batch] loss: 3.88580, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.11529,MAE：0.95342
2021-01-04 21:16:01.233062 Training: [23 epoch,  10 batch] loss: 3.84208, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:16:56.443436 Training: [23 epoch,  20 batch] loss: 3.76811, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:17:51.386166 Training: [23 epoch,  30 batch] loss: 3.83512, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:18:46.962665 Training: [23 epoch,  40 batch] loss: 3.75650, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:19:41.923248 Training: [23 epoch,  50 batch] loss: 3.79535, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:20:37.257415 Training: [23 epoch,  60 batch] loss: 3.76698, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:21:32.353540 Training: [23 epoch,  70 batch] loss: 3.71465, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:22:27.457825 Training: [23 epoch,  80 batch] loss: 3.72635, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:23:22.843551 Training: [23 epoch,  90 batch] loss: 3.67488, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:24:17.862894 Training: [23 epoch, 100 batch] loss: 3.71593, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:25:13.548130 Training: [23 epoch, 110 batch] loss: 3.73294, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:26:14.694893 Training: [23 epoch, 120 batch] loss: 3.70260, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:27:18.554532 Training: [23 epoch, 130 batch] loss: 3.66001, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:28:22.514835 Training: [23 epoch, 140 batch] loss: 3.67129, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:29:25.995837 Training: [23 epoch, 150 batch] loss: 3.62069, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:30:28.562570 Training: [23 epoch, 160 batch] loss: 3.68931, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:31:30.949390 Training: [23 epoch, 170 batch] loss: 3.64160, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:32:33.521242 Training: [23 epoch, 180 batch] loss: 3.56111, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:33:36.774665 Training: [23 epoch, 190 batch] loss: 3.62200, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:34:40.307143 Training: [23 epoch, 200 batch] loss: 3.51704, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.06271,MAE：0.88826
2021-01-04 21:38:47.417053 Training: [24 epoch,  10 batch] loss: 3.61280, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:39:50.702710 Training: [24 epoch,  20 batch] loss: 3.52907, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:40:54.058109 Training: [24 epoch,  30 batch] loss: 3.54399, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:41:57.865827 Training: [24 epoch,  40 batch] loss: 3.57691, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:43:01.279666 Training: [24 epoch,  50 batch] loss: 3.50400, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:44:05.121584 Training: [24 epoch,  60 batch] loss: 3.49807, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:45:08.718101 Training: [24 epoch,  70 batch] loss: 3.49106, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:46:12.403532 Training: [24 epoch,  80 batch] loss: 3.48251, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:47:16.207288 Training: [24 epoch,  90 batch] loss: 3.47667, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:48:19.823053 Training: [24 epoch, 100 batch] loss: 3.43694, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:49:23.577987 Training: [24 epoch, 110 batch] loss: 3.51018, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:50:27.386090 Training: [24 epoch, 120 batch] loss: 3.46331, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:51:31.170747 Training: [24 epoch, 130 batch] loss: 3.40985, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:52:35.102491 Training: [24 epoch, 140 batch] loss: 3.43429, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:53:38.931701 Training: [24 epoch, 150 batch] loss: 3.42544, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:54:42.590634 Training: [24 epoch, 160 batch] loss: 3.46360, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:55:46.647542 Training: [24 epoch, 170 batch] loss: 3.35563, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:56:50.127483 Training: [24 epoch, 180 batch] loss: 3.43588, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:57:54.189602 Training: [24 epoch, 190 batch] loss: 3.29415, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 21:58:56.250031 Training: [24 epoch, 200 batch] loss: 3.33406, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.19137,MAE：1.04414
2021-01-04 22:02:50.427677 Training: [25 epoch,  10 batch] loss: 3.36923, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:03:53.094557 Training: [25 epoch,  20 batch] loss: 3.31397, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:04:56.509414 Training: [25 epoch,  30 batch] loss: 3.32375, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:06:00.239171 Training: [25 epoch,  40 batch] loss: 3.31595, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:07:03.434066 Training: [25 epoch,  50 batch] loss: 3.30192, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:08:07.303925 Training: [25 epoch,  60 batch] loss: 3.30760, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:09:10.863897 Training: [25 epoch,  70 batch] loss: 3.35300, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:10:14.534583 Training: [25 epoch,  80 batch] loss: 3.28994, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:11:18.317993 Training: [25 epoch,  90 batch] loss: 3.22315, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:12:21.858545 Training: [25 epoch, 100 batch] loss: 3.26071, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:13:25.748287 Training: [25 epoch, 110 batch] loss: 3.31210, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:14:29.394663 Training: [25 epoch, 120 batch] loss: 3.22318, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:15:33.115037 Training: [25 epoch, 130 batch] loss: 3.26346, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:16:37.000831 Training: [25 epoch, 140 batch] loss: 3.19701, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:17:40.639712 Training: [25 epoch, 150 batch] loss: 3.18336, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:18:44.343935 Training: [25 epoch, 160 batch] loss: 3.20200, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:19:48.352338 Training: [25 epoch, 170 batch] loss: 3.17377, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:20:51.892666 Training: [25 epoch, 180 batch] loss: 3.18172, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:21:55.969299 Training: [25 epoch, 190 batch] loss: 3.12894, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:22:59.547512 Training: [25 epoch, 200 batch] loss: 3.13810, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.10702,MAE：0.94422
2021-01-04 22:27:10.581108 Training: [26 epoch,  10 batch] loss: 3.11822, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:28:11.454298 Training: [26 epoch,  20 batch] loss: 3.13166, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:29:11.569503 Training: [26 epoch,  30 batch] loss: 3.14847, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:30:12.414126 Training: [26 epoch,  40 batch] loss: 3.12543, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:31:14.173002 Training: [26 epoch,  50 batch] loss: 3.15304, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:32:17.801030 Training: [26 epoch,  60 batch] loss: 3.09787, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:33:21.153622 Training: [26 epoch,  70 batch] loss: 3.07310, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:34:29.661518 Training: [26 epoch,  80 batch] loss: 3.08417, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:35:38.024907 Training: [26 epoch,  90 batch] loss: 3.07227, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:36:45.818806 Training: [26 epoch, 100 batch] loss: 3.05323, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:37:53.704907 Training: [26 epoch, 110 batch] loss: 3.05280, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:39:01.127981 Training: [26 epoch, 120 batch] loss: 3.04570, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:40:09.150750 Training: [26 epoch, 130 batch] loss: 3.02878, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:41:18.548552 Training: [26 epoch, 140 batch] loss: 2.99737, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:42:26.694212 Training: [26 epoch, 150 batch] loss: 2.96017, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:43:35.179824 Training: [26 epoch, 160 batch] loss: 2.98233, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:44:43.242572 Training: [26 epoch, 170 batch] loss: 3.04111, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:45:51.661347 Training: [26 epoch, 180 batch] loss: 2.95840, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:47:00.269516 Training: [26 epoch, 190 batch] loss: 3.03040, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:48:08.485699 Training: [26 epoch, 200 batch] loss: 3.03163, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13072,MAE：0.97642
2021-01-04 22:52:22.834693 Training: [27 epoch,  10 batch] loss: 2.91601, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:53:24.474956 Training: [27 epoch,  20 batch] loss: 2.89732, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:54:26.274470 Training: [27 epoch,  30 batch] loss: 2.97761, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:55:29.793628 Training: [27 epoch,  40 batch] loss: 2.94964, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:56:33.839214 Training: [27 epoch,  50 batch] loss: 2.94903, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:57:36.285798 Training: [27 epoch,  60 batch] loss: 2.89642, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:58:38.529928 Training: [27 epoch,  70 batch] loss: 2.95138, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 22:59:41.477130 Training: [27 epoch,  80 batch] loss: 2.93478, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:00:44.727936 Training: [27 epoch,  90 batch] loss: 2.92236, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:01:48.571260 Training: [27 epoch, 100 batch] loss: 2.86597, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:02:52.635841 Training: [27 epoch, 110 batch] loss: 2.90973, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:03:57.159321 Training: [27 epoch, 120 batch] loss: 2.85195, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:05:00.998755 Training: [27 epoch, 130 batch] loss: 2.87972, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:06:04.859432 Training: [27 epoch, 140 batch] loss: 2.84641, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:07:09.226172 Training: [27 epoch, 150 batch] loss: 2.83573, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:08:13.386984 Training: [27 epoch, 160 batch] loss: 2.81591, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:09:17.131083 Training: [27 epoch, 170 batch] loss: 2.80979, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:10:21.389083 Training: [27 epoch, 180 batch] loss: 2.85113, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:11:25.663280 Training: [27 epoch, 190 batch] loss: 2.84625, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:12:29.434207 Training: [27 epoch, 200 batch] loss: 2.81137, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.19526,MAE：1.05126
2021-01-04 23:16:35.534705 Training: [28 epoch,  10 batch] loss: 2.83567, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:17:36.977278 Training: [28 epoch,  20 batch] loss: 2.71206, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:18:39.092573 Training: [28 epoch,  30 batch] loss: 2.83068, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:19:41.008885 Training: [28 epoch,  40 batch] loss: 2.73814, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:20:44.583248 Training: [28 epoch,  50 batch] loss: 2.77819, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:21:49.077386 Training: [28 epoch,  60 batch] loss: 2.73123, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:22:53.001469 Training: [28 epoch,  70 batch] loss: 2.73462, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:23:56.899436 Training: [28 epoch,  80 batch] loss: 2.74898, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:25:00.505714 Training: [28 epoch,  90 batch] loss: 2.77361, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:26:04.201815 Training: [28 epoch, 100 batch] loss: 2.71194, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:27:06.816387 Training: [28 epoch, 110 batch] loss: 2.68300, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:28:09.015531 Training: [28 epoch, 120 batch] loss: 2.69478, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:29:11.665571 Training: [28 epoch, 130 batch] loss: 2.70010, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:30:15.266862 Training: [28 epoch, 140 batch] loss: 2.70718, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:31:19.170448 Training: [28 epoch, 150 batch] loss: 2.75857, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:32:22.948473 Training: [28 epoch, 160 batch] loss: 2.68273, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:33:27.566652 Training: [28 epoch, 170 batch] loss: 2.65404, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:34:31.408424 Training: [28 epoch, 180 batch] loss: 2.67541, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:35:35.213750 Training: [28 epoch, 190 batch] loss: 2.63834, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:36:39.217126 Training: [28 epoch, 200 batch] loss: 2.67835, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.20057,MAE：1.05448
2021-01-04 23:40:53.312047 Training: [29 epoch,  10 batch] loss: 2.63086, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:41:57.153354 Training: [29 epoch,  20 batch] loss: 2.65944, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:42:59.109369 Training: [29 epoch,  30 batch] loss: 2.59557, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:44:01.150212 Training: [29 epoch,  40 batch] loss: 2.65875, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:45:04.029881 Training: [29 epoch,  50 batch] loss: 2.59523, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:46:07.869917 Training: [29 epoch,  60 batch] loss: 2.61791, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:47:11.651556 Training: [29 epoch,  70 batch] loss: 2.60821, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:48:15.863542 Training: [29 epoch,  80 batch] loss: 2.57492, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:49:20.415936 Training: [29 epoch,  90 batch] loss: 2.59883, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:50:24.174374 Training: [29 epoch, 100 batch] loss: 2.56518, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:51:26.660689 Training: [29 epoch, 110 batch] loss: 2.62562, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:52:29.097633 Training: [29 epoch, 120 batch] loss: 2.56014, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:53:31.975208 Training: [29 epoch, 130 batch] loss: 2.59633, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:54:34.822149 Training: [29 epoch, 140 batch] loss: 2.57874, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:55:37.391359 Training: [29 epoch, 150 batch] loss: 2.55970, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:56:40.163919 Training: [29 epoch, 160 batch] loss: 2.55951, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:57:43.510669 Training: [29 epoch, 170 batch] loss: 2.55167, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:58:46.463794 Training: [29 epoch, 180 batch] loss: 2.53523, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-04 23:59:50.525773 Training: [29 epoch, 190 batch] loss: 2.50275, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:00:55.323061 Training: [29 epoch, 200 batch] loss: 2.56897, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.11777,MAE：0.95896
2021-01-05 00:05:12.493048 Training: [30 epoch,  10 batch] loss: 2.53997, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:06:19.706921 Training: [30 epoch,  20 batch] loss: 2.54792, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:07:23.905727 Training: [30 epoch,  30 batch] loss: 2.48741, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:08:25.935318 Training: [30 epoch,  40 batch] loss: 2.45722, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:09:28.179085 Training: [30 epoch,  50 batch] loss: 2.54807, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:10:32.321947 Training: [30 epoch,  60 batch] loss: 2.47566, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:11:35.933635 Training: [30 epoch,  70 batch] loss: 2.50331, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:12:39.904934 Training: [30 epoch,  80 batch] loss: 2.49287, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:13:43.885954 Training: [30 epoch,  90 batch] loss: 2.45010, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:14:48.705987 Training: [30 epoch, 100 batch] loss: 2.51600, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:15:52.679435 Training: [30 epoch, 110 batch] loss: 2.51591, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:16:56.642947 Training: [30 epoch, 120 batch] loss: 2.43965, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:18:00.574427 Training: [30 epoch, 130 batch] loss: 2.43178, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:19:04.966983 Training: [30 epoch, 140 batch] loss: 2.50480, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:20:09.463984 Training: [30 epoch, 150 batch] loss: 2.42958, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:21:13.167023 Training: [30 epoch, 160 batch] loss: 2.43663, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:22:17.180311 Training: [30 epoch, 170 batch] loss: 2.42034, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:23:21.416603 Training: [30 epoch, 180 batch] loss: 2.46762, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:24:25.544024 Training: [30 epoch, 190 batch] loss: 2.43172, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:25:27.998756 Training: [30 epoch, 200 batch] loss: 2.43743, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.15082,MAE：1.00143
2021-01-05 00:29:31.806359 Training: [31 epoch,  10 batch] loss: 2.39506, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:30:33.522196 Training: [31 epoch,  20 batch] loss: 2.46914, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:31:35.376301 Training: [31 epoch,  30 batch] loss: 2.39527, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:32:36.832829 Training: [31 epoch,  40 batch] loss: 2.35590, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:33:38.781174 Training: [31 epoch,  50 batch] loss: 2.41765, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:34:41.834587 Training: [31 epoch,  60 batch] loss: 2.38112, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:35:45.300618 Training: [31 epoch,  70 batch] loss: 2.42252, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:36:49.158398 Training: [31 epoch,  80 batch] loss: 2.39293, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:37:53.114982 Training: [31 epoch,  90 batch] loss: 2.36922, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:38:57.377698 Training: [31 epoch, 100 batch] loss: 2.35154, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:40:01.015473 Training: [31 epoch, 110 batch] loss: 2.38059, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:41:04.797539 Training: [31 epoch, 120 batch] loss: 2.36840, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:42:09.157286 Training: [31 epoch, 130 batch] loss: 2.33646, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:43:13.319393 Training: [31 epoch, 140 batch] loss: 2.32553, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:44:17.303239 Training: [31 epoch, 150 batch] loss: 2.36281, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:45:20.992664 Training: [31 epoch, 160 batch] loss: 2.36077, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:46:25.359145 Training: [31 epoch, 170 batch] loss: 2.34868, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:47:29.591107 Training: [31 epoch, 180 batch] loss: 2.42632, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:48:33.502522 Training: [31 epoch, 190 batch] loss: 2.30308, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:49:37.275016 Training: [31 epoch, 200 batch] loss: 2.35277, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.15210,MAE：1.00328
2021-01-05 00:53:48.632567 Training: [32 epoch,  10 batch] loss: 2.32899, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:54:51.040019 Training: [32 epoch,  20 batch] loss: 2.33325, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:55:55.525596 Training: [32 epoch,  30 batch] loss: 2.33929, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:56:55.219775 Training: [32 epoch,  40 batch] loss: 2.29685, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:57:49.171194 Training: [32 epoch,  50 batch] loss: 2.27872, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:58:45.075183 Training: [32 epoch,  60 batch] loss: 2.26770, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 00:59:39.887802 Training: [32 epoch,  70 batch] loss: 2.31994, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:00:34.866526 Training: [32 epoch,  80 batch] loss: 2.33897, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:01:29.693334 Training: [32 epoch,  90 batch] loss: 2.27977, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:02:24.697797 Training: [32 epoch, 100 batch] loss: 2.30585, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:03:19.957289 Training: [32 epoch, 110 batch] loss: 2.28558, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:04:15.071340 Training: [32 epoch, 120 batch] loss: 2.32175, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:05:10.779209 Training: [32 epoch, 130 batch] loss: 2.34498, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:06:06.374027 Training: [32 epoch, 140 batch] loss: 2.26064, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:07:01.766946 Training: [32 epoch, 150 batch] loss: 2.28380, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:07:58.054407 Training: [32 epoch, 160 batch] loss: 2.30431, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:08:56.654334 Training: [32 epoch, 170 batch] loss: 2.29141, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:09:57.642387 Training: [32 epoch, 180 batch] loss: 2.27479, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:10:58.784542 Training: [32 epoch, 190 batch] loss: 2.27114, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:11:59.833804 Training: [32 epoch, 200 batch] loss: 2.23628, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.19452,MAE：1.05094
2021-01-05 01:15:35.723171 Training: [33 epoch,  10 batch] loss: 2.25041, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:16:35.614167 Training: [33 epoch,  20 batch] loss: 2.23469, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:17:33.000098 Training: [33 epoch,  30 batch] loss: 2.18142, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:18:26.894973 Training: [33 epoch,  40 batch] loss: 2.29828, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:19:20.963220 Training: [33 epoch,  50 batch] loss: 2.27578, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:20:16.241300 Training: [33 epoch,  60 batch] loss: 2.19453, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:21:11.886922 Training: [33 epoch,  70 batch] loss: 2.27371, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:22:07.576410 Training: [33 epoch,  80 batch] loss: 2.27110, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:23:02.775024 Training: [33 epoch,  90 batch] loss: 2.21960, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:23:57.408828 Training: [33 epoch, 100 batch] loss: 2.27590, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:24:52.079666 Training: [33 epoch, 110 batch] loss: 2.20442, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:25:47.922367 Training: [33 epoch, 120 batch] loss: 2.22280, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:26:44.413565 Training: [33 epoch, 130 batch] loss: 2.18968, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:27:40.702465 Training: [33 epoch, 140 batch] loss: 2.27198, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:28:36.569671 Training: [33 epoch, 150 batch] loss: 2.21490, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:29:31.747487 Training: [33 epoch, 160 batch] loss: 2.20386, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:30:26.209384 Training: [33 epoch, 170 batch] loss: 2.14993, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:31:21.939633 Training: [33 epoch, 180 batch] loss: 2.24152, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:32:18.510030 Training: [33 epoch, 190 batch] loss: 2.20126, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:33:14.786486 Training: [33 epoch, 200 batch] loss: 2.15041, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.07835,MAE：0.86948
2021-01-05 01:36:39.629887 Training: [34 epoch,  10 batch] loss: 2.18145, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:37:34.311111 Training: [34 epoch,  20 batch] loss: 2.20993, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:38:27.922951 Training: [34 epoch,  30 batch] loss: 2.23165, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:39:21.448195 Training: [34 epoch,  40 batch] loss: 2.16735, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:40:15.113440 Training: [34 epoch,  50 batch] loss: 2.18053, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:41:08.854281 Training: [34 epoch,  60 batch] loss: 2.12987, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:42:03.542989 Training: [34 epoch,  70 batch] loss: 2.12762, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:42:59.544774 Training: [34 epoch,  80 batch] loss: 2.13606, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:43:55.008899 Training: [34 epoch,  90 batch] loss: 2.16766, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:44:49.394405 Training: [34 epoch, 100 batch] loss: 2.21131, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:45:43.312385 Training: [34 epoch, 110 batch] loss: 2.15136, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:46:39.054629 Training: [34 epoch, 120 batch] loss: 2.15560, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:47:34.325785 Training: [34 epoch, 130 batch] loss: 2.14826, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:48:28.377109 Training: [34 epoch, 140 batch] loss: 2.13010, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:49:23.397217 Training: [34 epoch, 150 batch] loss: 2.12995, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:50:19.151514 Training: [34 epoch, 160 batch] loss: 2.17783, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:51:13.591297 Training: [34 epoch, 170 batch] loss: 2.13861, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:52:08.074835 Training: [34 epoch, 180 batch] loss: 2.11603, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:53:04.010502 Training: [34 epoch, 190 batch] loss: 2.14079, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:53:58.435473 Training: [34 epoch, 200 batch] loss: 2.12925, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13100,MAE：0.97418
2021-01-05 01:57:15.133053 Training: [35 epoch,  10 batch] loss: 2.15268, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:58:09.550237 Training: [35 epoch,  20 batch] loss: 2.12360, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:59:05.418241 Training: [35 epoch,  30 batch] loss: 2.14735, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 01:59:59.902555 Training: [35 epoch,  40 batch] loss: 2.13864, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:00:53.851394 Training: [35 epoch,  50 batch] loss: 2.13390, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:01:47.924817 Training: [35 epoch,  60 batch] loss: 2.15017, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:02:42.615073 Training: [35 epoch,  70 batch] loss: 2.04922, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:03:38.259210 Training: [35 epoch,  80 batch] loss: 2.12230, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:04:33.709841 Training: [35 epoch,  90 batch] loss: 2.07635, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:05:28.847388 Training: [35 epoch, 100 batch] loss: 2.13634, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:06:23.871255 Training: [35 epoch, 110 batch] loss: 2.12077, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:07:19.066541 Training: [35 epoch, 120 batch] loss: 2.13594, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:08:14.721188 Training: [35 epoch, 130 batch] loss: 2.10867, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:09:10.418713 Training: [35 epoch, 140 batch] loss: 2.11653, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:10:06.109043 Training: [35 epoch, 150 batch] loss: 2.12083, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:11:01.735175 Training: [35 epoch, 160 batch] loss: 2.09177, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:11:57.301620 Training: [35 epoch, 170 batch] loss: 2.06288, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:12:52.751148 Training: [35 epoch, 180 batch] loss: 2.08075, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:13:47.958994 Training: [35 epoch, 190 batch] loss: 2.08438, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:14:42.965835 Training: [35 epoch, 200 batch] loss: 2.08265, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.17282,MAE：1.02791
2021-01-05 02:17:58.840518 Training: [36 epoch,  10 batch] loss: 2.08949, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:18:54.208933 Training: [36 epoch,  20 batch] loss: 2.05249, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:19:49.990676 Training: [36 epoch,  30 batch] loss: 2.03771, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:20:45.054766 Training: [36 epoch,  40 batch] loss: 2.06588, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:21:39.135096 Training: [36 epoch,  50 batch] loss: 2.05773, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:22:33.090153 Training: [36 epoch,  60 batch] loss: 2.06286, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:23:29.689668 Training: [36 epoch,  70 batch] loss: 2.05866, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:24:25.704755 Training: [36 epoch,  80 batch] loss: 2.09875, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:25:21.225186 Training: [36 epoch,  90 batch] loss: 2.03702, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:26:16.997715 Training: [36 epoch, 100 batch] loss: 2.06839, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:27:11.729833 Training: [36 epoch, 110 batch] loss: 2.11819, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:28:05.570608 Training: [36 epoch, 120 batch] loss: 2.08468, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:29:01.248105 Training: [36 epoch, 130 batch] loss: 2.03973, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:29:56.749840 Training: [36 epoch, 140 batch] loss: 2.02733, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:30:50.593320 Training: [36 epoch, 150 batch] loss: 2.01252, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:31:45.669634 Training: [36 epoch, 160 batch] loss: 2.03038, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:32:41.556967 Training: [36 epoch, 170 batch] loss: 2.08218, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:33:35.966584 Training: [36 epoch, 180 batch] loss: 2.00355, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:34:29.850929 Training: [36 epoch, 190 batch] loss: 2.03911, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:35:24.846037 Training: [36 epoch, 200 batch] loss: 2.05559, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.12999,MAE：0.97098
2021-01-05 02:38:48.080803 Training: [37 epoch,  10 batch] loss: 2.05415, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:39:41.753510 Training: [37 epoch,  20 batch] loss: 1.99514, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:40:36.057964 Training: [37 epoch,  30 batch] loss: 1.98673, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:41:30.686824 Training: [37 epoch,  40 batch] loss: 2.03907, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:42:24.519679 Training: [37 epoch,  50 batch] loss: 2.02307, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:43:17.976866 Training: [37 epoch,  60 batch] loss: 2.04203, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:44:11.440483 Training: [37 epoch,  70 batch] loss: 2.00193, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:45:05.578957 Training: [37 epoch,  80 batch] loss: 1.99128, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:45:59.906066 Training: [37 epoch,  90 batch] loss: 2.06429, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:46:55.852819 Training: [37 epoch, 100 batch] loss: 2.03166, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:47:54.021179 Training: [37 epoch, 110 batch] loss: 2.03090, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:48:51.921436 Training: [37 epoch, 120 batch] loss: 1.99122, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:49:50.053140 Training: [37 epoch, 130 batch] loss: 2.02765, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:50:48.456202 Training: [37 epoch, 140 batch] loss: 2.00004, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:51:46.396333 Training: [37 epoch, 150 batch] loss: 2.03691, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:52:44.541043 Training: [37 epoch, 160 batch] loss: 1.99757, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:53:42.381376 Training: [37 epoch, 170 batch] loss: 1.99433, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:54:40.088203 Training: [37 epoch, 180 batch] loss: 1.98482, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:55:38.444519 Training: [37 epoch, 190 batch] loss: 1.99356, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 02:56:36.661525 Training: [37 epoch, 200 batch] loss: 2.04147, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.17045,MAE：1.02363
2021-01-05 02:59:55.818155 Training: [38 epoch,  10 batch] loss: 2.01649, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:00:50.894074 Training: [38 epoch,  20 batch] loss: 2.00749, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:01:44.709335 Training: [38 epoch,  30 batch] loss: 1.96159, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:02:40.099153 Training: [38 epoch,  40 batch] loss: 1.97327, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:03:33.874271 Training: [38 epoch,  50 batch] loss: 1.96547, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:04:26.526670 Training: [38 epoch,  60 batch] loss: 1.99239, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:05:19.355244 Training: [38 epoch,  70 batch] loss: 1.95609, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:06:13.093970 Training: [38 epoch,  80 batch] loss: 1.99450, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:07:08.183676 Training: [38 epoch,  90 batch] loss: 1.99784, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:08:03.444143 Training: [38 epoch, 100 batch] loss: 1.96515, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:08:59.029863 Training: [38 epoch, 110 batch] loss: 1.98773, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:09:54.369857 Training: [38 epoch, 120 batch] loss: 1.95293, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:10:49.405674 Training: [38 epoch, 130 batch] loss: 1.96798, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:11:48.711662 Training: [38 epoch, 140 batch] loss: 2.02318, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:12:48.825380 Training: [38 epoch, 150 batch] loss: 1.94605, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:13:48.795119 Training: [38 epoch, 160 batch] loss: 1.95100, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:14:48.794645 Training: [38 epoch, 170 batch] loss: 1.97145, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:15:48.785905 Training: [38 epoch, 180 batch] loss: 1.96721, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:16:48.839450 Training: [38 epoch, 190 batch] loss: 1.98290, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:17:48.907137 Training: [38 epoch, 200 batch] loss: 1.95795, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13430,MAE：0.97747
2021-01-05 03:21:16.341783 Training: [39 epoch,  10 batch] loss: 1.96828, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:22:19.207081 Training: [39 epoch,  20 batch] loss: 1.99537, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:23:22.012428 Training: [39 epoch,  30 batch] loss: 1.93305, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:24:25.018043 Training: [39 epoch,  40 batch] loss: 1.94392, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:25:27.839733 Training: [39 epoch,  50 batch] loss: 1.90095, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:26:30.244495 Training: [39 epoch,  60 batch] loss: 1.98525, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:27:32.416149 Training: [39 epoch,  70 batch] loss: 1.96755, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:28:33.083329 Training: [39 epoch,  80 batch] loss: 1.96005, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:29:33.474785 Training: [39 epoch,  90 batch] loss: 1.93839, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:30:34.381818 Training: [39 epoch, 100 batch] loss: 1.93295, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:31:35.226338 Training: [39 epoch, 110 batch] loss: 2.02294, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:32:35.679761 Training: [39 epoch, 120 batch] loss: 1.94155, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:33:36.372966 Training: [39 epoch, 130 batch] loss: 1.89770, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:34:36.691703 Training: [39 epoch, 140 batch] loss: 2.01803, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:35:37.489428 Training: [39 epoch, 150 batch] loss: 1.88865, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:36:38.226704 Training: [39 epoch, 160 batch] loss: 1.95778, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:37:38.777937 Training: [39 epoch, 170 batch] loss: 1.92437, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:38:39.542593 Training: [39 epoch, 180 batch] loss: 1.90163, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:39:40.588036 Training: [39 epoch, 190 batch] loss: 1.94168, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:40:41.314881 Training: [39 epoch, 200 batch] loss: 1.91043, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.20201,MAE：1.06211
2021-01-05 03:44:04.306353 Training: [40 epoch,  10 batch] loss: 1.90377, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:45:03.818843 Training: [40 epoch,  20 batch] loss: 1.89051, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:46:02.938636 Training: [40 epoch,  30 batch] loss: 1.97230, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:46:56.202138 Training: [40 epoch,  40 batch] loss: 1.93718, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:47:49.676098 Training: [40 epoch,  50 batch] loss: 1.87850, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:48:43.764810 Training: [40 epoch,  60 batch] loss: 1.95215, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:49:38.188929 Training: [40 epoch,  70 batch] loss: 1.95662, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:50:32.989335 Training: [40 epoch,  80 batch] loss: 1.93401, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:51:27.544106 Training: [40 epoch,  90 batch] loss: 1.89168, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:52:22.029063 Training: [40 epoch, 100 batch] loss: 1.91697, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:53:16.254162 Training: [40 epoch, 110 batch] loss: 1.92396, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:54:10.342726 Training: [40 epoch, 120 batch] loss: 1.92628, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:55:04.769753 Training: [40 epoch, 130 batch] loss: 1.91528, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:56:00.185500 Training: [40 epoch, 140 batch] loss: 1.92598, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:56:54.553483 Training: [40 epoch, 150 batch] loss: 1.91072, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:57:48.731618 Training: [40 epoch, 160 batch] loss: 1.90018, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:58:44.288768 Training: [40 epoch, 170 batch] loss: 1.89041, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 03:59:38.198688 Training: [40 epoch, 180 batch] loss: 1.89188, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:00:32.947489 Training: [40 epoch, 190 batch] loss: 1.93736, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:01:28.362638 Training: [40 epoch, 200 batch] loss: 1.94406, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13643,MAE：0.98021
2021-01-05 04:04:45.709692 Training: [41 epoch,  10 batch] loss: 1.94418, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:05:38.390754 Training: [41 epoch,  20 batch] loss: 1.87216, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:06:32.842386 Training: [41 epoch,  30 batch] loss: 1.89458, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:07:25.686603 Training: [41 epoch,  40 batch] loss: 1.84035, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:08:19.298293 Training: [41 epoch,  50 batch] loss: 1.89916, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:09:12.802144 Training: [41 epoch,  60 batch] loss: 1.92489, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:10:06.491698 Training: [41 epoch,  70 batch] loss: 1.92919, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:11:00.587215 Training: [41 epoch,  80 batch] loss: 1.83415, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:11:54.857796 Training: [41 epoch,  90 batch] loss: 1.95068, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:12:49.837815 Training: [41 epoch, 100 batch] loss: 1.87541, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:13:45.470089 Training: [41 epoch, 110 batch] loss: 1.89653, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:14:41.181398 Training: [41 epoch, 120 batch] loss: 1.92178, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:15:36.517725 Training: [41 epoch, 130 batch] loss: 1.85521, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:16:31.518475 Training: [41 epoch, 140 batch] loss: 1.90897, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:17:25.925612 Training: [41 epoch, 150 batch] loss: 1.87700, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:18:19.381378 Training: [41 epoch, 160 batch] loss: 1.84616, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:19:13.327282 Training: [41 epoch, 170 batch] loss: 1.85369, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:20:08.750858 Training: [41 epoch, 180 batch] loss: 1.88137, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:21:04.157983 Training: [41 epoch, 190 batch] loss: 1.86979, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:21:59.042129 Training: [41 epoch, 200 batch] loss: 1.89280, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.17817,MAE：1.03431
2021-01-05 04:25:14.838097 Training: [42 epoch,  10 batch] loss: 1.90152, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:26:09.521421 Training: [42 epoch,  20 batch] loss: 1.87370, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:27:05.144372 Training: [42 epoch,  30 batch] loss: 1.86355, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:28:00.258472 Training: [42 epoch,  40 batch] loss: 1.90145, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:28:54.710982 Training: [42 epoch,  50 batch] loss: 1.90694, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:29:49.299503 Training: [42 epoch,  60 batch] loss: 1.87609, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:30:50.104031 Training: [42 epoch,  70 batch] loss: 1.86167, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:31:50.583288 Training: [42 epoch,  80 batch] loss: 1.87420, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:32:51.203260 Training: [42 epoch,  90 batch] loss: 1.87928, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:33:51.994341 Training: [42 epoch, 100 batch] loss: 1.88003, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:34:52.101757 Training: [42 epoch, 110 batch] loss: 1.89380, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:35:52.582146 Training: [42 epoch, 120 batch] loss: 1.87812, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:36:53.237733 Training: [42 epoch, 130 batch] loss: 1.85605, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:37:53.867211 Training: [42 epoch, 140 batch] loss: 1.89737, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:38:54.589654 Training: [42 epoch, 150 batch] loss: 1.83420, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:39:55.318807 Training: [42 epoch, 160 batch] loss: 1.85220, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:40:55.772710 Training: [42 epoch, 170 batch] loss: 1.84307, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:41:56.784642 Training: [42 epoch, 180 batch] loss: 1.85674, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:42:57.257968 Training: [42 epoch, 190 batch] loss: 1.88287, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:43:58.031549 Training: [42 epoch, 200 batch] loss: 1.84004, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13975,MAE：0.98582
2021-01-05 04:47:21.365960 Training: [43 epoch,  10 batch] loss: 1.82763, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:48:19.982305 Training: [43 epoch,  20 batch] loss: 1.87640, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:49:18.988665 Training: [43 epoch,  30 batch] loss: 1.87028, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:50:20.070563 Training: [43 epoch,  40 batch] loss: 1.87641, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:51:20.303451 Training: [43 epoch,  50 batch] loss: 1.84343, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:52:19.002324 Training: [43 epoch,  60 batch] loss: 1.84818, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:53:07.877401 Training: [43 epoch,  70 batch] loss: 1.83315, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:53:56.784811 Training: [43 epoch,  80 batch] loss: 1.85757, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:54:45.741946 Training: [43 epoch,  90 batch] loss: 1.83560, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:55:34.597644 Training: [43 epoch, 100 batch] loss: 1.81864, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:56:23.525420 Training: [43 epoch, 110 batch] loss: 1.80904, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:57:12.474073 Training: [43 epoch, 120 batch] loss: 1.88158, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:58:01.602073 Training: [43 epoch, 130 batch] loss: 1.88143, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:58:50.669155 Training: [43 epoch, 140 batch] loss: 1.81442, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 04:59:39.745301 Training: [43 epoch, 150 batch] loss: 1.88666, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:00:28.894582 Training: [43 epoch, 160 batch] loss: 1.84364, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:01:17.925031 Training: [43 epoch, 170 batch] loss: 1.87020, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:02:03.312962 Training: [43 epoch, 180 batch] loss: 1.83296, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:02:46.786712 Training: [43 epoch, 190 batch] loss: 1.79047, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:03:30.666294 Training: [43 epoch, 200 batch] loss: 1.86515, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13795,MAE：0.98300
2021-01-05 05:06:10.496536 Training: [44 epoch,  10 batch] loss: 1.85988, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:06:58.174906 Training: [44 epoch,  20 batch] loss: 1.85154, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:07:46.223002 Training: [44 epoch,  30 batch] loss: 1.84213, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:08:34.419591 Training: [44 epoch,  40 batch] loss: 1.84313, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:09:22.659298 Training: [44 epoch,  50 batch] loss: 1.79116, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:10:11.010871 Training: [44 epoch,  60 batch] loss: 1.89414, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:10:59.409519 Training: [44 epoch,  70 batch] loss: 1.83732, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:11:47.776772 Training: [44 epoch,  80 batch] loss: 1.82922, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:12:36.194113 Training: [44 epoch,  90 batch] loss: 1.78061, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:13:24.572274 Training: [44 epoch, 100 batch] loss: 1.84694, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:14:13.043249 Training: [44 epoch, 110 batch] loss: 1.79667, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:15:01.687046 Training: [44 epoch, 120 batch] loss: 1.82015, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:15:50.283640 Training: [44 epoch, 130 batch] loss: 1.82071, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:16:38.934778 Training: [44 epoch, 140 batch] loss: 1.84716, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:17:27.554912 Training: [44 epoch, 150 batch] loss: 1.81494, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:18:16.127673 Training: [44 epoch, 160 batch] loss: 1.86783, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:19:04.702571 Training: [44 epoch, 170 batch] loss: 1.82792, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:19:53.370536 Training: [44 epoch, 180 batch] loss: 1.85320, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:20:42.036889 Training: [44 epoch, 190 batch] loss: 1.82259, the best RMSE/MAE: 0.95721 / 0.74710
2021-01-05 05:21:30.668300 Training: [44 epoch, 200 batch] loss: 1.88231, the best RMSE/MAE: 0.95721 / 0.74710
<Test> RMSE：1.13907,MAE：0.98713
The best RMSE/MAE：0.95721/0.74710
