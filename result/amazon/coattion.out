-------------------- Hyperparams --------------------
time: 2021-01-04 10:30:15.126519
Dataset: amazon
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-04 10:47:54.669908 Training: [1 epoch,  10 batch] loss: 24.43372, the best RMSE/MAE: inf / inf
2021-01-04 10:48:26.026797 Training: [1 epoch,  20 batch] loss: 22.46619, the best RMSE/MAE: inf / inf
2021-01-04 10:48:57.645484 Training: [1 epoch,  30 batch] loss: 21.88421, the best RMSE/MAE: inf / inf
2021-01-04 10:49:29.237745 Training: [1 epoch,  40 batch] loss: 20.40060, the best RMSE/MAE: inf / inf
2021-01-04 10:50:00.829857 Training: [1 epoch,  50 batch] loss: 19.43541, the best RMSE/MAE: inf / inf
2021-01-04 10:50:32.726241 Training: [1 epoch,  60 batch] loss: 18.50561, the best RMSE/MAE: inf / inf
2021-01-04 10:51:04.614652 Training: [1 epoch,  70 batch] loss: 17.21863, the best RMSE/MAE: inf / inf
2021-01-04 10:51:36.342035 Training: [1 epoch,  80 batch] loss: 16.60840, the best RMSE/MAE: inf / inf
2021-01-04 10:52:08.186050 Training: [1 epoch,  90 batch] loss: 15.61242, the best RMSE/MAE: inf / inf
2021-01-04 10:52:40.127245 Training: [1 epoch, 100 batch] loss: 15.05963, the best RMSE/MAE: inf / inf
2021-01-04 10:53:12.105377 Training: [1 epoch, 110 batch] loss: 14.17496, the best RMSE/MAE: inf / inf
2021-01-04 10:53:44.082636 Training: [1 epoch, 120 batch] loss: 13.58870, the best RMSE/MAE: inf / inf
2021-01-04 10:54:16.173737 Training: [1 epoch, 130 batch] loss: 13.00377, the best RMSE/MAE: inf / inf
2021-01-04 10:54:48.066880 Training: [1 epoch, 140 batch] loss: 12.42952, the best RMSE/MAE: inf / inf
2021-01-04 10:55:20.020010 Training: [1 epoch, 150 batch] loss: 12.18108, the best RMSE/MAE: inf / inf
2021-01-04 10:55:52.027087 Training: [1 epoch, 160 batch] loss: 11.79595, the best RMSE/MAE: inf / inf
2021-01-04 10:56:24.062644 Training: [1 epoch, 170 batch] loss: 11.25944, the best RMSE/MAE: inf / inf
2021-01-04 10:56:56.062366 Training: [1 epoch, 180 batch] loss: 11.07711, the best RMSE/MAE: inf / inf
2021-01-04 10:57:28.274779 Training: [1 epoch, 190 batch] loss: 10.72233, the best RMSE/MAE: inf / inf
2021-01-04 10:58:00.434359 Training: [1 epoch, 200 batch] loss: 10.55437, the best RMSE/MAE: inf / inf
<Test> RMSE：533851.00000,MAE：453454.75000
2021-01-04 10:59:49.013938 Training: [2 epoch,  10 batch] loss: 10.17648, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:00:21.079808 Training: [2 epoch,  20 batch] loss: 9.94885, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:00:53.414538 Training: [2 epoch,  30 batch] loss: 9.81468, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:01:25.721394 Training: [2 epoch,  40 batch] loss: 9.86020, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:01:58.235865 Training: [2 epoch,  50 batch] loss: 9.78283, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:02:30.783012 Training: [2 epoch,  60 batch] loss: 9.76749, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:03:03.482728 Training: [2 epoch,  70 batch] loss: 9.72642, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:03:36.106320 Training: [2 epoch,  80 batch] loss: 9.44785, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:04:08.924511 Training: [2 epoch,  90 batch] loss: 9.63563, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:04:42.337373 Training: [2 epoch, 100 batch] loss: 9.62816, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:05:14.809987 Training: [2 epoch, 110 batch] loss: 9.66595, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:05:47.445930 Training: [2 epoch, 120 batch] loss: 9.47146, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:06:20.031379 Training: [2 epoch, 130 batch] loss: 9.55160, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:06:52.706497 Training: [2 epoch, 140 batch] loss: 9.51145, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:07:25.403700 Training: [2 epoch, 150 batch] loss: 9.50603, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:07:58.030091 Training: [2 epoch, 160 batch] loss: 9.47609, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:08:30.724197 Training: [2 epoch, 170 batch] loss: 9.43955, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:09:03.471341 Training: [2 epoch, 180 batch] loss: 9.28841, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:09:36.210282 Training: [2 epoch, 190 batch] loss: 9.46808, the best RMSE/MAE: 533851.00000 / 453454.75000
2021-01-04 11:10:08.885867 Training: [2 epoch, 200 batch] loss: 9.34354, the best RMSE/MAE: 533851.00000 / 453454.75000
<Test> RMSE：1449.64832,MAE：1250.22461
2021-01-04 11:12:09.924033 Training: [3 epoch,  10 batch] loss: 9.42161, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:12:41.931217 Training: [3 epoch,  20 batch] loss: 9.23727, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:13:14.018701 Training: [3 epoch,  30 batch] loss: 9.35397, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:13:46.254477 Training: [3 epoch,  40 batch] loss: 9.28118, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:14:18.550672 Training: [3 epoch,  50 batch] loss: 9.28310, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:14:50.954416 Training: [3 epoch,  60 batch] loss: 9.25727, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:15:23.438046 Training: [3 epoch,  70 batch] loss: 9.18347, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:15:55.825741 Training: [3 epoch,  80 batch] loss: 9.15872, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:16:28.310473 Training: [3 epoch,  90 batch] loss: 9.12925, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:17:00.815228 Training: [3 epoch, 100 batch] loss: 9.16840, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:17:33.303517 Training: [3 epoch, 110 batch] loss: 9.10309, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:18:05.851099 Training: [3 epoch, 120 batch] loss: 9.23209, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:18:38.465854 Training: [3 epoch, 130 batch] loss: 9.21004, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:19:11.014484 Training: [3 epoch, 140 batch] loss: 9.10523, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:19:43.573786 Training: [3 epoch, 150 batch] loss: 8.94263, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:20:16.245959 Training: [3 epoch, 160 batch] loss: 9.00156, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:20:48.930635 Training: [3 epoch, 170 batch] loss: 9.06832, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:21:21.611632 Training: [3 epoch, 180 batch] loss: 9.02656, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:21:54.285140 Training: [3 epoch, 190 batch] loss: 9.01982, the best RMSE/MAE: 1449.64832 / 1250.22461
2021-01-04 11:22:26.985706 Training: [3 epoch, 200 batch] loss: 8.98922, the best RMSE/MAE: 1449.64832 / 1250.22461
<Test> RMSE：88.59147,MAE：74.56013
2021-01-04 11:24:15.674651 Training: [4 epoch,  10 batch] loss: 8.96518, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:24:47.789936 Training: [4 epoch,  20 batch] loss: 8.95410, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:25:20.166419 Training: [4 epoch,  30 batch] loss: 8.93627, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:25:52.610646 Training: [4 epoch,  40 batch] loss: 8.91088, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:26:25.183260 Training: [4 epoch,  50 batch] loss: 8.96649, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:26:57.686797 Training: [4 epoch,  60 batch] loss: 8.92060, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:27:30.303636 Training: [4 epoch,  70 batch] loss: 8.93529, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:28:02.893343 Training: [4 epoch,  80 batch] loss: 8.78920, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:28:35.595350 Training: [4 epoch,  90 batch] loss: 8.81525, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:29:08.051936 Training: [4 epoch, 100 batch] loss: 8.89993, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:29:40.674011 Training: [4 epoch, 110 batch] loss: 8.71933, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:30:13.314983 Training: [4 epoch, 120 batch] loss: 8.78493, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:30:46.259461 Training: [4 epoch, 130 batch] loss: 8.67994, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:31:18.950816 Training: [4 epoch, 140 batch] loss: 8.79202, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:31:51.649604 Training: [4 epoch, 150 batch] loss: 8.72533, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:32:24.338923 Training: [4 epoch, 160 batch] loss: 8.75618, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:32:57.105915 Training: [4 epoch, 170 batch] loss: 8.66616, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:33:29.823785 Training: [4 epoch, 180 batch] loss: 8.76704, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:34:02.611067 Training: [4 epoch, 190 batch] loss: 8.58789, the best RMSE/MAE: 88.59147 / 74.56013
2021-01-04 11:34:35.382413 Training: [4 epoch, 200 batch] loss: 8.60198, the best RMSE/MAE: 88.59147 / 74.56013
<Test> RMSE：17.93749,MAE：15.78878
2021-01-04 11:36:28.618329 Training: [5 epoch,  10 batch] loss: 8.57356, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:37:04.586135 Training: [5 epoch,  20 batch] loss: 8.60794, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:37:41.009780 Training: [5 epoch,  30 batch] loss: 8.53367, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:38:17.412644 Training: [5 epoch,  40 batch] loss: 8.58371, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:38:51.555589 Training: [5 epoch,  50 batch] loss: 8.49735, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:39:24.159216 Training: [5 epoch,  60 batch] loss: 8.55441, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:39:56.735516 Training: [5 epoch,  70 batch] loss: 8.50530, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:40:29.314395 Training: [5 epoch,  80 batch] loss: 8.51438, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:41:01.963099 Training: [5 epoch,  90 batch] loss: 8.51762, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:41:34.695142 Training: [5 epoch, 100 batch] loss: 8.52984, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:42:07.442634 Training: [5 epoch, 110 batch] loss: 8.42591, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:42:40.141762 Training: [5 epoch, 120 batch] loss: 8.43935, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:43:12.814493 Training: [5 epoch, 130 batch] loss: 8.43199, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:43:45.471315 Training: [5 epoch, 140 batch] loss: 8.45300, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:44:18.215164 Training: [5 epoch, 150 batch] loss: 8.40682, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:44:50.963335 Training: [5 epoch, 160 batch] loss: 8.46343, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:45:23.738674 Training: [5 epoch, 170 batch] loss: 8.37224, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:45:56.508042 Training: [5 epoch, 180 batch] loss: 8.40735, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:46:29.270201 Training: [5 epoch, 190 batch] loss: 8.43554, the best RMSE/MAE: 17.93749 / 15.78878
2021-01-04 11:47:02.103238 Training: [5 epoch, 200 batch] loss: 8.39813, the best RMSE/MAE: 17.93749 / 15.78878
<Test> RMSE：4.77131,MAE：4.05313
2021-01-04 11:48:53.074642 Training: [6 epoch,  10 batch] loss: 8.30641, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:49:24.681430 Training: [6 epoch,  20 batch] loss: 8.33869, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:49:56.446949 Training: [6 epoch,  30 batch] loss: 8.37617, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:50:28.205449 Training: [6 epoch,  40 batch] loss: 8.23447, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:51:00.085631 Training: [6 epoch,  50 batch] loss: 8.23972, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:51:31.906914 Training: [6 epoch,  60 batch] loss: 8.23379, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:52:03.957423 Training: [6 epoch,  70 batch] loss: 8.21493, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:52:35.934485 Training: [6 epoch,  80 batch] loss: 8.13169, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:53:07.977462 Training: [6 epoch,  90 batch] loss: 8.25626, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:53:40.090857 Training: [6 epoch, 100 batch] loss: 8.23530, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:54:12.042873 Training: [6 epoch, 110 batch] loss: 8.21654, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:54:44.035234 Training: [6 epoch, 120 batch] loss: 8.19415, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:55:16.214614 Training: [6 epoch, 130 batch] loss: 8.04426, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:55:48.318721 Training: [6 epoch, 140 batch] loss: 8.12795, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:56:20.510690 Training: [6 epoch, 150 batch] loss: 8.11608, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:56:52.653429 Training: [6 epoch, 160 batch] loss: 8.13087, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:57:24.794974 Training: [6 epoch, 170 batch] loss: 8.10707, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:57:57.035296 Training: [6 epoch, 180 batch] loss: 8.07029, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:58:29.303364 Training: [6 epoch, 190 batch] loss: 8.11918, the best RMSE/MAE: 4.77131 / 4.05313
2021-01-04 11:59:01.529907 Training: [6 epoch, 200 batch] loss: 8.11077, the best RMSE/MAE: 4.77131 / 4.05313
<Test> RMSE：1.97386,MAE：1.52025
2021-01-04 12:00:59.333705 Training: [7 epoch,  10 batch] loss: 8.01282, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:01:32.652203 Training: [7 epoch,  20 batch] loss: 7.99576, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:02:05.379122 Training: [7 epoch,  30 batch] loss: 7.97148, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:02:38.186994 Training: [7 epoch,  40 batch] loss: 7.98441, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:03:10.978402 Training: [7 epoch,  50 batch] loss: 7.97004, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:03:43.882460 Training: [7 epoch,  60 batch] loss: 7.95643, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:04:16.889916 Training: [7 epoch,  70 batch] loss: 7.97131, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:04:49.891934 Training: [7 epoch,  80 batch] loss: 7.96239, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:05:22.995840 Training: [7 epoch,  90 batch] loss: 7.97004, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:05:56.042724 Training: [7 epoch, 100 batch] loss: 7.97031, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:06:29.150285 Training: [7 epoch, 110 batch] loss: 7.94214, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:07:02.208357 Training: [7 epoch, 120 batch] loss: 7.88146, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:07:35.275101 Training: [7 epoch, 130 batch] loss: 7.81956, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:08:08.370196 Training: [7 epoch, 140 batch] loss: 7.85720, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:08:41.470774 Training: [7 epoch, 150 batch] loss: 7.73251, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:09:14.546416 Training: [7 epoch, 160 batch] loss: 7.74704, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:09:47.623643 Training: [7 epoch, 170 batch] loss: 7.75077, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:10:20.742135 Training: [7 epoch, 180 batch] loss: 7.83438, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:10:53.928220 Training: [7 epoch, 190 batch] loss: 7.75202, the best RMSE/MAE: 1.97386 / 1.52025
2021-01-04 12:11:27.110928 Training: [7 epoch, 200 batch] loss: 7.69106, the best RMSE/MAE: 1.97386 / 1.52025
<Test> RMSE：1.21041,MAE：0.92404
2021-01-04 12:13:15.827114 Training: [8 epoch,  10 batch] loss: 7.67304, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:13:47.874048 Training: [8 epoch,  20 batch] loss: 7.70997, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:14:20.031141 Training: [8 epoch,  30 batch] loss: 7.68662, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:14:52.378835 Training: [8 epoch,  40 batch] loss: 7.66392, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:15:24.659340 Training: [8 epoch,  50 batch] loss: 7.65070, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:15:56.952630 Training: [8 epoch,  60 batch] loss: 7.66277, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:16:29.510577 Training: [8 epoch,  70 batch] loss: 7.60622, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:17:01.860650 Training: [8 epoch,  80 batch] loss: 7.65151, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:17:34.431064 Training: [8 epoch,  90 batch] loss: 7.59881, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:18:06.873475 Training: [8 epoch, 100 batch] loss: 7.56916, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:18:39.518905 Training: [8 epoch, 110 batch] loss: 7.62801, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:19:12.070658 Training: [8 epoch, 120 batch] loss: 7.58610, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:19:44.658427 Training: [8 epoch, 130 batch] loss: 7.52272, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:20:17.271240 Training: [8 epoch, 140 batch] loss: 7.53673, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:20:49.827347 Training: [8 epoch, 150 batch] loss: 7.46846, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:21:22.393873 Training: [8 epoch, 160 batch] loss: 7.53072, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:21:55.047104 Training: [8 epoch, 170 batch] loss: 7.51437, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:22:27.638957 Training: [8 epoch, 180 batch] loss: 7.52248, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:23:00.310341 Training: [8 epoch, 190 batch] loss: 7.43664, the best RMSE/MAE: 1.21041 / 0.92404
2021-01-04 12:23:33.045472 Training: [8 epoch, 200 batch] loss: 7.47963, the best RMSE/MAE: 1.21041 / 0.92404
<Test> RMSE：1.02256,MAE：0.76814
2021-01-04 12:25:22.177563 Training: [9 epoch,  10 batch] loss: 7.37753, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:25:54.747084 Training: [9 epoch,  20 batch] loss: 7.47323, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:26:27.499782 Training: [9 epoch,  30 batch] loss: 7.44943, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:27:00.345827 Training: [9 epoch,  40 batch] loss: 7.31274, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:27:33.272218 Training: [9 epoch,  50 batch] loss: 7.37488, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:28:06.222514 Training: [9 epoch,  60 batch] loss: 7.39373, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:28:39.315868 Training: [9 epoch,  70 batch] loss: 7.42055, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:29:12.334983 Training: [9 epoch,  80 batch] loss: 7.35546, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:29:45.450261 Training: [9 epoch,  90 batch] loss: 7.22770, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:30:18.647089 Training: [9 epoch, 100 batch] loss: 7.29988, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:30:51.751895 Training: [9 epoch, 110 batch] loss: 7.27365, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:31:24.895028 Training: [9 epoch, 120 batch] loss: 7.20561, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:31:58.007862 Training: [9 epoch, 130 batch] loss: 7.28944, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:32:31.200059 Training: [9 epoch, 140 batch] loss: 7.21902, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:33:04.410801 Training: [9 epoch, 150 batch] loss: 7.20269, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:33:37.578252 Training: [9 epoch, 160 batch] loss: 7.19968, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:34:10.736302 Training: [9 epoch, 170 batch] loss: 7.27061, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:34:43.899957 Training: [9 epoch, 180 batch] loss: 7.31415, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:35:17.107407 Training: [9 epoch, 190 batch] loss: 7.18559, the best RMSE/MAE: 1.02256 / 0.76814
2021-01-04 12:35:50.314537 Training: [9 epoch, 200 batch] loss: 7.20509, the best RMSE/MAE: 1.02256 / 0.76814
<Test> RMSE：0.95449,MAE：0.75088
2021-01-04 12:37:58.641300 Training: [10 epoch,  10 batch] loss: 7.10455, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:38:35.167286 Training: [10 epoch,  20 batch] loss: 7.04612, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:39:08.310854 Training: [10 epoch,  30 batch] loss: 7.07964, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:39:41.103077 Training: [10 epoch,  40 batch] loss: 7.12787, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:40:13.930960 Training: [10 epoch,  50 batch] loss: 7.01643, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:40:46.799535 Training: [10 epoch,  60 batch] loss: 7.02263, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:41:19.654567 Training: [10 epoch,  70 batch] loss: 7.06916, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:41:52.633521 Training: [10 epoch,  80 batch] loss: 7.03589, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:42:25.541920 Training: [10 epoch,  90 batch] loss: 7.00976, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:42:58.642119 Training: [10 epoch, 100 batch] loss: 7.00819, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:43:31.711183 Training: [10 epoch, 110 batch] loss: 7.01746, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:44:04.559698 Training: [10 epoch, 120 batch] loss: 6.94987, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:44:37.610409 Training: [10 epoch, 130 batch] loss: 6.91797, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:45:10.674283 Training: [10 epoch, 140 batch] loss: 6.95699, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:45:43.657128 Training: [10 epoch, 150 batch] loss: 6.98816, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:46:16.745322 Training: [10 epoch, 160 batch] loss: 6.90494, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:46:49.891603 Training: [10 epoch, 170 batch] loss: 6.90610, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:47:22.977810 Training: [10 epoch, 180 batch] loss: 6.92073, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:47:56.091057 Training: [10 epoch, 190 batch] loss: 6.85010, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:48:29.222988 Training: [10 epoch, 200 batch] loss: 6.86315, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.00958,MAE：0.83531
2021-01-04 12:50:20.836410 Training: [11 epoch,  10 batch] loss: 6.81549, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:50:57.142167 Training: [11 epoch,  20 batch] loss: 6.84618, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:51:33.597519 Training: [11 epoch,  30 batch] loss: 6.77801, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:52:10.280366 Training: [11 epoch,  40 batch] loss: 6.75507, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:52:46.998011 Training: [11 epoch,  50 batch] loss: 6.71929, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:53:23.794417 Training: [11 epoch,  60 batch] loss: 6.69379, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:54:00.705031 Training: [11 epoch,  70 batch] loss: 6.72272, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:54:37.668357 Training: [11 epoch,  80 batch] loss: 6.71229, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:55:14.675738 Training: [11 epoch,  90 batch] loss: 6.70739, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:55:51.669611 Training: [11 epoch, 100 batch] loss: 6.72590, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:56:28.560882 Training: [11 epoch, 110 batch] loss: 6.68528, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:57:05.501241 Training: [11 epoch, 120 batch] loss: 6.72828, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:57:42.524745 Training: [11 epoch, 130 batch] loss: 6.63779, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:58:19.534116 Training: [11 epoch, 140 batch] loss: 6.64383, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:58:56.452980 Training: [11 epoch, 150 batch] loss: 6.59381, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 12:59:33.453230 Training: [11 epoch, 160 batch] loss: 6.67782, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:00:10.468214 Training: [11 epoch, 170 batch] loss: 6.57312, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:00:47.416217 Training: [11 epoch, 180 batch] loss: 6.55801, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:01:24.433208 Training: [11 epoch, 190 batch] loss: 6.54415, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:02:01.496599 Training: [11 epoch, 200 batch] loss: 6.54874, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03980,MAE：0.87948
2021-01-04 13:03:51.884972 Training: [12 epoch,  10 batch] loss: 6.53757, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:04:24.969903 Training: [12 epoch,  20 batch] loss: 6.53273, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:04:57.693640 Training: [12 epoch,  30 batch] loss: 6.48682, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:05:30.383321 Training: [12 epoch,  40 batch] loss: 6.53107, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:06:03.243920 Training: [12 epoch,  50 batch] loss: 6.48670, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:06:36.210911 Training: [12 epoch,  60 batch] loss: 6.51704, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:07:09.282876 Training: [12 epoch,  70 batch] loss: 6.41364, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:07:42.367582 Training: [12 epoch,  80 batch] loss: 6.43700, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:08:15.563171 Training: [12 epoch,  90 batch] loss: 6.39576, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:08:48.711676 Training: [12 epoch, 100 batch] loss: 6.43076, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:09:21.860073 Training: [12 epoch, 110 batch] loss: 6.39635, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:09:55.074752 Training: [12 epoch, 120 batch] loss: 6.41306, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:10:28.235130 Training: [12 epoch, 130 batch] loss: 6.33578, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:11:01.393837 Training: [12 epoch, 140 batch] loss: 6.35401, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:11:34.591675 Training: [12 epoch, 150 batch] loss: 6.30298, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:12:07.798896 Training: [12 epoch, 160 batch] loss: 6.34569, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:12:40.922701 Training: [12 epoch, 170 batch] loss: 6.27639, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:13:14.095177 Training: [12 epoch, 180 batch] loss: 6.32791, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:13:47.310783 Training: [12 epoch, 190 batch] loss: 6.29928, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:14:20.590758 Training: [12 epoch, 200 batch] loss: 6.26013, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.06043,MAE：0.90218
2021-01-04 13:16:09.257744 Training: [13 epoch,  10 batch] loss: 6.26808, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:16:41.189729 Training: [13 epoch,  20 batch] loss: 6.23872, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:17:13.342648 Training: [13 epoch,  30 batch] loss: 6.13209, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:17:45.608919 Training: [13 epoch,  40 batch] loss: 6.16798, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:18:17.673139 Training: [13 epoch,  50 batch] loss: 6.14339, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:18:49.907372 Training: [13 epoch,  60 batch] loss: 6.12964, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:19:22.266095 Training: [13 epoch,  70 batch] loss: 6.10413, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:19:54.698584 Training: [13 epoch,  80 batch] loss: 6.14511, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:20:27.054371 Training: [13 epoch,  90 batch] loss: 6.06864, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:20:59.435432 Training: [13 epoch, 100 batch] loss: 6.05821, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:21:31.907006 Training: [13 epoch, 110 batch] loss: 6.03770, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:22:04.329821 Training: [13 epoch, 120 batch] loss: 6.03770, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:22:36.951315 Training: [13 epoch, 130 batch] loss: 6.03891, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:23:09.467745 Training: [13 epoch, 140 batch] loss: 6.00896, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:23:41.987062 Training: [13 epoch, 150 batch] loss: 6.01946, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:24:14.508269 Training: [13 epoch, 160 batch] loss: 6.01726, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:24:46.950980 Training: [13 epoch, 170 batch] loss: 6.01299, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:25:19.403650 Training: [13 epoch, 180 batch] loss: 6.03281, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:25:51.873759 Training: [13 epoch, 190 batch] loss: 5.96808, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:26:24.394336 Training: [13 epoch, 200 batch] loss: 5.93936, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.00700,MAE：0.84036
2021-01-04 13:28:12.679043 Training: [14 epoch,  10 batch] loss: 5.93729, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:28:45.005790 Training: [14 epoch,  20 batch] loss: 5.90271, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:29:17.461636 Training: [14 epoch,  30 batch] loss: 5.88193, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:29:50.105848 Training: [14 epoch,  40 batch] loss: 5.90421, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:30:22.789240 Training: [14 epoch,  50 batch] loss: 5.85093, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:30:55.478899 Training: [14 epoch,  60 batch] loss: 5.88065, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:31:28.126300 Training: [14 epoch,  70 batch] loss: 5.85144, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:32:00.938487 Training: [14 epoch,  80 batch] loss: 5.83710, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:32:33.783038 Training: [14 epoch,  90 batch] loss: 5.84035, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:33:06.746054 Training: [14 epoch, 100 batch] loss: 5.82842, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:33:39.641845 Training: [14 epoch, 110 batch] loss: 5.74182, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:34:12.601674 Training: [14 epoch, 120 batch] loss: 5.70271, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:34:45.547258 Training: [14 epoch, 130 batch] loss: 5.70623, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:35:18.831537 Training: [14 epoch, 140 batch] loss: 5.74675, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:36:22.726082 Training: [14 epoch, 150 batch] loss: 5.74462, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:37:26.271903 Training: [14 epoch, 160 batch] loss: 5.65317, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:38:29.686732 Training: [14 epoch, 170 batch] loss: 5.69226, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:39:33.683283 Training: [14 epoch, 180 batch] loss: 5.67290, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:40:37.734704 Training: [14 epoch, 190 batch] loss: 5.65589, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:41:41.438579 Training: [14 epoch, 200 batch] loss: 5.69054, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03317,MAE：0.87083
2021-01-04 13:45:48.865659 Training: [15 epoch,  10 batch] loss: 5.69381, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:46:51.763462 Training: [15 epoch,  20 batch] loss: 5.65675, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:47:54.603846 Training: [15 epoch,  30 batch] loss: 5.55200, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:48:58.038883 Training: [15 epoch,  40 batch] loss: 5.49115, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:50:02.111809 Training: [15 epoch,  50 batch] loss: 5.60817, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:51:05.586746 Training: [15 epoch,  60 batch] loss: 5.55127, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:52:09.002737 Training: [15 epoch,  70 batch] loss: 5.53006, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:53:12.484347 Training: [15 epoch,  80 batch] loss: 5.52615, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:54:15.928061 Training: [15 epoch,  90 batch] loss: 5.58437, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:55:18.495393 Training: [15 epoch, 100 batch] loss: 5.44904, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:56:21.159402 Training: [15 epoch, 110 batch] loss: 5.55150, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:57:24.706570 Training: [15 epoch, 120 batch] loss: 5.49156, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:58:28.319519 Training: [15 epoch, 130 batch] loss: 5.49268, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 13:59:32.487495 Training: [15 epoch, 140 batch] loss: 5.55706, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:00:36.341685 Training: [15 epoch, 150 batch] loss: 5.45026, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:01:40.211650 Training: [15 epoch, 160 batch] loss: 5.41064, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:02:43.684229 Training: [15 epoch, 170 batch] loss: 5.39631, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:03:47.735871 Training: [15 epoch, 180 batch] loss: 5.39083, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:04:51.583623 Training: [15 epoch, 190 batch] loss: 5.35913, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:05:55.615751 Training: [15 epoch, 200 batch] loss: 5.35737, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.05418,MAE：0.89514
2021-01-04 14:10:15.204924 Training: [16 epoch,  10 batch] loss: 5.32834, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:11:17.912045 Training: [16 epoch,  20 batch] loss: 5.35017, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:12:21.028054 Training: [16 epoch,  30 batch] loss: 5.29299, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:13:24.743217 Training: [16 epoch,  40 batch] loss: 5.31746, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:14:28.069755 Training: [16 epoch,  50 batch] loss: 5.26054, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:15:30.410611 Training: [16 epoch,  60 batch] loss: 5.29514, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:16:32.675196 Training: [16 epoch,  70 batch] loss: 5.31021, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:17:35.207982 Training: [16 epoch,  80 batch] loss: 5.15262, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:18:39.090799 Training: [16 epoch,  90 batch] loss: 5.30243, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:19:42.867600 Training: [16 epoch, 100 batch] loss: 5.20038, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:20:46.959543 Training: [16 epoch, 110 batch] loss: 5.17808, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:21:50.756003 Training: [16 epoch, 120 batch] loss: 5.17205, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:22:54.039486 Training: [16 epoch, 130 batch] loss: 5.12510, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:23:57.450704 Training: [16 epoch, 140 batch] loss: 5.22666, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:25:01.572261 Training: [16 epoch, 150 batch] loss: 5.20786, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:26:05.388194 Training: [16 epoch, 160 batch] loss: 5.17199, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:27:08.802664 Training: [16 epoch, 170 batch] loss: 5.10524, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:28:12.067266 Training: [16 epoch, 180 batch] loss: 5.09064, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:29:15.884266 Training: [16 epoch, 190 batch] loss: 5.09693, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:30:19.982219 Training: [16 epoch, 200 batch] loss: 5.08644, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03230,MAE：0.86696
2021-01-04 14:34:30.246312 Training: [17 epoch,  10 batch] loss: 5.08479, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:35:36.174357 Training: [17 epoch,  20 batch] loss: 5.00465, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:36:40.290527 Training: [17 epoch,  30 batch] loss: 5.03258, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:37:43.634450 Training: [17 epoch,  40 batch] loss: 5.03388, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:38:48.854621 Training: [17 epoch,  50 batch] loss: 5.08102, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:39:55.734057 Training: [17 epoch,  60 batch] loss: 4.98120, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:41:02.509170 Training: [17 epoch,  70 batch] loss: 4.93723, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:42:09.221000 Training: [17 epoch,  80 batch] loss: 4.99256, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:43:16.033732 Training: [17 epoch,  90 batch] loss: 4.93420, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:44:22.817704 Training: [17 epoch, 100 batch] loss: 4.91581, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:45:29.673895 Training: [17 epoch, 110 batch] loss: 4.90237, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:46:36.250212 Training: [17 epoch, 120 batch] loss: 4.83916, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:47:42.954248 Training: [17 epoch, 130 batch] loss: 4.90620, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:48:49.758014 Training: [17 epoch, 140 batch] loss: 4.87725, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:49:56.568663 Training: [17 epoch, 150 batch] loss: 4.86019, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:51:03.402163 Training: [17 epoch, 160 batch] loss: 4.87223, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:52:10.174883 Training: [17 epoch, 170 batch] loss: 4.85410, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:53:17.057927 Training: [17 epoch, 180 batch] loss: 4.91772, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:54:23.622714 Training: [17 epoch, 190 batch] loss: 4.82976, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 14:55:30.502355 Training: [17 epoch, 200 batch] loss: 4.80664, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.05168,MAE：0.89195
2021-01-04 14:59:48.514465 Training: [18 epoch,  10 batch] loss: 4.77513, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:00:51.776092 Training: [18 epoch,  20 batch] loss: 4.77139, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:01:55.184140 Training: [18 epoch,  30 batch] loss: 4.83061, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:02:58.479167 Training: [18 epoch,  40 batch] loss: 4.73432, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:04:02.243110 Training: [18 epoch,  50 batch] loss: 4.71479, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:05:05.815261 Training: [18 epoch,  60 batch] loss: 4.73362, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:06:09.515806 Training: [18 epoch,  70 batch] loss: 4.71618, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:07:13.232222 Training: [18 epoch,  80 batch] loss: 4.72577, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:08:17.010720 Training: [18 epoch,  90 batch] loss: 4.66642, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:09:20.299620 Training: [18 epoch, 100 batch] loss: 4.66700, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:10:24.135166 Training: [18 epoch, 110 batch] loss: 4.66542, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:11:27.953495 Training: [18 epoch, 120 batch] loss: 4.67892, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:12:31.367905 Training: [18 epoch, 130 batch] loss: 4.62992, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:13:35.121987 Training: [18 epoch, 140 batch] loss: 4.66842, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:14:38.890121 Training: [18 epoch, 150 batch] loss: 4.65744, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:15:42.343867 Training: [18 epoch, 160 batch] loss: 4.58169, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:16:45.751970 Training: [18 epoch, 170 batch] loss: 4.55093, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:17:48.683561 Training: [18 epoch, 180 batch] loss: 4.65773, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:18:51.528295 Training: [18 epoch, 190 batch] loss: 4.51905, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:19:54.208956 Training: [18 epoch, 200 batch] loss: 4.57604, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：0.99225,MAE：0.81858
2021-01-04 15:23:58.416310 Training: [19 epoch,  10 batch] loss: 4.51462, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:25:00.837059 Training: [19 epoch,  20 batch] loss: 4.50782, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:26:03.848875 Training: [19 epoch,  30 batch] loss: 4.45373, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:27:07.092270 Training: [19 epoch,  40 batch] loss: 4.44212, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:28:10.671507 Training: [19 epoch,  50 batch] loss: 4.48621, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:29:14.220154 Training: [19 epoch,  60 batch] loss: 4.44527, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:30:18.003686 Training: [19 epoch,  70 batch] loss: 4.45398, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:31:21.821723 Training: [19 epoch,  80 batch] loss: 4.47616, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:32:25.639188 Training: [19 epoch,  90 batch] loss: 4.41641, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:33:29.146530 Training: [19 epoch, 100 batch] loss: 4.43325, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:34:33.058567 Training: [19 epoch, 110 batch] loss: 4.41937, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:35:36.825955 Training: [19 epoch, 120 batch] loss: 4.40687, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:36:40.421409 Training: [19 epoch, 130 batch] loss: 4.37532, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:37:43.764904 Training: [19 epoch, 140 batch] loss: 4.41119, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:38:47.027276 Training: [19 epoch, 150 batch] loss: 4.28566, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:39:50.368295 Training: [19 epoch, 160 batch] loss: 4.41642, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:40:54.044272 Training: [19 epoch, 170 batch] loss: 4.31681, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:41:57.468424 Training: [19 epoch, 180 batch] loss: 4.35778, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:43:00.520199 Training: [19 epoch, 190 batch] loss: 4.37637, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:44:03.716122 Training: [19 epoch, 200 batch] loss: 4.32935, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.04049,MAE：0.87891
2021-01-04 15:48:11.133915 Training: [20 epoch,  10 batch] loss: 4.29435, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:49:13.976617 Training: [20 epoch,  20 batch] loss: 4.28008, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:50:17.163211 Training: [20 epoch,  30 batch] loss: 4.27122, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:51:20.618768 Training: [20 epoch,  40 batch] loss: 4.23042, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:52:23.844882 Training: [20 epoch,  50 batch] loss: 4.19697, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:53:27.542038 Training: [20 epoch,  60 batch] loss: 4.25792, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:54:30.930071 Training: [20 epoch,  70 batch] loss: 4.20566, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:55:34.488952 Training: [20 epoch,  80 batch] loss: 4.22499, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:56:38.612042 Training: [20 epoch,  90 batch] loss: 4.18928, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:57:42.153135 Training: [20 epoch, 100 batch] loss: 4.21043, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:58:45.335459 Training: [20 epoch, 110 batch] loss: 4.21404, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 15:59:48.115578 Training: [20 epoch, 120 batch] loss: 4.15438, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:00:51.880178 Training: [20 epoch, 130 batch] loss: 4.16719, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:01:56.257138 Training: [20 epoch, 140 batch] loss: 4.13434, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:02:59.781466 Training: [20 epoch, 150 batch] loss: 4.11394, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:04:02.664806 Training: [20 epoch, 160 batch] loss: 4.12368, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:05:05.570047 Training: [20 epoch, 170 batch] loss: 4.13709, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:06:09.455287 Training: [20 epoch, 180 batch] loss: 4.04523, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:07:13.618425 Training: [20 epoch, 190 batch] loss: 4.16241, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:08:17.833662 Training: [20 epoch, 200 batch] loss: 4.08377, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.05150,MAE：0.89228
2021-01-04 16:12:42.546033 Training: [21 epoch,  10 batch] loss: 4.04012, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:13:49.328379 Training: [21 epoch,  20 batch] loss: 4.00801, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:14:52.935245 Training: [21 epoch,  30 batch] loss: 4.00386, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:15:57.119335 Training: [21 epoch,  40 batch] loss: 3.99399, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:17:01.009351 Training: [21 epoch,  50 batch] loss: 3.98939, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:18:04.844095 Training: [21 epoch,  60 batch] loss: 3.96640, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:19:07.891449 Training: [21 epoch,  70 batch] loss: 3.99494, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:20:10.906530 Training: [21 epoch,  80 batch] loss: 3.97395, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:21:14.413242 Training: [21 epoch,  90 batch] loss: 3.99603, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:22:18.623628 Training: [21 epoch, 100 batch] loss: 3.92342, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:23:22.715566 Training: [21 epoch, 110 batch] loss: 3.90769, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:24:25.860077 Training: [21 epoch, 120 batch] loss: 3.95188, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:25:28.807797 Training: [21 epoch, 130 batch] loss: 3.88887, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:26:31.733272 Training: [21 epoch, 140 batch] loss: 3.92240, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:27:35.616270 Training: [21 epoch, 150 batch] loss: 3.89674, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:28:40.115574 Training: [21 epoch, 160 batch] loss: 3.89604, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:29:44.220763 Training: [21 epoch, 170 batch] loss: 3.91126, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:30:48.305696 Training: [21 epoch, 180 batch] loss: 3.83068, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:31:51.987194 Training: [21 epoch, 190 batch] loss: 3.88383, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:32:55.783255 Training: [21 epoch, 200 batch] loss: 3.86022, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.02841,MAE：0.86285
2021-01-04 16:37:02.066489 Training: [22 epoch,  10 batch] loss: 3.84398, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:38:04.915846 Training: [22 epoch,  20 batch] loss: 3.82439, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:39:07.743828 Training: [22 epoch,  30 batch] loss: 3.86600, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:40:11.177141 Training: [22 epoch,  40 batch] loss: 3.84434, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:41:15.371863 Training: [22 epoch,  50 batch] loss: 3.80836, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:42:20.450760 Training: [22 epoch,  60 batch] loss: 3.80140, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:43:25.521206 Training: [22 epoch,  70 batch] loss: 3.78887, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:44:30.821057 Training: [22 epoch,  80 batch] loss: 3.75013, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:45:36.304651 Training: [22 epoch,  90 batch] loss: 3.72076, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:46:40.726501 Training: [22 epoch, 100 batch] loss: 3.71381, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:47:44.644124 Training: [22 epoch, 110 batch] loss: 3.75092, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:48:48.513103 Training: [22 epoch, 120 batch] loss: 3.69720, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:49:53.807286 Training: [22 epoch, 130 batch] loss: 3.69702, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:50:59.276628 Training: [22 epoch, 140 batch] loss: 3.69359, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:52:04.741079 Training: [22 epoch, 150 batch] loss: 3.67654, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:53:10.111711 Training: [22 epoch, 160 batch] loss: 3.74981, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:54:15.572167 Training: [22 epoch, 170 batch] loss: 3.65393, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:55:19.836696 Training: [22 epoch, 180 batch] loss: 3.62635, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:56:23.110555 Training: [22 epoch, 190 batch] loss: 3.63697, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 16:57:25.913090 Training: [22 epoch, 200 batch] loss: 3.60266, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.02292,MAE：0.85599
2021-01-04 17:01:30.470050 Training: [23 epoch,  10 batch] loss: 3.57754, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:02:32.675730 Training: [23 epoch,  20 batch] loss: 3.63972, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:03:35.602510 Training: [23 epoch,  30 batch] loss: 3.56982, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:04:38.024016 Training: [23 epoch,  40 batch] loss: 3.51712, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:05:40.858489 Training: [23 epoch,  50 batch] loss: 3.56813, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:06:44.284150 Training: [23 epoch,  60 batch] loss: 3.54349, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:07:47.352769 Training: [23 epoch,  70 batch] loss: 3.53850, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:08:49.680138 Training: [23 epoch,  80 batch] loss: 3.52764, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:09:52.150720 Training: [23 epoch,  90 batch] loss: 3.56819, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:10:54.842618 Training: [23 epoch, 100 batch] loss: 3.57091, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:11:57.544766 Training: [23 epoch, 110 batch] loss: 3.51489, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:13:00.472553 Training: [23 epoch, 120 batch] loss: 3.56573, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:14:02.896840 Training: [23 epoch, 130 batch] loss: 3.55959, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:15:05.183338 Training: [23 epoch, 140 batch] loss: 3.51697, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:16:08.598403 Training: [23 epoch, 150 batch] loss: 3.49882, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:17:12.106437 Training: [23 epoch, 160 batch] loss: 3.44167, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:18:15.612711 Training: [23 epoch, 170 batch] loss: 3.49690, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:19:18.826943 Training: [23 epoch, 180 batch] loss: 3.45583, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:20:22.203286 Training: [23 epoch, 190 batch] loss: 3.51025, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:21:25.163334 Training: [23 epoch, 200 batch] loss: 3.45057, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：0.99295,MAE：0.81114
2021-01-04 17:25:30.071372 Training: [24 epoch,  10 batch] loss: 3.42242, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:26:32.780477 Training: [24 epoch,  20 batch] loss: 3.41563, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:27:35.930911 Training: [24 epoch,  30 batch] loss: 3.42172, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:28:38.498670 Training: [24 epoch,  40 batch] loss: 3.39773, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:29:41.091197 Training: [24 epoch,  50 batch] loss: 3.39657, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:30:43.218667 Training: [24 epoch,  60 batch] loss: 3.39184, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:31:45.195050 Training: [24 epoch,  70 batch] loss: 3.44110, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:32:50.659599 Training: [24 epoch,  80 batch] loss: 3.37932, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:33:55.703449 Training: [24 epoch,  90 batch] loss: 3.31281, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:35:00.022824 Training: [24 epoch, 100 batch] loss: 3.36140, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:36:03.971965 Training: [24 epoch, 110 batch] loss: 3.39889, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:37:07.828922 Training: [24 epoch, 120 batch] loss: 3.32097, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:38:11.537216 Training: [24 epoch, 130 batch] loss: 3.37209, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:39:15.377904 Training: [24 epoch, 140 batch] loss: 3.30799, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:40:19.843630 Training: [24 epoch, 150 batch] loss: 3.31733, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:41:23.957557 Training: [24 epoch, 160 batch] loss: 3.31784, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:42:27.248220 Training: [24 epoch, 170 batch] loss: 3.30376, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:43:30.344899 Training: [24 epoch, 180 batch] loss: 3.29609, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:44:34.331618 Training: [24 epoch, 190 batch] loss: 3.29434, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:45:38.465339 Training: [24 epoch, 200 batch] loss: 3.29984, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03573,MAE：0.87449
2021-01-04 17:49:47.374397 Training: [25 epoch,  10 batch] loss: 3.25884, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:50:50.095372 Training: [25 epoch,  20 batch] loss: 3.20536, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:51:52.824595 Training: [25 epoch,  30 batch] loss: 3.23001, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:52:56.080494 Training: [25 epoch,  40 batch] loss: 3.26457, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:54:00.453964 Training: [25 epoch,  50 batch] loss: 3.22050, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:55:04.510087 Training: [25 epoch,  60 batch] loss: 3.21108, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:56:08.870886 Training: [25 epoch,  70 batch] loss: 3.17435, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:57:12.601353 Training: [25 epoch,  80 batch] loss: 3.22056, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:58:15.907494 Training: [25 epoch,  90 batch] loss: 3.21494, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 17:59:19.510874 Training: [25 epoch, 100 batch] loss: 3.20751, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:00:23.164645 Training: [25 epoch, 110 batch] loss: 3.13161, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:01:26.573580 Training: [25 epoch, 120 batch] loss: 3.20736, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:02:29.747305 Training: [25 epoch, 130 batch] loss: 3.15008, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:03:32.334179 Training: [25 epoch, 140 batch] loss: 3.19245, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:04:35.220358 Training: [25 epoch, 150 batch] loss: 3.07658, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:05:38.458965 Training: [25 epoch, 160 batch] loss: 3.14705, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:06:41.422510 Training: [25 epoch, 170 batch] loss: 3.07665, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:07:44.588187 Training: [25 epoch, 180 batch] loss: 3.19681, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:08:48.140311 Training: [25 epoch, 190 batch] loss: 3.10496, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:09:52.040590 Training: [25 epoch, 200 batch] loss: 3.11233, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.10255,MAE：0.94821
2021-01-04 18:13:55.272128 Training: [26 epoch,  10 batch] loss: 3.01477, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:14:58.808092 Training: [26 epoch,  20 batch] loss: 3.10875, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:16:02.235845 Training: [26 epoch,  30 batch] loss: 3.06963, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:17:06.551813 Training: [26 epoch,  40 batch] loss: 3.08631, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:18:10.285972 Training: [26 epoch,  50 batch] loss: 3.11098, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:19:14.684231 Training: [26 epoch,  60 batch] loss: 3.04868, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:20:18.745195 Training: [26 epoch,  70 batch] loss: 3.05362, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:21:22.636000 Training: [26 epoch,  80 batch] loss: 3.04274, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:22:27.160726 Training: [26 epoch,  90 batch] loss: 3.04685, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:23:30.846837 Training: [26 epoch, 100 batch] loss: 3.02205, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:24:34.233513 Training: [26 epoch, 110 batch] loss: 2.97410, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:25:38.013202 Training: [26 epoch, 120 batch] loss: 2.97775, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:26:42.203885 Training: [26 epoch, 130 batch] loss: 3.06745, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:27:46.394933 Training: [26 epoch, 140 batch] loss: 3.00434, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:28:50.294265 Training: [26 epoch, 150 batch] loss: 3.02420, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:29:54.184003 Training: [26 epoch, 160 batch] loss: 2.94176, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:30:58.017021 Training: [26 epoch, 170 batch] loss: 2.98132, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:32:01.846170 Training: [26 epoch, 180 batch] loss: 2.93430, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:33:05.291677 Training: [26 epoch, 190 batch] loss: 2.89428, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:34:08.343960 Training: [26 epoch, 200 batch] loss: 2.96245, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01943,MAE：0.84756
2021-01-04 18:38:16.576323 Training: [27 epoch,  10 batch] loss: 2.93749, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:39:19.531785 Training: [27 epoch,  20 batch] loss: 2.87746, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:40:23.043026 Training: [27 epoch,  30 batch] loss: 2.91474, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:41:26.559516 Training: [27 epoch,  40 batch] loss: 2.90429, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:42:30.430411 Training: [27 epoch,  50 batch] loss: 2.90592, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:43:34.424675 Training: [27 epoch,  60 batch] loss: 2.88125, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:44:37.523087 Training: [27 epoch,  70 batch] loss: 2.89451, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:45:40.765936 Training: [27 epoch,  80 batch] loss: 2.87610, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:46:44.676797 Training: [27 epoch,  90 batch] loss: 2.86961, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:47:48.823941 Training: [27 epoch, 100 batch] loss: 2.90758, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:48:52.712416 Training: [27 epoch, 110 batch] loss: 2.84911, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:49:57.011724 Training: [27 epoch, 120 batch] loss: 2.87464, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:51:00.870082 Training: [27 epoch, 130 batch] loss: 2.85316, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:52:05.140585 Training: [27 epoch, 140 batch] loss: 2.89504, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:53:09.175264 Training: [27 epoch, 150 batch] loss: 2.87837, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:54:12.040724 Training: [27 epoch, 160 batch] loss: 2.83047, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:55:15.131885 Training: [27 epoch, 170 batch] loss: 2.78535, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:56:18.956076 Training: [27 epoch, 180 batch] loss: 2.81249, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:57:23.091761 Training: [27 epoch, 190 batch] loss: 2.75968, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 18:58:27.243443 Training: [27 epoch, 200 batch] loss: 2.78965, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01406,MAE：0.83753
2021-01-04 19:02:35.834763 Training: [28 epoch,  10 batch] loss: 2.78765, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:03:39.474084 Training: [28 epoch,  20 batch] loss: 2.80738, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:04:42.554579 Training: [28 epoch,  30 batch] loss: 2.79883, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:05:45.566733 Training: [28 epoch,  40 batch] loss: 2.80414, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:06:48.867535 Training: [28 epoch,  50 batch] loss: 2.80872, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:07:53.078006 Training: [28 epoch,  60 batch] loss: 2.72052, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:08:56.959663 Training: [28 epoch,  70 batch] loss: 2.81647, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:10:01.436696 Training: [28 epoch,  80 batch] loss: 2.67918, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:11:05.471978 Training: [28 epoch,  90 batch] loss: 2.66806, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:12:09.500321 Training: [28 epoch, 100 batch] loss: 2.71386, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:13:13.832312 Training: [28 epoch, 110 batch] loss: 2.68675, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:14:18.213095 Training: [28 epoch, 120 batch] loss: 2.73967, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:15:21.691882 Training: [28 epoch, 130 batch] loss: 2.74321, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:16:24.857336 Training: [28 epoch, 140 batch] loss: 2.74281, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:17:28.497330 Training: [28 epoch, 150 batch] loss: 2.64766, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:18:32.611775 Training: [28 epoch, 160 batch] loss: 2.74437, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:19:36.601216 Training: [28 epoch, 170 batch] loss: 2.62494, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:20:40.562769 Training: [28 epoch, 180 batch] loss: 2.69265, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:21:44.345498 Training: [28 epoch, 190 batch] loss: 2.65197, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:22:48.399655 Training: [28 epoch, 200 batch] loss: 2.70465, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03832,MAE：0.87776
2021-01-04 19:26:53.203973 Training: [29 epoch,  10 batch] loss: 2.61714, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:27:56.296168 Training: [29 epoch,  20 batch] loss: 2.66805, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:28:59.632371 Training: [29 epoch,  30 batch] loss: 2.67201, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:30:03.521221 Training: [29 epoch,  40 batch] loss: 2.58812, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:31:07.021239 Training: [29 epoch,  50 batch] loss: 2.61736, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:32:10.722286 Training: [29 epoch,  60 batch] loss: 2.64555, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:33:14.645698 Training: [29 epoch,  70 batch] loss: 2.62257, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:34:18.519009 Training: [29 epoch,  80 batch] loss: 2.59194, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:35:22.487859 Training: [29 epoch,  90 batch] loss: 2.54548, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:36:25.373561 Training: [29 epoch, 100 batch] loss: 2.57092, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:37:28.019268 Training: [29 epoch, 110 batch] loss: 2.57375, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:38:31.261661 Training: [29 epoch, 120 batch] loss: 2.60818, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:39:34.732578 Training: [29 epoch, 130 batch] loss: 2.55675, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:40:38.253287 Training: [29 epoch, 140 batch] loss: 2.60433, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:41:41.894120 Training: [29 epoch, 150 batch] loss: 2.58825, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:42:45.140647 Training: [29 epoch, 160 batch] loss: 2.57104, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:43:48.948217 Training: [29 epoch, 170 batch] loss: 2.56734, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:44:52.480085 Training: [29 epoch, 180 batch] loss: 2.58101, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:45:55.884680 Training: [29 epoch, 190 batch] loss: 2.49395, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:46:59.561045 Training: [29 epoch, 200 batch] loss: 2.57101, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.08856,MAE：0.93673
2021-01-04 19:51:06.662601 Training: [30 epoch,  10 batch] loss: 2.54510, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:52:10.304193 Training: [30 epoch,  20 batch] loss: 2.54125, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:53:14.570342 Training: [30 epoch,  30 batch] loss: 2.46595, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:54:19.237903 Training: [30 epoch,  40 batch] loss: 2.50992, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:55:23.516401 Training: [30 epoch,  50 batch] loss: 2.54829, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:56:27.472839 Training: [30 epoch,  60 batch] loss: 2.45424, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:57:31.727128 Training: [30 epoch,  70 batch] loss: 2.46147, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:58:34.782231 Training: [30 epoch,  80 batch] loss: 2.48763, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 19:59:41.323332 Training: [30 epoch,  90 batch] loss: 2.46978, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:00:45.287112 Training: [30 epoch, 100 batch] loss: 2.51131, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:01:49.551035 Training: [30 epoch, 110 batch] loss: 2.49935, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:02:53.899000 Training: [30 epoch, 120 batch] loss: 2.46182, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:03:57.309416 Training: [30 epoch, 130 batch] loss: 2.42304, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:05:00.875161 Training: [30 epoch, 140 batch] loss: 2.47346, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:06:04.840554 Training: [30 epoch, 150 batch] loss: 2.48347, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:07:08.084636 Training: [30 epoch, 160 batch] loss: 2.47549, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:08:11.476397 Training: [30 epoch, 170 batch] loss: 2.44906, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:09:14.316168 Training: [30 epoch, 180 batch] loss: 2.39131, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:10:17.646194 Training: [30 epoch, 190 batch] loss: 2.43603, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:11:20.553026 Training: [30 epoch, 200 batch] loss: 2.45105, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.00696,MAE：0.83572
2021-01-04 20:15:42.291892 Training: [31 epoch,  10 batch] loss: 2.40115, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:16:55.262389 Training: [31 epoch,  20 batch] loss: 2.38900, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:18:06.597017 Training: [31 epoch,  30 batch] loss: 2.36579, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:19:20.120492 Training: [31 epoch,  40 batch] loss: 2.40988, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:20:35.283345 Training: [31 epoch,  50 batch] loss: 2.40033, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:21:47.737657 Training: [31 epoch,  60 batch] loss: 2.41181, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:22:59.170656 Training: [31 epoch,  70 batch] loss: 2.38313, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:24:11.946152 Training: [31 epoch,  80 batch] loss: 2.35897, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:25:25.232577 Training: [31 epoch,  90 batch] loss: 2.40324, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:26:39.763163 Training: [31 epoch, 100 batch] loss: 2.39635, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:27:53.017609 Training: [31 epoch, 110 batch] loss: 2.37755, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:29:07.870611 Training: [31 epoch, 120 batch] loss: 2.35947, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:30:19.737624 Training: [31 epoch, 130 batch] loss: 2.42344, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:31:32.252526 Training: [31 epoch, 140 batch] loss: 2.37733, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:32:45.072834 Training: [31 epoch, 150 batch] loss: 2.37442, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:33:58.859183 Training: [31 epoch, 160 batch] loss: 2.32875, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:35:13.429958 Training: [31 epoch, 170 batch] loss: 2.32980, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:36:28.707913 Training: [31 epoch, 180 batch] loss: 2.31236, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:37:43.943049 Training: [31 epoch, 190 batch] loss: 2.29892, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:38:55.642159 Training: [31 epoch, 200 batch] loss: 2.29429, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01138,MAE：0.83829
2021-01-04 20:43:34.549599 Training: [32 epoch,  10 batch] loss: 2.24834, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:44:49.174203 Training: [32 epoch,  20 batch] loss: 2.36907, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:45:59.271591 Training: [32 epoch,  30 batch] loss: 2.31385, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:47:02.035599 Training: [32 epoch,  40 batch] loss: 2.26754, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:48:04.229599 Training: [32 epoch,  50 batch] loss: 2.30894, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:49:07.428408 Training: [32 epoch,  60 batch] loss: 2.23242, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:50:10.605570 Training: [32 epoch,  70 batch] loss: 2.34529, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:51:13.674789 Training: [32 epoch,  80 batch] loss: 2.27303, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:52:16.864137 Training: [32 epoch,  90 batch] loss: 2.27298, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:53:19.735645 Training: [32 epoch, 100 batch] loss: 2.31167, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:54:22.785656 Training: [32 epoch, 110 batch] loss: 2.26291, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:55:26.147987 Training: [32 epoch, 120 batch] loss: 2.22501, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:56:29.630743 Training: [32 epoch, 130 batch] loss: 2.28816, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:57:32.931919 Training: [32 epoch, 140 batch] loss: 2.27690, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:58:36.695775 Training: [32 epoch, 150 batch] loss: 2.22163, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 20:59:41.027119 Training: [32 epoch, 160 batch] loss: 2.25864, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:00:45.153123 Training: [32 epoch, 170 batch] loss: 2.23523, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:01:49.320095 Training: [32 epoch, 180 batch] loss: 2.25951, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:02:53.326366 Training: [32 epoch, 190 batch] loss: 2.26063, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:03:57.459202 Training: [32 epoch, 200 batch] loss: 2.23626, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.04785,MAE：0.89029
2021-01-04 21:08:04.450359 Training: [33 epoch,  10 batch] loss: 2.22851, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:09:07.815704 Training: [33 epoch,  20 batch] loss: 2.21479, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:10:10.680694 Training: [33 epoch,  30 batch] loss: 2.27925, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:11:13.132779 Training: [33 epoch,  40 batch] loss: 2.20835, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:12:16.544725 Training: [33 epoch,  50 batch] loss: 2.18823, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:13:21.283648 Training: [33 epoch,  60 batch] loss: 2.23479, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:14:24.496307 Training: [33 epoch,  70 batch] loss: 2.17891, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:15:28.111329 Training: [33 epoch,  80 batch] loss: 2.18141, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:16:32.784729 Training: [33 epoch,  90 batch] loss: 2.24433, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:17:37.053135 Training: [33 epoch, 100 batch] loss: 2.17208, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:18:41.445219 Training: [33 epoch, 110 batch] loss: 2.18022, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:19:45.759653 Training: [33 epoch, 120 batch] loss: 2.23170, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:20:50.307927 Training: [33 epoch, 130 batch] loss: 2.19439, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:21:54.738240 Training: [33 epoch, 140 batch] loss: 2.19121, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:22:59.114302 Training: [33 epoch, 150 batch] loss: 2.18386, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:24:03.589318 Training: [33 epoch, 160 batch] loss: 2.16192, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:25:08.039546 Training: [33 epoch, 170 batch] loss: 2.13830, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:26:20.099805 Training: [33 epoch, 180 batch] loss: 2.13588, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:27:36.039172 Training: [33 epoch, 190 batch] loss: 2.14077, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:28:52.060165 Training: [33 epoch, 200 batch] loss: 2.14014, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03791,MAE：0.87284
2021-01-04 21:34:10.070892 Training: [34 epoch,  10 batch] loss: 2.13898, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:35:23.205621 Training: [34 epoch,  20 batch] loss: 2.13199, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:36:35.013315 Training: [34 epoch,  30 batch] loss: 2.15655, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:37:48.195576 Training: [34 epoch,  40 batch] loss: 2.16649, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:39:03.939141 Training: [34 epoch,  50 batch] loss: 2.14424, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:40:19.580505 Training: [34 epoch,  60 batch] loss: 2.10702, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:41:35.235807 Training: [34 epoch,  70 batch] loss: 2.10378, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:42:50.682225 Training: [34 epoch,  80 batch] loss: 2.11437, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:44:06.315782 Training: [34 epoch,  90 batch] loss: 2.08400, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:45:22.357359 Training: [34 epoch, 100 batch] loss: 2.15783, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:46:38.378215 Training: [34 epoch, 110 batch] loss: 2.07155, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:47:54.082811 Training: [34 epoch, 120 batch] loss: 2.08046, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:49:10.133289 Training: [34 epoch, 130 batch] loss: 2.06886, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:50:26.328512 Training: [34 epoch, 140 batch] loss: 2.11581, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:51:42.310499 Training: [34 epoch, 150 batch] loss: 2.12030, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:52:58.148684 Training: [34 epoch, 160 batch] loss: 2.08336, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:54:14.004108 Training: [34 epoch, 170 batch] loss: 2.10416, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:55:30.291546 Training: [34 epoch, 180 batch] loss: 2.09117, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:56:46.283945 Training: [34 epoch, 190 batch] loss: 2.10610, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 21:58:02.013659 Training: [34 epoch, 200 batch] loss: 2.07552, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01302,MAE：0.82388
2021-01-04 22:03:05.029422 Training: [35 epoch,  10 batch] loss: 2.05955, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:04:19.784692 Training: [35 epoch,  20 batch] loss: 2.05582, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:05:35.440538 Training: [35 epoch,  30 batch] loss: 2.05358, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:06:51.146887 Training: [35 epoch,  40 batch] loss: 2.01956, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:08:07.044404 Training: [35 epoch,  50 batch] loss: 2.04706, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:09:22.878947 Training: [35 epoch,  60 batch] loss: 2.05404, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:10:38.824734 Training: [35 epoch,  70 batch] loss: 2.07448, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:11:54.712821 Training: [35 epoch,  80 batch] loss: 2.05627, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:13:10.700438 Training: [35 epoch,  90 batch] loss: 2.03946, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:14:26.716838 Training: [35 epoch, 100 batch] loss: 2.06551, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:15:42.617687 Training: [35 epoch, 110 batch] loss: 2.03517, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:16:58.385490 Training: [35 epoch, 120 batch] loss: 2.02480, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:18:14.274316 Training: [35 epoch, 130 batch] loss: 1.99776, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:19:30.370915 Training: [35 epoch, 140 batch] loss: 1.99554, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:20:46.399451 Training: [35 epoch, 150 batch] loss: 2.02195, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:22:02.080627 Training: [35 epoch, 160 batch] loss: 2.04767, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:23:18.062805 Training: [35 epoch, 170 batch] loss: 2.03786, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:24:32.668124 Training: [35 epoch, 180 batch] loss: 1.97644, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:25:46.870643 Training: [35 epoch, 190 batch] loss: 2.00664, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:27:02.237013 Training: [35 epoch, 200 batch] loss: 2.05358, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.04902,MAE：0.89050
2021-01-04 22:32:17.550964 Training: [36 epoch,  10 batch] loss: 2.02175, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:33:33.285205 Training: [36 epoch,  20 batch] loss: 2.01553, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:34:48.392065 Training: [36 epoch,  30 batch] loss: 2.01921, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:36:03.255000 Training: [36 epoch,  40 batch] loss: 1.96522, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:37:17.021036 Training: [36 epoch,  50 batch] loss: 1.96685, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:38:30.697262 Training: [36 epoch,  60 batch] loss: 1.91924, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:39:44.477374 Training: [36 epoch,  70 batch] loss: 1.93547, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:40:59.532814 Training: [36 epoch,  80 batch] loss: 2.00459, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:42:15.616926 Training: [36 epoch,  90 batch] loss: 2.05915, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:43:31.417702 Training: [36 epoch, 100 batch] loss: 1.94839, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:44:47.400225 Training: [36 epoch, 110 batch] loss: 1.97228, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:46:03.416319 Training: [36 epoch, 120 batch] loss: 1.97244, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:47:19.145876 Training: [36 epoch, 130 batch] loss: 2.00224, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:48:35.116625 Training: [36 epoch, 140 batch] loss: 1.96611, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:49:49.584643 Training: [36 epoch, 150 batch] loss: 1.97069, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:51:04.172293 Training: [36 epoch, 160 batch] loss: 1.97455, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:52:19.725421 Training: [36 epoch, 170 batch] loss: 1.90841, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:53:33.671056 Training: [36 epoch, 180 batch] loss: 1.92234, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:54:47.571538 Training: [36 epoch, 190 batch] loss: 1.97261, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 22:56:03.417021 Training: [36 epoch, 200 batch] loss: 1.95994, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03779,MAE：0.87333
2021-01-04 23:01:24.170683 Training: [37 epoch,  10 batch] loss: 1.92255, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:02:39.696777 Training: [37 epoch,  20 batch] loss: 1.94219, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:03:55.551168 Training: [37 epoch,  30 batch] loss: 1.90185, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:05:11.419842 Training: [37 epoch,  40 batch] loss: 1.95060, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:06:27.140478 Training: [37 epoch,  50 batch] loss: 1.96735, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:07:42.954283 Training: [37 epoch,  60 batch] loss: 1.91932, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:08:58.874534 Training: [37 epoch,  70 batch] loss: 1.92811, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:10:15.051244 Training: [37 epoch,  80 batch] loss: 1.94336, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:11:31.175763 Training: [37 epoch,  90 batch] loss: 1.90796, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:12:47.117038 Training: [37 epoch, 100 batch] loss: 1.84816, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:13:59.879608 Training: [37 epoch, 110 batch] loss: 1.93617, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:15:12.536725 Training: [37 epoch, 120 batch] loss: 1.93985, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:16:26.396100 Training: [37 epoch, 130 batch] loss: 1.86922, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:17:40.331126 Training: [37 epoch, 140 batch] loss: 1.90139, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:18:53.867672 Training: [37 epoch, 150 batch] loss: 1.92357, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:20:07.870616 Training: [37 epoch, 160 batch] loss: 1.89865, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:21:24.091535 Training: [37 epoch, 170 batch] loss: 1.89514, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:22:40.295813 Training: [37 epoch, 180 batch] loss: 1.86843, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:23:56.423936 Training: [37 epoch, 190 batch] loss: 1.86957, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:25:12.343481 Training: [37 epoch, 200 batch] loss: 1.84973, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01635,MAE：0.83282
2021-01-04 23:30:32.701527 Training: [38 epoch,  10 batch] loss: 1.89280, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:31:47.979078 Training: [38 epoch,  20 batch] loss: 1.88422, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:33:03.527044 Training: [38 epoch,  30 batch] loss: 1.89588, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:34:19.183052 Training: [38 epoch,  40 batch] loss: 1.88531, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:35:35.024543 Training: [38 epoch,  50 batch] loss: 1.88173, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:36:50.764669 Training: [38 epoch,  60 batch] loss: 1.88916, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:38:05.044533 Training: [38 epoch,  70 batch] loss: 1.90768, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:39:19.141045 Training: [38 epoch,  80 batch] loss: 1.86150, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:40:34.290567 Training: [38 epoch,  90 batch] loss: 1.88523, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:41:50.332522 Training: [38 epoch, 100 batch] loss: 1.83970, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:43:04.068899 Training: [38 epoch, 110 batch] loss: 1.84921, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:44:17.483837 Training: [38 epoch, 120 batch] loss: 1.84066, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:45:32.572454 Training: [38 epoch, 130 batch] loss: 1.84732, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:46:48.579751 Training: [38 epoch, 140 batch] loss: 1.81693, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:48:04.644380 Training: [38 epoch, 150 batch] loss: 1.81951, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:49:20.504631 Training: [38 epoch, 160 batch] loss: 1.89551, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:50:35.705249 Training: [38 epoch, 170 batch] loss: 1.81523, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:51:49.917467 Training: [38 epoch, 180 batch] loss: 1.86311, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:53:04.114713 Training: [38 epoch, 190 batch] loss: 1.81233, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-04 23:54:18.476425 Training: [38 epoch, 200 batch] loss: 1.84652, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.12994,MAE：0.97967
2021-01-04 23:59:38.822576 Training: [39 epoch,  10 batch] loss: 1.80794, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:00:54.429080 Training: [39 epoch,  20 batch] loss: 1.82530, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:02:08.912026 Training: [39 epoch,  30 batch] loss: 1.77205, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:03:23.003185 Training: [39 epoch,  40 batch] loss: 1.78422, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:04:38.132992 Training: [39 epoch,  50 batch] loss: 1.82309, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:05:53.540896 Training: [39 epoch,  60 batch] loss: 1.78196, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:07:08.465539 Training: [39 epoch,  70 batch] loss: 1.82142, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:08:22.219643 Training: [39 epoch,  80 batch] loss: 1.82294, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:09:36.488910 Training: [39 epoch,  90 batch] loss: 1.82762, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:10:52.561070 Training: [39 epoch, 100 batch] loss: 1.80297, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:12:08.376365 Training: [39 epoch, 110 batch] loss: 1.82744, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:13:24.408637 Training: [39 epoch, 120 batch] loss: 1.87226, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:14:40.491915 Training: [39 epoch, 130 batch] loss: 1.83026, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:15:56.472283 Training: [39 epoch, 140 batch] loss: 1.84727, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:17:12.214779 Training: [39 epoch, 150 batch] loss: 1.78307, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:18:28.084255 Training: [39 epoch, 160 batch] loss: 1.81429, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:19:44.134633 Training: [39 epoch, 170 batch] loss: 1.80843, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:21:00.235001 Training: [39 epoch, 180 batch] loss: 1.79828, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:22:16.121270 Training: [39 epoch, 190 batch] loss: 1.76163, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:23:32.071820 Training: [39 epoch, 200 batch] loss: 1.82677, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.03466,MAE：0.86820
2021-01-05 00:28:44.628193 Training: [40 epoch,  10 batch] loss: 1.78005, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:29:58.346357 Training: [40 epoch,  20 batch] loss: 1.79245, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:31:12.423449 Training: [40 epoch,  30 batch] loss: 1.80738, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:32:25.172416 Training: [40 epoch,  40 batch] loss: 1.85014, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:33:38.691970 Training: [40 epoch,  50 batch] loss: 1.76316, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:34:53.816884 Training: [40 epoch,  60 batch] loss: 1.75180, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:36:09.955734 Training: [40 epoch,  70 batch] loss: 1.73496, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:37:25.965296 Training: [40 epoch,  80 batch] loss: 1.79376, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:38:42.056750 Training: [40 epoch,  90 batch] loss: 1.79439, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:39:58.023963 Training: [40 epoch, 100 batch] loss: 1.76343, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:41:14.356770 Training: [40 epoch, 110 batch] loss: 1.79243, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:42:30.407245 Training: [40 epoch, 120 batch] loss: 1.83156, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:43:46.072151 Training: [40 epoch, 130 batch] loss: 1.77655, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:45:02.004856 Training: [40 epoch, 140 batch] loss: 1.71461, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:46:17.904572 Training: [40 epoch, 150 batch] loss: 1.73547, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:47:33.628941 Training: [40 epoch, 160 batch] loss: 1.75664, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:48:49.390648 Training: [40 epoch, 170 batch] loss: 1.72729, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:50:05.360727 Training: [40 epoch, 180 batch] loss: 1.75085, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:51:19.500495 Training: [40 epoch, 190 batch] loss: 1.76174, the best RMSE/MAE: 0.95449 / 0.75088
2021-01-05 00:52:33.624968 Training: [40 epoch, 200 batch] loss: 1.72250, the best RMSE/MAE: 0.95449 / 0.75088
<Test> RMSE：1.01378,MAE：0.82404
The best RMSE/MAE：0.95449/0.75088
