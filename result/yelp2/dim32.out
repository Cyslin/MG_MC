-------------------- Hyperparams --------------------
time: 2021-01-05 18:37:20.408788
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-05 18:49:59.573157 Training: [1 epoch,  10 batch] loss: 8.72847, the best RMSE/MAE: inf / inf
2021-01-05 18:51:08.860399 Training: [1 epoch,  20 batch] loss: 8.34928, the best RMSE/MAE: inf / inf
2021-01-05 18:52:27.994893 Training: [1 epoch,  30 batch] loss: 8.06954, the best RMSE/MAE: inf / inf
2021-01-05 18:53:48.009191 Training: [1 epoch,  40 batch] loss: 7.81873, the best RMSE/MAE: inf / inf
2021-01-05 18:55:03.822376 Training: [1 epoch,  50 batch] loss: 7.60590, the best RMSE/MAE: inf / inf
2021-01-05 18:56:24.124126 Training: [1 epoch,  60 batch] loss: 7.39854, the best RMSE/MAE: inf / inf
2021-01-05 18:57:42.651530 Training: [1 epoch,  70 batch] loss: 7.31889, the best RMSE/MAE: inf / inf
2021-01-05 18:59:01.478036 Training: [1 epoch,  80 batch] loss: 7.29677, the best RMSE/MAE: inf / inf
2021-01-05 19:00:19.346318 Training: [1 epoch,  90 batch] loss: 7.22644, the best RMSE/MAE: inf / inf
<Test> RMSE：71127272.00000,MAE：40446160.00000
2021-01-05 19:04:11.702190 Training: [2 epoch,  10 batch] loss: 7.17031, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:05:31.999143 Training: [2 epoch,  20 batch] loss: 7.22070, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:06:55.572713 Training: [2 epoch,  30 batch] loss: 7.11364, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:08:17.434415 Training: [2 epoch,  40 batch] loss: 7.08314, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:09:36.034978 Training: [2 epoch,  50 batch] loss: 7.04026, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:10:54.903727 Training: [2 epoch,  60 batch] loss: 7.05980, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:12:13.804600 Training: [2 epoch,  70 batch] loss: 7.02686, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:13:21.963700 Training: [2 epoch,  80 batch] loss: 7.01931, the best RMSE/MAE: 71127272.00000 / 40446160.00000
2021-01-05 19:14:30.406907 Training: [2 epoch,  90 batch] loss: 6.97078, the best RMSE/MAE: 71127272.00000 / 40446160.00000
<Test> RMSE：141486.67188,MAE：92271.96094
2021-01-05 19:17:49.985199 Training: [3 epoch,  10 batch] loss: 6.93953, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:18:58.755668 Training: [3 epoch,  20 batch] loss: 6.96199, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:20:10.877670 Training: [3 epoch,  30 batch] loss: 6.90769, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:21:20.958678 Training: [3 epoch,  40 batch] loss: 6.87499, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:22:31.096214 Training: [3 epoch,  50 batch] loss: 6.88394, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:23:40.966064 Training: [3 epoch,  60 batch] loss: 6.91517, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:24:51.925574 Training: [3 epoch,  70 batch] loss: 6.81398, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:26:05.031815 Training: [3 epoch,  80 batch] loss: 6.80770, the best RMSE/MAE: 141486.67188 / 92271.96094
2021-01-05 19:27:16.981947 Training: [3 epoch,  90 batch] loss: 6.84853, the best RMSE/MAE: 141486.67188 / 92271.96094
<Test> RMSE：2927.77124,MAE：1831.17249
2021-01-05 19:30:45.754147 Training: [4 epoch,  10 batch] loss: 6.81559, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:31:57.575573 Training: [4 epoch,  20 batch] loss: 6.76843, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:33:09.980222 Training: [4 epoch,  30 batch] loss: 6.81063, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:34:20.298495 Training: [4 epoch,  40 batch] loss: 6.73934, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:35:28.998698 Training: [4 epoch,  50 batch] loss: 6.68322, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:36:41.140022 Training: [4 epoch,  60 batch] loss: 6.69228, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:37:53.541754 Training: [4 epoch,  70 batch] loss: 6.67324, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:39:06.590537 Training: [4 epoch,  80 batch] loss: 6.65867, the best RMSE/MAE: 2927.77124 / 1831.17249
2021-01-05 19:40:17.718877 Training: [4 epoch,  90 batch] loss: 6.65436, the best RMSE/MAE: 2927.77124 / 1831.17249
<Test> RMSE：442.69009,MAE：232.04689
2021-01-05 19:43:40.433112 Training: [5 epoch,  10 batch] loss: 6.62809, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:44:55.886118 Training: [5 epoch,  20 batch] loss: 6.55996, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:46:13.846756 Training: [5 epoch,  30 batch] loss: 6.58907, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:47:25.795633 Training: [5 epoch,  40 batch] loss: 6.59151, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:48:39.164591 Training: [5 epoch,  50 batch] loss: 6.57907, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:49:56.640172 Training: [5 epoch,  60 batch] loss: 6.50849, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:51:12.564645 Training: [5 epoch,  70 batch] loss: 6.52973, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:52:30.155665 Training: [5 epoch,  80 batch] loss: 6.47160, the best RMSE/MAE: 442.69009 / 232.04689
2021-01-05 19:53:46.046386 Training: [5 epoch,  90 batch] loss: 6.48818, the best RMSE/MAE: 442.69009 / 232.04689
<Test> RMSE：69.77477,MAE：45.96733
2021-01-05 19:57:32.949368 Training: [6 epoch,  10 batch] loss: 6.40236, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 19:58:52.757193 Training: [6 epoch,  20 batch] loss: 6.41050, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:00:10.660327 Training: [6 epoch,  30 batch] loss: 6.43577, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:01:22.847047 Training: [6 epoch,  40 batch] loss: 6.36312, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:02:28.159351 Training: [6 epoch,  50 batch] loss: 6.35995, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:03:32.089640 Training: [6 epoch,  60 batch] loss: 6.40425, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:04:35.768722 Training: [6 epoch,  70 batch] loss: 6.36170, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:05:38.567776 Training: [6 epoch,  80 batch] loss: 6.29618, the best RMSE/MAE: 69.77477 / 45.96733
2021-01-05 20:06:41.781777 Training: [6 epoch,  90 batch] loss: 6.26733, the best RMSE/MAE: 69.77477 / 45.96733
<Test> RMSE：23.80055,MAE：16.66914
2021-01-05 20:09:47.875088 Training: [7 epoch,  10 batch] loss: 6.22621, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:10:52.012773 Training: [7 epoch,  20 batch] loss: 6.22899, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:11:55.356510 Training: [7 epoch,  30 batch] loss: 6.20365, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:12:55.945961 Training: [7 epoch,  40 batch] loss: 6.20098, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:13:58.945393 Training: [7 epoch,  50 batch] loss: 6.21173, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:15:03.017834 Training: [7 epoch,  60 batch] loss: 6.16436, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:16:05.378935 Training: [7 epoch,  70 batch] loss: 6.12220, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:17:10.023746 Training: [7 epoch,  80 batch] loss: 6.11150, the best RMSE/MAE: 23.80055 / 16.66914
2021-01-05 20:18:12.386262 Training: [7 epoch,  90 batch] loss: 6.11922, the best RMSE/MAE: 23.80055 / 16.66914
<Test> RMSE：9.56485,MAE：7.58477
2021-01-05 20:21:11.324958 Training: [8 epoch,  10 batch] loss: 6.05156, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:22:14.753694 Training: [8 epoch,  20 batch] loss: 6.10994, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:23:17.466866 Training: [8 epoch,  30 batch] loss: 6.02016, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:24:19.719234 Training: [8 epoch,  40 batch] loss: 5.99123, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:25:22.623157 Training: [8 epoch,  50 batch] loss: 5.96485, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:26:25.318941 Training: [8 epoch,  60 batch] loss: 5.93368, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:27:29.281889 Training: [8 epoch,  70 batch] loss: 5.89929, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:28:33.891963 Training: [8 epoch,  80 batch] loss: 5.91349, the best RMSE/MAE: 9.56485 / 7.58477
2021-01-05 20:29:35.059004 Training: [8 epoch,  90 batch] loss: 5.87541, the best RMSE/MAE: 9.56485 / 7.58477
<Test> RMSE：4.59413,MAE：3.51184
2021-01-05 20:32:38.718885 Training: [9 epoch,  10 batch] loss: 5.84804, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:33:42.961828 Training: [9 epoch,  20 batch] loss: 5.82995, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:34:47.162458 Training: [9 epoch,  30 batch] loss: 5.80703, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:35:49.130489 Training: [9 epoch,  40 batch] loss: 5.77212, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:36:54.105346 Training: [9 epoch,  50 batch] loss: 5.73418, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:37:59.011217 Training: [9 epoch,  60 batch] loss: 5.73915, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:39:05.825473 Training: [9 epoch,  70 batch] loss: 5.76254, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:40:11.133930 Training: [9 epoch,  80 batch] loss: 5.70877, the best RMSE/MAE: 4.59413 / 3.51184
2021-01-05 20:41:14.079027 Training: [9 epoch,  90 batch] loss: 5.67955, the best RMSE/MAE: 4.59413 / 3.51184
<Test> RMSE：3.27262,MAE：2.46634
2021-01-05 20:44:15.376752 Training: [10 epoch,  10 batch] loss: 5.63430, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:45:19.998435 Training: [10 epoch,  20 batch] loss: 5.63147, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:46:28.838878 Training: [10 epoch,  30 batch] loss: 5.62810, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:47:29.247114 Training: [10 epoch,  40 batch] loss: 5.58126, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:48:32.725534 Training: [10 epoch,  50 batch] loss: 5.51282, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:49:37.471054 Training: [10 epoch,  60 batch] loss: 5.49187, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:50:39.594218 Training: [10 epoch,  70 batch] loss: 5.48390, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:51:42.343111 Training: [10 epoch,  80 batch] loss: 5.51031, the best RMSE/MAE: 3.27262 / 2.46634
2021-01-05 20:52:44.616071 Training: [10 epoch,  90 batch] loss: 5.42652, the best RMSE/MAE: 3.27262 / 2.46634
<Test> RMSE：1.78652,MAE：1.30138
2021-01-05 20:55:45.421150 Training: [11 epoch,  10 batch] loss: 5.38187, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 20:56:49.448528 Training: [11 epoch,  20 batch] loss: 5.39179, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 20:57:51.405297 Training: [11 epoch,  30 batch] loss: 5.38612, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 20:58:46.484328 Training: [11 epoch,  40 batch] loss: 5.40310, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 20:59:42.219875 Training: [11 epoch,  50 batch] loss: 5.29563, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 21:00:38.119339 Training: [11 epoch,  60 batch] loss: 5.26816, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 21:01:33.958956 Training: [11 epoch,  70 batch] loss: 5.29614, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 21:02:29.111326 Training: [11 epoch,  80 batch] loss: 5.22073, the best RMSE/MAE: 1.78652 / 1.30138
2021-01-05 21:03:23.327171 Training: [11 epoch,  90 batch] loss: 5.22420, the best RMSE/MAE: 1.78652 / 1.30138
<Test> RMSE：1.37691,MAE：1.00957
2021-01-05 21:06:02.014982 Training: [12 epoch,  10 batch] loss: 5.16532, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:06:57.349628 Training: [12 epoch,  20 batch] loss: 5.13395, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:07:52.610456 Training: [12 epoch,  30 batch] loss: 5.10462, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:08:47.035004 Training: [12 epoch,  40 batch] loss: 5.08660, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:09:41.955258 Training: [12 epoch,  50 batch] loss: 5.12543, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:10:37.549110 Training: [12 epoch,  60 batch] loss: 5.07932, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:11:35.513164 Training: [12 epoch,  70 batch] loss: 5.03362, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:12:34.334606 Training: [12 epoch,  80 batch] loss: 5.00865, the best RMSE/MAE: 1.37691 / 1.00957
2021-01-05 21:13:37.010720 Training: [12 epoch,  90 batch] loss: 5.06554, the best RMSE/MAE: 1.37691 / 1.00957
<Test> RMSE：0.97261,MAE：0.69920
2021-01-05 21:16:41.661570 Training: [13 epoch,  10 batch] loss: 5.00263, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:17:48.562671 Training: [13 epoch,  20 batch] loss: 4.96315, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:18:50.921707 Training: [13 epoch,  30 batch] loss: 4.87602, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:19:52.228042 Training: [13 epoch,  40 batch] loss: 4.87353, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:20:56.838996 Training: [13 epoch,  50 batch] loss: 4.83341, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:21:59.621928 Training: [13 epoch,  60 batch] loss: 4.82227, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:23:03.129103 Training: [13 epoch,  70 batch] loss: 4.81156, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:24:06.819784 Training: [13 epoch,  80 batch] loss: 4.76537, the best RMSE/MAE: 0.97261 / 0.69920
2021-01-05 21:25:08.615978 Training: [13 epoch,  90 batch] loss: 4.73976, the best RMSE/MAE: 0.97261 / 0.69920
<Test> RMSE：0.69893,MAE：0.45855
2021-01-05 21:28:11.705038 Training: [14 epoch,  10 batch] loss: 4.69855, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:29:14.152773 Training: [14 epoch,  20 batch] loss: 4.68953, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:30:17.625137 Training: [14 epoch,  30 batch] loss: 4.72727, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:31:19.676888 Training: [14 epoch,  40 batch] loss: 4.64233, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:32:21.415973 Training: [14 epoch,  50 batch] loss: 4.61150, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:33:25.214404 Training: [14 epoch,  60 batch] loss: 4.59655, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:34:29.819308 Training: [14 epoch,  70 batch] loss: 4.55814, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:35:31.875655 Training: [14 epoch,  80 batch] loss: 4.53615, the best RMSE/MAE: 0.69893 / 0.45855
2021-01-05 21:36:35.308078 Training: [14 epoch,  90 batch] loss: 4.52243, the best RMSE/MAE: 0.69893 / 0.45855
<Test> RMSE：0.57036,MAE：0.38308
2021-01-05 21:39:45.107300 Training: [15 epoch,  10 batch] loss: 4.46201, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:40:49.498153 Training: [15 epoch,  20 batch] loss: 4.44423, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:41:51.618229 Training: [15 epoch,  30 batch] loss: 4.45505, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:42:53.312251 Training: [15 epoch,  40 batch] loss: 4.39463, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:43:58.821754 Training: [15 epoch,  50 batch] loss: 4.36521, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:45:03.010504 Training: [15 epoch,  60 batch] loss: 4.39029, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:46:06.799138 Training: [15 epoch,  70 batch] loss: 4.36114, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:47:13.224443 Training: [15 epoch,  80 batch] loss: 4.33096, the best RMSE/MAE: 0.57036 / 0.38308
2021-01-05 21:48:21.247653 Training: [15 epoch,  90 batch] loss: 4.30000, the best RMSE/MAE: 0.57036 / 0.38308
<Test> RMSE：0.53566,MAE：0.32083
2021-01-05 21:51:26.035888 Training: [16 epoch,  10 batch] loss: 4.23512, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:52:28.708103 Training: [16 epoch,  20 batch] loss: 4.21367, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:53:31.798027 Training: [16 epoch,  30 batch] loss: 4.20584, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:54:33.821422 Training: [16 epoch,  40 batch] loss: 4.15504, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:55:36.567986 Training: [16 epoch,  50 batch] loss: 4.12963, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:56:41.007615 Training: [16 epoch,  60 batch] loss: 4.13130, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:57:45.386922 Training: [16 epoch,  70 batch] loss: 4.13675, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:58:46.295377 Training: [16 epoch,  80 batch] loss: 4.16692, the best RMSE/MAE: 0.53566 / 0.32083
2021-01-05 21:59:49.169457 Training: [16 epoch,  90 batch] loss: 4.04675, the best RMSE/MAE: 0.53566 / 0.32083
<Test> RMSE：0.44635,MAE：0.23983
2021-01-05 22:02:52.890029 Training: [17 epoch,  10 batch] loss: 4.00925, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:03:57.309073 Training: [17 epoch,  20 batch] loss: 3.96567, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:05:00.566768 Training: [17 epoch,  30 batch] loss: 3.97659, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:06:01.227258 Training: [17 epoch,  40 batch] loss: 3.98109, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:07:05.607346 Training: [17 epoch,  50 batch] loss: 3.90971, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:08:09.339816 Training: [17 epoch,  60 batch] loss: 3.88184, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:09:12.292743 Training: [17 epoch,  70 batch] loss: 3.91681, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:10:14.855510 Training: [17 epoch,  80 batch] loss: 3.90842, the best RMSE/MAE: 0.44635 / 0.23983
2021-01-05 22:11:18.527934 Training: [17 epoch,  90 batch] loss: 3.83359, the best RMSE/MAE: 0.44635 / 0.23983
<Test> RMSE：0.43603,MAE：0.25068
2021-01-05 22:14:22.367029 Training: [18 epoch,  10 batch] loss: 3.79411, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:15:24.900094 Training: [18 epoch,  20 batch] loss: 3.79342, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:16:28.409928 Training: [18 epoch,  30 batch] loss: 3.74762, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:17:30.563052 Training: [18 epoch,  40 batch] loss: 3.74230, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:18:32.797070 Training: [18 epoch,  50 batch] loss: 3.69890, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:19:35.652805 Training: [18 epoch,  60 batch] loss: 3.70502, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:20:39.362870 Training: [18 epoch,  70 batch] loss: 3.65930, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:21:40.814819 Training: [18 epoch,  80 batch] loss: 3.63311, the best RMSE/MAE: 0.43603 / 0.25068
2021-01-05 22:22:44.255645 Training: [18 epoch,  90 batch] loss: 3.65086, the best RMSE/MAE: 0.43603 / 0.25068
<Test> RMSE：0.40196,MAE：0.19526
2021-01-05 22:25:53.521781 Training: [19 epoch,  10 batch] loss: 3.59914, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:26:58.690474 Training: [19 epoch,  20 batch] loss: 3.55829, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:28:09.835116 Training: [19 epoch,  30 batch] loss: 3.53272, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:29:11.730814 Training: [19 epoch,  40 batch] loss: 3.51391, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:30:16.995024 Training: [19 epoch,  50 batch] loss: 3.49656, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:31:21.230995 Training: [19 epoch,  60 batch] loss: 3.49103, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:32:25.023167 Training: [19 epoch,  70 batch] loss: 3.45863, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:33:28.784019 Training: [19 epoch,  80 batch] loss: 3.44977, the best RMSE/MAE: 0.40196 / 0.19526
2021-01-05 22:34:42.143188 Training: [19 epoch,  90 batch] loss: 3.42192, the best RMSE/MAE: 0.40196 / 0.19526
<Test> RMSE：0.38850,MAE：0.17416
2021-01-05 22:37:57.737183 Training: [20 epoch,  10 batch] loss: 3.41032, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:39:02.886716 Training: [20 epoch,  20 batch] loss: 3.36847, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:40:05.886229 Training: [20 epoch,  30 batch] loss: 3.35476, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:41:07.900528 Training: [20 epoch,  40 batch] loss: 3.32851, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:42:10.415268 Training: [20 epoch,  50 batch] loss: 3.28762, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:43:13.904815 Training: [20 epoch,  60 batch] loss: 3.26724, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:44:17.206578 Training: [20 epoch,  70 batch] loss: 3.27589, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:45:18.059857 Training: [20 epoch,  80 batch] loss: 3.23611, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:46:22.315206 Training: [20 epoch,  90 batch] loss: 3.18695, the best RMSE/MAE: 0.38850 / 0.17416
<Test> RMSE：0.39003,MAE：0.15874
2021-01-05 22:49:27.069165 Training: [21 epoch,  10 batch] loss: 3.17896, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:50:31.259396 Training: [21 epoch,  20 batch] loss: 3.15314, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:51:32.920694 Training: [21 epoch,  30 batch] loss: 3.15897, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:52:30.575334 Training: [21 epoch,  40 batch] loss: 3.12901, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:53:28.264363 Training: [21 epoch,  50 batch] loss: 3.09412, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:54:26.669137 Training: [21 epoch,  60 batch] loss: 3.10228, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:55:24.632306 Training: [21 epoch,  70 batch] loss: 3.05164, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:56:21.783431 Training: [21 epoch,  80 batch] loss: 3.06474, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 22:57:21.263847 Training: [21 epoch,  90 batch] loss: 3.03729, the best RMSE/MAE: 0.38850 / 0.17416
<Test> RMSE：0.39407,MAE：0.17810
2021-01-05 23:00:10.043856 Training: [22 epoch,  10 batch] loss: 3.00558, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:01:11.081012 Training: [22 epoch,  20 batch] loss: 2.96105, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:02:08.792995 Training: [22 epoch,  30 batch] loss: 2.97174, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:03:07.742060 Training: [22 epoch,  40 batch] loss: 2.91291, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:04:06.629117 Training: [22 epoch,  50 batch] loss: 2.94537, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:05:05.643763 Training: [22 epoch,  60 batch] loss: 2.88799, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:06:03.584975 Training: [22 epoch,  70 batch] loss: 2.89952, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:07:02.067374 Training: [22 epoch,  80 batch] loss: 2.90209, the best RMSE/MAE: 0.38850 / 0.17416
2021-01-05 23:08:01.234970 Training: [22 epoch,  90 batch] loss: 2.86282, the best RMSE/MAE: 0.38850 / 0.17416
<Test> RMSE：0.38524,MAE：0.16603
2021-01-05 23:10:45.873205 Training: [23 epoch,  10 batch] loss: 2.82154, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:11:43.418976 Training: [23 epoch,  20 batch] loss: 2.80951, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:12:42.182198 Training: [23 epoch,  30 batch] loss: 2.78795, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:13:41.305961 Training: [23 epoch,  40 batch] loss: 2.77098, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:14:40.434861 Training: [23 epoch,  50 batch] loss: 2.72905, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:15:38.572713 Training: [23 epoch,  60 batch] loss: 2.74221, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:16:37.048033 Training: [23 epoch,  70 batch] loss: 2.76062, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:17:36.519100 Training: [23 epoch,  80 batch] loss: 2.70350, the best RMSE/MAE: 0.38524 / 0.16603
2021-01-05 23:18:36.084520 Training: [23 epoch,  90 batch] loss: 2.68221, the best RMSE/MAE: 0.38524 / 0.16603
<Test> RMSE：0.38484,MAE：0.15789
2021-01-05 23:21:19.225764 Training: [24 epoch,  10 batch] loss: 2.64488, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:22:13.801646 Training: [24 epoch,  20 batch] loss: 2.64320, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:23:09.551071 Training: [24 epoch,  30 batch] loss: 2.62451, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:24:05.285275 Training: [24 epoch,  40 batch] loss: 2.60770, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:25:01.190474 Training: [24 epoch,  50 batch] loss: 2.59652, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:25:59.160273 Training: [24 epoch,  60 batch] loss: 2.56351, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:26:54.235747 Training: [24 epoch,  70 batch] loss: 2.53986, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:27:49.638412 Training: [24 epoch,  80 batch] loss: 2.58465, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:28:45.395898 Training: [24 epoch,  90 batch] loss: 2.51962, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38673,MAE：0.15031
2021-01-05 23:31:22.437217 Training: [25 epoch,  10 batch] loss: 2.47737, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:32:17.089710 Training: [25 epoch,  20 batch] loss: 2.48452, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:33:12.314574 Training: [25 epoch,  30 batch] loss: 2.47194, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:34:07.346852 Training: [25 epoch,  40 batch] loss: 2.45561, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:35:02.611105 Training: [25 epoch,  50 batch] loss: 2.43420, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:35:58.195676 Training: [25 epoch,  60 batch] loss: 2.43511, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:36:52.399969 Training: [25 epoch,  70 batch] loss: 2.38967, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:37:47.589615 Training: [25 epoch,  80 batch] loss: 2.42326, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:38:43.017984 Training: [25 epoch,  90 batch] loss: 2.37698, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38557,MAE：0.14817
2021-01-05 23:41:19.790117 Training: [26 epoch,  10 batch] loss: 2.34508, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:42:15.205747 Training: [26 epoch,  20 batch] loss: 2.33190, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:43:10.806864 Training: [26 epoch,  30 batch] loss: 2.32446, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:44:06.079533 Training: [26 epoch,  40 batch] loss: 2.28906, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:45:01.994479 Training: [26 epoch,  50 batch] loss: 2.33880, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:45:57.305875 Training: [26 epoch,  60 batch] loss: 2.30734, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:46:52.212262 Training: [26 epoch,  70 batch] loss: 2.25120, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:47:46.507765 Training: [26 epoch,  80 batch] loss: 2.26996, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:48:41.819633 Training: [26 epoch,  90 batch] loss: 2.23772, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38624,MAE：0.15720
2021-01-05 23:51:17.893868 Training: [27 epoch,  10 batch] loss: 2.23059, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:52:13.115091 Training: [27 epoch,  20 batch] loss: 2.20839, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:53:08.130237 Training: [27 epoch,  30 batch] loss: 2.21346, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:54:03.357855 Training: [27 epoch,  40 batch] loss: 2.19725, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:54:58.475067 Training: [27 epoch,  50 batch] loss: 2.15755, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:55:53.606729 Training: [27 epoch,  60 batch] loss: 2.13137, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:56:48.715618 Training: [27 epoch,  70 batch] loss: 2.12602, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:57:42.888215 Training: [27 epoch,  80 batch] loss: 2.10707, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-05 23:58:37.719508 Training: [27 epoch,  90 batch] loss: 2.10507, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38996,MAE：0.15965
2021-01-06 00:01:13.630794 Training: [28 epoch,  10 batch] loss: 2.06936, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:02:08.529828 Training: [28 epoch,  20 batch] loss: 2.07066, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:03:03.894579 Training: [28 epoch,  30 batch] loss: 2.05390, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:03:59.420995 Training: [28 epoch,  40 batch] loss: 2.03657, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:04:55.219139 Training: [28 epoch,  50 batch] loss: 2.05557, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:05:50.671141 Training: [28 epoch,  60 batch] loss: 2.03160, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:06:46.110466 Training: [28 epoch,  70 batch] loss: 2.00472, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:07:41.051647 Training: [28 epoch,  80 batch] loss: 1.99228, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:08:35.331687 Training: [28 epoch,  90 batch] loss: 1.98115, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39281,MAE：0.15913
2021-01-06 00:11:17.162465 Training: [29 epoch,  10 batch] loss: 1.96637, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:12:18.641430 Training: [29 epoch,  20 batch] loss: 1.97581, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:13:16.573484 Training: [29 epoch,  30 batch] loss: 1.92822, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:14:14.568004 Training: [29 epoch,  40 batch] loss: 1.92061, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:15:12.908775 Training: [29 epoch,  50 batch] loss: 1.89628, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:16:10.732950 Training: [29 epoch,  60 batch] loss: 1.88752, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:17:08.776084 Training: [29 epoch,  70 batch] loss: 1.92596, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:18:06.183749 Training: [29 epoch,  80 batch] loss: 1.89895, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:19:02.620234 Training: [29 epoch,  90 batch] loss: 1.87286, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38653,MAE：0.14983
2021-01-06 00:21:42.928783 Training: [30 epoch,  10 batch] loss: 1.84399, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:22:37.257918 Training: [30 epoch,  20 batch] loss: 1.82314, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:23:32.394690 Training: [30 epoch,  30 batch] loss: 1.84112, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:24:27.768156 Training: [30 epoch,  40 batch] loss: 1.85379, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:25:23.544521 Training: [30 epoch,  50 batch] loss: 1.80242, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:26:18.819937 Training: [30 epoch,  60 batch] loss: 1.81617, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:27:14.141632 Training: [30 epoch,  70 batch] loss: 1.78717, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:28:08.674822 Training: [30 epoch,  80 batch] loss: 1.76308, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:29:03.010292 Training: [30 epoch,  90 batch] loss: 1.76115, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40013,MAE：0.17204
2021-01-06 00:31:44.130709 Training: [31 epoch,  10 batch] loss: 1.74017, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:32:38.822744 Training: [31 epoch,  20 batch] loss: 1.72118, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:33:33.451274 Training: [31 epoch,  30 batch] loss: 1.73743, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:34:28.478407 Training: [31 epoch,  40 batch] loss: 1.72330, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:35:23.602597 Training: [31 epoch,  50 batch] loss: 1.72391, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:36:18.719344 Training: [31 epoch,  60 batch] loss: 1.72309, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:37:13.910166 Training: [31 epoch,  70 batch] loss: 1.68483, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:38:08.105908 Training: [31 epoch,  80 batch] loss: 1.67681, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:39:02.487159 Training: [31 epoch,  90 batch] loss: 1.67617, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40443,MAE：0.18848
2021-01-06 00:41:37.394696 Training: [32 epoch,  10 batch] loss: 1.63135, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:42:32.380277 Training: [32 epoch,  20 batch] loss: 1.66152, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:43:27.019472 Training: [32 epoch,  30 batch] loss: 1.62065, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:44:21.848781 Training: [32 epoch,  40 batch] loss: 1.61835, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:45:17.151708 Training: [32 epoch,  50 batch] loss: 1.61599, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:46:12.691365 Training: [32 epoch,  60 batch] loss: 1.60491, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:47:07.749574 Training: [32 epoch,  70 batch] loss: 1.60538, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:48:02.321816 Training: [32 epoch,  80 batch] loss: 1.59589, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:48:55.716463 Training: [32 epoch,  90 batch] loss: 1.58490, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40314,MAE：0.17954
2021-01-06 00:51:31.858656 Training: [33 epoch,  10 batch] loss: 1.56607, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:52:26.966624 Training: [33 epoch,  20 batch] loss: 1.57085, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:53:22.118120 Training: [33 epoch,  30 batch] loss: 1.54187, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:54:17.255724 Training: [33 epoch,  40 batch] loss: 1.55639, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:55:12.443808 Training: [33 epoch,  50 batch] loss: 1.52307, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:56:07.767152 Training: [33 epoch,  60 batch] loss: 1.51923, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:57:03.023162 Training: [33 epoch,  70 batch] loss: 1.50490, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:57:57.795381 Training: [33 epoch,  80 batch] loss: 1.48757, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 00:58:51.981001 Training: [33 epoch,  90 batch] loss: 1.48403, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40204,MAE：0.17509
2021-01-06 01:01:27.676587 Training: [34 epoch,  10 batch] loss: 1.46386, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:02:23.335168 Training: [34 epoch,  20 batch] loss: 1.46159, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:03:19.122069 Training: [34 epoch,  30 batch] loss: 1.45906, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:04:14.563062 Training: [34 epoch,  40 batch] loss: 1.45715, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:05:10.049678 Training: [34 epoch,  50 batch] loss: 1.45767, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:06:05.542133 Training: [34 epoch,  60 batch] loss: 1.42280, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:07:01.197750 Training: [34 epoch,  70 batch] loss: 1.43226, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:07:56.103503 Training: [34 epoch,  80 batch] loss: 1.43048, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:08:50.105748 Training: [34 epoch,  90 batch] loss: 1.40516, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40842,MAE：0.19237
2021-01-06 01:11:25.781581 Training: [35 epoch,  10 batch] loss: 1.40690, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:12:20.765216 Training: [35 epoch,  20 batch] loss: 1.37053, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:13:15.922176 Training: [35 epoch,  30 batch] loss: 1.38119, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:14:11.170546 Training: [35 epoch,  40 batch] loss: 1.37855, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:15:07.036064 Training: [35 epoch,  50 batch] loss: 1.34413, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:16:02.909248 Training: [35 epoch,  60 batch] loss: 1.35909, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:16:58.374937 Training: [35 epoch,  70 batch] loss: 1.34000, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:17:53.348070 Training: [35 epoch,  80 batch] loss: 1.36447, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:18:47.571610 Training: [35 epoch,  90 batch] loss: 1.38703, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39432,MAE：0.16072
2021-01-06 01:21:24.170090 Training: [36 epoch,  10 batch] loss: 1.31484, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:22:19.204514 Training: [36 epoch,  20 batch] loss: 1.31981, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:23:14.454605 Training: [36 epoch,  30 batch] loss: 1.34852, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:24:10.253669 Training: [36 epoch,  40 batch] loss: 1.30776, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:25:06.163749 Training: [36 epoch,  50 batch] loss: 1.28371, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:26:01.452545 Training: [36 epoch,  60 batch] loss: 1.26874, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:26:56.940111 Training: [36 epoch,  70 batch] loss: 1.30351, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:27:52.028094 Training: [36 epoch,  80 batch] loss: 1.29122, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:28:45.804184 Training: [36 epoch,  90 batch] loss: 1.25635, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39522,MAE：0.16562
2021-01-06 01:31:29.247447 Training: [37 epoch,  10 batch] loss: 1.26298, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:32:26.013177 Training: [37 epoch,  20 batch] loss: 1.22956, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:33:20.963517 Training: [37 epoch,  30 batch] loss: 1.24392, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:34:16.107589 Training: [37 epoch,  40 batch] loss: 1.22895, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:35:11.310466 Training: [37 epoch,  50 batch] loss: 1.21896, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:36:06.503543 Training: [37 epoch,  60 batch] loss: 1.21869, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:37:01.743877 Training: [37 epoch,  70 batch] loss: 1.21180, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:37:56.338634 Training: [37 epoch,  80 batch] loss: 1.19235, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:38:50.314425 Training: [37 epoch,  90 batch] loss: 1.22852, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39379,MAE：0.14636
2021-01-06 01:41:28.065842 Training: [38 epoch,  10 batch] loss: 1.21081, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:42:23.060791 Training: [38 epoch,  20 batch] loss: 1.16931, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:43:18.255554 Training: [38 epoch,  30 batch] loss: 1.16826, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:44:13.549232 Training: [38 epoch,  40 batch] loss: 1.16182, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:45:09.163043 Training: [38 epoch,  50 batch] loss: 1.15744, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:46:04.376729 Training: [38 epoch,  60 batch] loss: 1.14537, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:46:59.781799 Training: [38 epoch,  70 batch] loss: 1.15136, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:47:54.202322 Training: [38 epoch,  80 batch] loss: 1.15892, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:48:47.714903 Training: [38 epoch,  90 batch] loss: 1.14120, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.40076,MAE：0.16052
2021-01-06 01:51:24.734246 Training: [39 epoch,  10 batch] loss: 1.14364, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:52:19.850682 Training: [39 epoch,  20 batch] loss: 1.10971, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:53:15.438864 Training: [39 epoch,  30 batch] loss: 1.11212, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:54:10.518951 Training: [39 epoch,  40 batch] loss: 1.12240, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:55:06.063371 Training: [39 epoch,  50 batch] loss: 1.11279, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:56:01.207394 Training: [39 epoch,  60 batch] loss: 1.08571, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:56:56.246928 Training: [39 epoch,  70 batch] loss: 1.08108, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:57:52.119998 Training: [39 epoch,  80 batch] loss: 1.12952, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 01:58:45.659270 Training: [39 epoch,  90 batch] loss: 1.07421, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39503,MAE：0.15108
2021-01-06 02:01:23.008255 Training: [40 epoch,  10 batch] loss: 1.08128, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:02:18.958953 Training: [40 epoch,  20 batch] loss: 1.06039, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:03:14.066309 Training: [40 epoch,  30 batch] loss: 1.05868, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:04:09.598842 Training: [40 epoch,  40 batch] loss: 1.04815, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:05:05.087859 Training: [40 epoch,  50 batch] loss: 1.04698, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:06:00.245657 Training: [40 epoch,  60 batch] loss: 1.04165, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:06:56.335447 Training: [40 epoch,  70 batch] loss: 1.04068, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:07:52.254989 Training: [40 epoch,  80 batch] loss: 1.03420, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:08:45.974449 Training: [40 epoch,  90 batch] loss: 1.05217, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39387,MAE：0.14647
2021-01-06 02:11:31.073779 Training: [41 epoch,  10 batch] loss: 1.02810, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:12:29.381184 Training: [41 epoch,  20 batch] loss: 1.01511, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:13:27.221673 Training: [41 epoch,  30 batch] loss: 1.01257, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:14:25.501364 Training: [41 epoch,  40 batch] loss: 1.00409, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:15:23.920498 Training: [41 epoch,  50 batch] loss: 0.98086, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:16:21.787093 Training: [41 epoch,  60 batch] loss: 1.02067, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:17:20.221503 Training: [41 epoch,  70 batch] loss: 0.99633, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:18:16.865751 Training: [41 epoch,  80 batch] loss: 0.98543, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:19:12.566581 Training: [41 epoch,  90 batch] loss: 1.00186, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39197,MAE：0.13834
2021-01-06 02:21:53.882404 Training: [42 epoch,  10 batch] loss: 0.96465, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:22:48.927682 Training: [42 epoch,  20 batch] loss: 0.96585, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:23:44.043277 Training: [42 epoch,  30 batch] loss: 0.94851, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:24:39.215119 Training: [42 epoch,  40 batch] loss: 0.95699, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:25:34.450144 Training: [42 epoch,  50 batch] loss: 0.96838, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:26:29.674212 Training: [42 epoch,  60 batch] loss: 0.94920, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:27:24.982244 Training: [42 epoch,  70 batch] loss: 0.92518, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:28:18.773738 Training: [42 epoch,  80 batch] loss: 0.93097, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:29:14.525778 Training: [42 epoch,  90 batch] loss: 0.96226, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39414,MAE：0.13537
2021-01-06 02:31:58.600028 Training: [43 epoch,  10 batch] loss: 0.90557, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:32:56.585324 Training: [43 epoch,  20 batch] loss: 0.91749, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:33:54.288562 Training: [43 epoch,  30 batch] loss: 0.91173, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:34:52.620151 Training: [43 epoch,  40 batch] loss: 0.89892, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:35:50.654047 Training: [43 epoch,  50 batch] loss: 0.92798, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:36:48.834113 Training: [43 epoch,  60 batch] loss: 0.91238, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:37:46.359339 Training: [43 epoch,  70 batch] loss: 0.90020, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:38:39.335600 Training: [43 epoch,  80 batch] loss: 0.89518, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:39:37.049047 Training: [43 epoch,  90 batch] loss: 0.89344, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38862,MAE：0.14593
2021-01-06 02:42:17.606844 Training: [44 epoch,  10 batch] loss: 0.88555, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:43:12.896653 Training: [44 epoch,  20 batch] loss: 0.87652, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:44:08.225319 Training: [44 epoch,  30 batch] loss: 0.87776, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:45:03.608019 Training: [44 epoch,  40 batch] loss: 0.87564, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:45:59.006814 Training: [44 epoch,  50 batch] loss: 0.85342, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:46:56.876485 Training: [44 epoch,  60 batch] loss: 0.85409, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:47:54.615322 Training: [44 epoch,  70 batch] loss: 0.84384, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:48:51.990126 Training: [44 epoch,  80 batch] loss: 0.86161, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:49:51.623999 Training: [44 epoch,  90 batch] loss: 0.84388, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39073,MAE：0.15539
2021-01-06 02:52:41.714364 Training: [45 epoch,  10 batch] loss: 0.84963, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:53:40.698361 Training: [45 epoch,  20 batch] loss: 0.84673, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:54:39.898843 Training: [45 epoch,  30 batch] loss: 0.83171, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:55:39.238814 Training: [45 epoch,  40 batch] loss: 0.81470, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:56:38.666961 Training: [45 epoch,  50 batch] loss: 0.82618, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:57:37.179214 Training: [45 epoch,  60 batch] loss: 0.83338, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:58:34.272058 Training: [45 epoch,  70 batch] loss: 0.80120, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 02:59:34.023741 Training: [45 epoch,  80 batch] loss: 0.80759, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:00:33.658151 Training: [45 epoch,  90 batch] loss: 0.80354, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38894,MAE：0.12758
2021-01-06 03:03:19.123508 Training: [46 epoch,  10 batch] loss: 0.78619, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:04:18.115027 Training: [46 epoch,  20 batch] loss: 0.78397, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:05:17.396752 Training: [46 epoch,  30 batch] loss: 0.77591, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:06:16.679005 Training: [46 epoch,  40 batch] loss: 0.79538, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:07:15.602352 Training: [46 epoch,  50 batch] loss: 0.77751, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:08:12.207013 Training: [46 epoch,  60 batch] loss: 0.77966, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:09:11.643706 Training: [46 epoch,  70 batch] loss: 0.78401, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:10:11.269845 Training: [46 epoch,  80 batch] loss: 0.77548, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:11:11.059696 Training: [46 epoch,  90 batch] loss: 0.80786, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39208,MAE：0.15295
2021-01-06 03:13:52.334087 Training: [47 epoch,  10 batch] loss: 0.76832, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:14:47.493484 Training: [47 epoch,  20 batch] loss: 0.74905, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:15:43.234522 Training: [47 epoch,  30 batch] loss: 0.79158, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:16:38.481771 Training: [47 epoch,  40 batch] loss: 0.74734, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:17:32.132112 Training: [47 epoch,  50 batch] loss: 0.75419, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:18:25.603674 Training: [47 epoch,  60 batch] loss: 0.73701, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:19:21.266146 Training: [47 epoch,  70 batch] loss: 0.72644, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:20:16.894939 Training: [47 epoch,  80 batch] loss: 0.75167, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:21:12.871589 Training: [47 epoch,  90 batch] loss: 0.73836, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38961,MAE：0.15511
2021-01-06 03:23:52.470717 Training: [48 epoch,  10 batch] loss: 0.72184, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:24:47.906168 Training: [48 epoch,  20 batch] loss: 0.73606, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:25:43.294752 Training: [48 epoch,  30 batch] loss: 0.73702, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:26:38.787181 Training: [48 epoch,  40 batch] loss: 0.71443, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:27:32.540586 Training: [48 epoch,  50 batch] loss: 0.70516, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:28:26.138569 Training: [48 epoch,  60 batch] loss: 0.70859, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:29:21.838660 Training: [48 epoch,  70 batch] loss: 0.71263, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:30:17.900337 Training: [48 epoch,  80 batch] loss: 0.70817, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:31:13.730221 Training: [48 epoch,  90 batch] loss: 0.71425, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38943,MAE：0.14880
2021-01-06 03:33:52.901656 Training: [49 epoch,  10 batch] loss: 0.68366, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:34:47.535177 Training: [49 epoch,  20 batch] loss: 0.69150, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:35:42.510937 Training: [49 epoch,  30 batch] loss: 0.66982, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:36:37.461879 Training: [49 epoch,  40 batch] loss: 0.70953, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:37:31.178511 Training: [49 epoch,  50 batch] loss: 0.69008, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:38:24.627110 Training: [49 epoch,  60 batch] loss: 0.68985, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:39:19.107071 Training: [49 epoch,  70 batch] loss: 0.69348, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:40:13.997871 Training: [49 epoch,  80 batch] loss: 0.68385, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:41:08.919136 Training: [49 epoch,  90 batch] loss: 0.67492, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39288,MAE：0.15076
2021-01-06 03:43:51.128644 Training: [50 epoch,  10 batch] loss: 0.65533, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:44:46.386538 Training: [50 epoch,  20 batch] loss: 0.66850, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:45:41.423084 Training: [50 epoch,  30 batch] loss: 0.65323, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:46:36.515498 Training: [50 epoch,  40 batch] loss: 0.63532, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:47:30.746116 Training: [50 epoch,  50 batch] loss: 0.65728, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:48:24.624011 Training: [50 epoch,  60 batch] loss: 0.65250, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:49:19.927374 Training: [50 epoch,  70 batch] loss: 0.66360, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:50:15.979437 Training: [50 epoch,  80 batch] loss: 0.65897, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:51:11.449671 Training: [50 epoch,  90 batch] loss: 0.68032, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38751,MAE：0.14255
2021-01-06 03:53:57.700367 Training: [51 epoch,  10 batch] loss: 0.64291, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:54:53.446386 Training: [51 epoch,  20 batch] loss: 0.63185, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:55:48.819542 Training: [51 epoch,  30 batch] loss: 0.66018, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:56:43.407552 Training: [51 epoch,  40 batch] loss: 0.61364, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:57:37.837438 Training: [51 epoch,  50 batch] loss: 0.61965, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:58:32.251590 Training: [51 epoch,  60 batch] loss: 0.61792, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 03:59:26.656767 Training: [51 epoch,  70 batch] loss: 0.61968, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:00:21.889036 Training: [51 epoch,  80 batch] loss: 0.62027, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:01:17.440991 Training: [51 epoch,  90 batch] loss: 0.64282, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38550,MAE：0.13940
2021-01-06 04:03:59.312733 Training: [52 epoch,  10 batch] loss: 0.60971, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:04:56.953337 Training: [52 epoch,  20 batch] loss: 0.62085, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:05:51.982126 Training: [52 epoch,  30 batch] loss: 0.59528, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:06:46.275299 Training: [52 epoch,  40 batch] loss: 0.60424, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:07:40.694652 Training: [52 epoch,  50 batch] loss: 0.60781, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:08:34.662603 Training: [52 epoch,  60 batch] loss: 0.59169, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:09:28.946779 Training: [52 epoch,  70 batch] loss: 0.59177, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:10:24.331263 Training: [52 epoch,  80 batch] loss: 0.59063, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:11:19.478247 Training: [52 epoch,  90 batch] loss: 0.57617, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38750,MAE：0.16216
2021-01-06 04:14:01.963408 Training: [53 epoch,  10 batch] loss: 0.57465, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:14:56.771685 Training: [53 epoch,  20 batch] loss: 0.59513, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:15:51.900645 Training: [53 epoch,  30 batch] loss: 0.61179, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:16:45.982548 Training: [53 epoch,  40 batch] loss: 0.56755, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:17:41.377354 Training: [53 epoch,  50 batch] loss: 0.57651, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:18:38.266570 Training: [53 epoch,  60 batch] loss: 0.57037, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:19:36.096079 Training: [53 epoch,  70 batch] loss: 0.56246, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:20:34.471472 Training: [53 epoch,  80 batch] loss: 0.56677, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:21:32.954324 Training: [53 epoch,  90 batch] loss: 0.56075, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.39262,MAE：0.19101
2021-01-06 04:24:13.795030 Training: [54 epoch,  10 batch] loss: 0.55856, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:25:09.477762 Training: [54 epoch,  20 batch] loss: 0.54323, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:26:04.834412 Training: [54 epoch,  30 batch] loss: 0.55116, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:26:58.719754 Training: [54 epoch,  40 batch] loss: 0.55133, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:27:53.666205 Training: [54 epoch,  50 batch] loss: 0.56417, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:28:47.818072 Training: [54 epoch,  60 batch] loss: 0.55861, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:29:42.645950 Training: [54 epoch,  70 batch] loss: 0.54148, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:30:38.058001 Training: [54 epoch,  80 batch] loss: 0.52292, the best RMSE/MAE: 0.38484 / 0.15789
2021-01-06 04:31:34.067865 Training: [54 epoch,  90 batch] loss: 0.54504, the best RMSE/MAE: 0.38484 / 0.15789
<Test> RMSE：0.38964,MAE：0.14693
The best RMSE/MAE：0.38484/0.15789
