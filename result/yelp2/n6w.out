-------------------- Hyperparams --------------------
time: 2021-01-06 20:55:49.037994
Dataset: yelp
N: 60000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-06 21:07:28.157480 Training: [1 epoch,  10 batch] loss: 7.22528, the best RMSE/MAE: inf / inf
2021-01-06 21:09:34.839495 Training: [1 epoch,  20 batch] loss: 6.87007, the best RMSE/MAE: inf / inf
2021-01-06 21:11:45.026120 Training: [1 epoch,  30 batch] loss: 6.44491, the best RMSE/MAE: inf / inf
2021-01-06 21:13:54.502660 Training: [1 epoch,  40 batch] loss: 6.26250, the best RMSE/MAE: inf / inf
2021-01-06 21:16:01.650610 Training: [1 epoch,  50 batch] loss: 6.05965, the best RMSE/MAE: inf / inf
2021-01-06 21:18:10.458988 Training: [1 epoch,  60 batch] loss: 5.86325, the best RMSE/MAE: inf / inf
2021-01-06 21:20:16.162947 Training: [1 epoch,  70 batch] loss: 5.75930, the best RMSE/MAE: inf / inf
2021-01-06 21:22:21.143916 Training: [1 epoch,  80 batch] loss: 5.67191, the best RMSE/MAE: inf / inf
2021-01-06 21:24:28.376525 Training: [1 epoch,  90 batch] loss: 5.63471, the best RMSE/MAE: inf / inf
<Test> RMSE：420752192.00000,MAE：215358304.00000
2021-01-06 21:30:50.269300 Training: [2 epoch,  10 batch] loss: 5.58600, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:32:55.693691 Training: [2 epoch,  20 batch] loss: 5.57634, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:35:01.314622 Training: [2 epoch,  30 batch] loss: 5.50550, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:37:08.789126 Training: [2 epoch,  40 batch] loss: 5.51888, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:39:17.616719 Training: [2 epoch,  50 batch] loss: 5.45867, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:41:25.991262 Training: [2 epoch,  60 batch] loss: 5.44585, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:43:32.854780 Training: [2 epoch,  70 batch] loss: 5.44098, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:45:32.505409 Training: [2 epoch,  80 batch] loss: 5.39590, the best RMSE/MAE: 420752192.00000 / 215358304.00000
2021-01-06 21:47:40.482968 Training: [2 epoch,  90 batch] loss: 5.40210, the best RMSE/MAE: 420752192.00000 / 215358304.00000
<Test> RMSE：231842.48438,MAE：143465.07812
2021-01-06 21:54:05.578840 Training: [3 epoch,  10 batch] loss: 5.37789, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 21:56:08.066210 Training: [3 epoch,  20 batch] loss: 5.34441, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 21:58:16.541795 Training: [3 epoch,  30 batch] loss: 5.34512, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:00:24.914700 Training: [3 epoch,  40 batch] loss: 5.32791, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:02:33.461646 Training: [3 epoch,  50 batch] loss: 5.33908, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:04:38.980279 Training: [3 epoch,  60 batch] loss: 5.29661, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:06:45.174042 Training: [3 epoch,  70 batch] loss: 5.26990, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:08:49.577474 Training: [3 epoch,  80 batch] loss: 5.34342, the best RMSE/MAE: 231842.48438 / 143465.07812
2021-01-06 22:10:55.680284 Training: [3 epoch,  90 batch] loss: 5.24270, the best RMSE/MAE: 231842.48438 / 143465.07812
<Test> RMSE：3755.81787,MAE：2667.24414
2021-01-06 22:17:20.151907 Training: [4 epoch,  10 batch] loss: 5.26248, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:19:25.123453 Training: [4 epoch,  20 batch] loss: 5.23841, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:21:33.427474 Training: [4 epoch,  30 batch] loss: 5.24426, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:23:40.263878 Training: [4 epoch,  40 batch] loss: 5.15997, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:25:46.464496 Training: [4 epoch,  50 batch] loss: 5.19997, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:27:54.716393 Training: [4 epoch,  60 batch] loss: 5.17593, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:30:01.615883 Training: [4 epoch,  70 batch] loss: 5.18275, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:32:07.259664 Training: [4 epoch,  80 batch] loss: 5.14988, the best RMSE/MAE: 3755.81787 / 2667.24414
2021-01-06 22:34:10.004209 Training: [4 epoch,  90 batch] loss: 5.18322, the best RMSE/MAE: 3755.81787 / 2667.24414
<Test> RMSE：300.31424,MAE：217.23001
2021-01-06 22:40:34.460972 Training: [5 epoch,  10 batch] loss: 5.12584, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:42:39.736058 Training: [5 epoch,  20 batch] loss: 5.08128, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:44:44.833157 Training: [5 epoch,  30 batch] loss: 5.13160, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:46:53.672172 Training: [5 epoch,  40 batch] loss: 5.08539, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:49:02.827086 Training: [5 epoch,  50 batch] loss: 5.05926, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:51:11.232717 Training: [5 epoch,  60 batch] loss: 5.05120, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:53:16.791783 Training: [5 epoch,  70 batch] loss: 5.06334, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:55:21.027382 Training: [5 epoch,  80 batch] loss: 5.03829, the best RMSE/MAE: 300.31424 / 217.23001
2021-01-06 22:57:27.524748 Training: [5 epoch,  90 batch] loss: 5.08983, the best RMSE/MAE: 300.31424 / 217.23001
<Test> RMSE：61.06226,MAE：41.71483
2021-01-06 23:03:49.066574 Training: [6 epoch,  10 batch] loss: 4.99702, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:05:54.289498 Training: [6 epoch,  20 batch] loss: 4.96983, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:08:02.977203 Training: [6 epoch,  30 batch] loss: 4.99192, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:10:11.858850 Training: [6 epoch,  40 batch] loss: 5.00528, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:12:20.276004 Training: [6 epoch,  50 batch] loss: 4.93123, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:14:25.506027 Training: [6 epoch,  60 batch] loss: 4.94821, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:16:32.655487 Training: [6 epoch,  70 batch] loss: 4.96384, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:18:38.147913 Training: [6 epoch,  80 batch] loss: 4.92465, the best RMSE/MAE: 61.06226 / 41.71483
2021-01-06 23:20:43.617232 Training: [6 epoch,  90 batch] loss: 4.88696, the best RMSE/MAE: 61.06226 / 41.71483
<Test> RMSE：19.84392,MAE：13.14266
2021-01-06 23:27:05.507442 Training: [7 epoch,  10 batch] loss: 4.85752, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:29:12.278097 Training: [7 epoch,  20 batch] loss: 4.87966, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:31:21.039011 Training: [7 epoch,  30 batch] loss: 4.84484, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:33:26.392261 Training: [7 epoch,  40 batch] loss: 4.87601, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:35:35.130064 Training: [7 epoch,  50 batch] loss: 4.81776, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:37:43.670296 Training: [7 epoch,  60 batch] loss: 4.86933, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:39:51.012116 Training: [7 epoch,  70 batch] loss: 4.83213, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:41:56.448571 Training: [7 epoch,  80 batch] loss: 4.78796, the best RMSE/MAE: 19.84392 / 13.14266
2021-01-06 23:43:58.262186 Training: [7 epoch,  90 batch] loss: 4.78983, the best RMSE/MAE: 19.84392 / 13.14266
<Test> RMSE：6.13770,MAE：4.10296
2021-01-06 23:50:00.439005 Training: [8 epoch,  10 batch] loss: 4.72764, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-06 23:51:50.455184 Training: [8 epoch,  20 batch] loss: 4.74294, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-06 23:53:44.471195 Training: [8 epoch,  30 batch] loss: 4.74521, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-06 23:55:39.700724 Training: [8 epoch,  40 batch] loss: 4.70936, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-06 23:57:34.954658 Training: [8 epoch,  50 batch] loss: 4.75044, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-06 23:59:29.654623 Training: [8 epoch,  60 batch] loss: 4.69978, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-07 00:01:20.403274 Training: [8 epoch,  70 batch] loss: 4.67143, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-07 00:03:12.550675 Training: [8 epoch,  80 batch] loss: 4.63999, the best RMSE/MAE: 6.13770 / 4.10296
2021-01-07 00:05:04.240360 Training: [8 epoch,  90 batch] loss: 4.63949, the best RMSE/MAE: 6.13770 / 4.10296
<Test> RMSE：3.27915,MAE：2.17854
2021-01-07 00:10:48.412219 Training: [9 epoch,  10 batch] loss: 4.64295, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:12:45.291061 Training: [9 epoch,  20 batch] loss: 4.59483, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:14:40.819591 Training: [9 epoch,  30 batch] loss: 4.59203, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:16:35.642298 Training: [9 epoch,  40 batch] loss: 4.61068, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:18:28.940079 Training: [9 epoch,  50 batch] loss: 4.55537, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:20:25.088027 Training: [9 epoch,  60 batch] loss: 4.53861, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:22:18.994174 Training: [9 epoch,  70 batch] loss: 4.51524, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:24:10.625154 Training: [9 epoch,  80 batch] loss: 4.49443, the best RMSE/MAE: 3.27915 / 2.17854
2021-01-07 00:26:01.983337 Training: [9 epoch,  90 batch] loss: 4.50798, the best RMSE/MAE: 3.27915 / 2.17854
<Test> RMSE：1.59893,MAE：1.08611
2021-01-07 00:31:41.402695 Training: [10 epoch,  10 batch] loss: 4.47484, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:33:39.458204 Training: [10 epoch,  20 batch] loss: 4.48075, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:35:36.817440 Training: [10 epoch,  30 batch] loss: 4.44399, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:37:37.020034 Training: [10 epoch,  40 batch] loss: 4.44736, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:39:37.961886 Training: [10 epoch,  50 batch] loss: 4.44041, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:41:38.550449 Training: [10 epoch,  60 batch] loss: 4.41523, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:43:37.226969 Training: [10 epoch,  70 batch] loss: 4.35317, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:45:32.136102 Training: [10 epoch,  80 batch] loss: 4.37556, the best RMSE/MAE: 1.59893 / 1.08611
2021-01-07 00:47:30.626387 Training: [10 epoch,  90 batch] loss: 4.35482, the best RMSE/MAE: 1.59893 / 1.08611
<Test> RMSE：1.32284,MAE：0.86639
2021-01-07 00:53:19.579235 Training: [11 epoch,  10 batch] loss: 4.33340, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 00:55:13.190799 Training: [11 epoch,  20 batch] loss: 4.34786, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 00:57:06.918563 Training: [11 epoch,  30 batch] loss: 4.31459, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 00:59:03.639238 Training: [11 epoch,  40 batch] loss: 4.28708, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 01:00:58.204614 Training: [11 epoch,  50 batch] loss: 4.25781, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 01:02:51.547599 Training: [11 epoch,  60 batch] loss: 4.23301, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 01:04:44.420322 Training: [11 epoch,  70 batch] loss: 4.26827, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 01:06:36.834761 Training: [11 epoch,  80 batch] loss: 4.19677, the best RMSE/MAE: 1.32284 / 0.86639
2021-01-07 01:08:28.264526 Training: [11 epoch,  90 batch] loss: 4.18584, the best RMSE/MAE: 1.32284 / 0.86639
<Test> RMSE：0.71295,MAE：0.44228
2021-01-07 01:14:05.153947 Training: [12 epoch,  10 batch] loss: 4.23345, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:15:57.195111 Training: [12 epoch,  20 batch] loss: 4.16529, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:17:51.483984 Training: [12 epoch,  30 batch] loss: 4.11442, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:19:43.021437 Training: [12 epoch,  40 batch] loss: 4.13575, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:21:35.673910 Training: [12 epoch,  50 batch] loss: 4.08842, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:23:29.311839 Training: [12 epoch,  60 batch] loss: 4.12150, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:25:20.822792 Training: [12 epoch,  70 batch] loss: 4.08888, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:27:14.800083 Training: [12 epoch,  80 batch] loss: 4.06356, the best RMSE/MAE: 0.71295 / 0.44228
2021-01-07 01:29:10.647158 Training: [12 epoch,  90 batch] loss: 4.06405, the best RMSE/MAE: 0.71295 / 0.44228
<Test> RMSE：0.55306,MAE：0.33374
2021-01-07 01:34:57.936830 Training: [13 epoch,  10 batch] loss: 3.99346, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:36:51.499040 Training: [13 epoch,  20 batch] loss: 4.04046, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:38:49.080179 Training: [13 epoch,  30 batch] loss: 3.99735, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:40:50.080806 Training: [13 epoch,  40 batch] loss: 3.98022, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:42:50.730817 Training: [13 epoch,  50 batch] loss: 3.98315, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:44:49.180134 Training: [13 epoch,  60 batch] loss: 3.93529, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:46:45.485070 Training: [13 epoch,  70 batch] loss: 3.91815, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:48:42.745900 Training: [13 epoch,  80 batch] loss: 3.90957, the best RMSE/MAE: 0.55306 / 0.33374
2021-01-07 01:50:39.506452 Training: [13 epoch,  90 batch] loss: 3.87744, the best RMSE/MAE: 0.55306 / 0.33374
<Test> RMSE：0.49184,MAE：0.28898
2021-01-07 01:56:16.131894 Training: [14 epoch,  10 batch] loss: 3.90864, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 01:58:10.034632 Training: [14 epoch,  20 batch] loss: 3.82463, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:00:03.747885 Training: [14 epoch,  30 batch] loss: 3.81969, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:01:57.228860 Training: [14 epoch,  40 batch] loss: 3.79797, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:03:48.974605 Training: [14 epoch,  50 batch] loss: 3.77448, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:05:38.864096 Training: [14 epoch,  60 batch] loss: 3.79485, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:07:30.240196 Training: [14 epoch,  70 batch] loss: 3.74003, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:09:21.018791 Training: [14 epoch,  80 batch] loss: 3.75698, the best RMSE/MAE: 0.49184 / 0.28898
2021-01-07 02:11:12.528081 Training: [14 epoch,  90 batch] loss: 3.72812, the best RMSE/MAE: 0.49184 / 0.28898
<Test> RMSE：0.47721,MAE：0.27759
2021-01-07 02:16:47.387391 Training: [15 epoch,  10 batch] loss: 3.65499, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:18:40.669152 Training: [15 epoch,  20 batch] loss: 3.70629, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:20:33.183257 Training: [15 epoch,  30 batch] loss: 3.65769, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:22:26.538515 Training: [15 epoch,  40 batch] loss: 3.67417, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:24:18.672177 Training: [15 epoch,  50 batch] loss: 3.63033, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:26:10.335642 Training: [15 epoch,  60 batch] loss: 3.63926, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:28:02.318823 Training: [15 epoch,  70 batch] loss: 3.60075, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:29:52.527252 Training: [15 epoch,  80 batch] loss: 3.56444, the best RMSE/MAE: 0.47721 / 0.27759
2021-01-07 02:31:46.466571 Training: [15 epoch,  90 batch] loss: 3.56529, the best RMSE/MAE: 0.47721 / 0.27759
<Test> RMSE：0.46004,MAE：0.25214
2021-01-07 02:37:30.586573 Training: [16 epoch,  10 batch] loss: 3.53664, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:39:22.287308 Training: [16 epoch,  20 batch] loss: 3.52234, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:41:14.796022 Training: [16 epoch,  30 batch] loss: 3.47905, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:43:07.794136 Training: [16 epoch,  40 batch] loss: 3.53396, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:45:01.064008 Training: [16 epoch,  50 batch] loss: 3.46578, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:46:51.756389 Training: [16 epoch,  60 batch] loss: 3.44195, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:48:43.386560 Training: [16 epoch,  70 batch] loss: 3.44870, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:50:33.463081 Training: [16 epoch,  80 batch] loss: 3.40328, the best RMSE/MAE: 0.46004 / 0.25214
2021-01-07 02:52:25.781468 Training: [16 epoch,  90 batch] loss: 3.39870, the best RMSE/MAE: 0.46004 / 0.25214
<Test> RMSE：0.42214,MAE：0.18784
2021-01-07 02:57:55.538027 Training: [17 epoch,  10 batch] loss: 3.35712, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 02:59:36.850236 Training: [17 epoch,  20 batch] loss: 3.34153, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:01:23.983599 Training: [17 epoch,  30 batch] loss: 3.33913, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:03:11.214013 Training: [17 epoch,  40 batch] loss: 3.34636, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:04:56.704905 Training: [17 epoch,  50 batch] loss: 3.29330, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:06:39.747096 Training: [17 epoch,  60 batch] loss: 3.27249, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:08:18.801897 Training: [17 epoch,  70 batch] loss: 3.27615, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:09:57.161766 Training: [17 epoch,  80 batch] loss: 3.25886, the best RMSE/MAE: 0.42214 / 0.18784
2021-01-07 03:11:37.547773 Training: [17 epoch,  90 batch] loss: 3.25535, the best RMSE/MAE: 0.42214 / 0.18784
<Test> RMSE：0.41048,MAE：0.15691
2021-01-07 03:16:35.291467 Training: [18 epoch,  10 batch] loss: 3.21240, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:18:15.875796 Training: [18 epoch,  20 batch] loss: 3.17230, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:19:56.481630 Training: [18 epoch,  30 batch] loss: 3.17577, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:21:36.241543 Training: [18 epoch,  40 batch] loss: 3.14958, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:23:14.488643 Training: [18 epoch,  50 batch] loss: 3.15330, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:24:52.412777 Training: [18 epoch,  60 batch] loss: 3.14645, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:26:31.524821 Training: [18 epoch,  70 batch] loss: 3.08741, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:28:09.195215 Training: [18 epoch,  80 batch] loss: 3.10990, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:29:47.774835 Training: [18 epoch,  90 batch] loss: 3.14135, the best RMSE/MAE: 0.41048 / 0.15691
<Test> RMSE：0.42927,MAE：0.20000
2021-01-07 03:34:42.342037 Training: [19 epoch,  10 batch] loss: 3.05179, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:36:21.643450 Training: [19 epoch,  20 batch] loss: 3.01804, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:38:00.439916 Training: [19 epoch,  30 batch] loss: 3.05870, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:39:39.424946 Training: [19 epoch,  40 batch] loss: 2.97953, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:41:19.359893 Training: [19 epoch,  50 batch] loss: 2.98645, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:42:58.039852 Training: [19 epoch,  60 batch] loss: 2.96977, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:44:36.784059 Training: [19 epoch,  70 batch] loss: 2.95817, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:46:13.878328 Training: [19 epoch,  80 batch] loss: 2.93489, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:47:53.314243 Training: [19 epoch,  90 batch] loss: 2.93417, the best RMSE/MAE: 0.41048 / 0.15691
<Test> RMSE：0.41191,MAE：0.16171
2021-01-07 03:52:51.527624 Training: [20 epoch,  10 batch] loss: 2.92165, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:54:30.904663 Training: [20 epoch,  20 batch] loss: 2.86606, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:56:11.167215 Training: [20 epoch,  30 batch] loss: 2.85951, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:57:51.471356 Training: [20 epoch,  40 batch] loss: 2.84263, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 03:59:31.908638 Training: [20 epoch,  50 batch] loss: 2.84983, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:01:09.398052 Training: [20 epoch,  60 batch] loss: 2.81450, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:02:47.395990 Training: [20 epoch,  70 batch] loss: 2.80622, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:04:26.240088 Training: [20 epoch,  80 batch] loss: 2.78143, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:06:06.590751 Training: [20 epoch,  90 batch] loss: 2.82025, the best RMSE/MAE: 0.41048 / 0.15691
<Test> RMSE：0.44375,MAE：0.22793
2021-01-07 04:11:00.211471 Training: [21 epoch,  10 batch] loss: 2.71288, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:12:40.189278 Training: [21 epoch,  20 batch] loss: 2.75333, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:14:20.675033 Training: [21 epoch,  30 batch] loss: 2.70508, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:16:00.399728 Training: [21 epoch,  40 batch] loss: 2.70840, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:17:39.223822 Training: [21 epoch,  50 batch] loss: 2.74054, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:19:15.766788 Training: [21 epoch,  60 batch] loss: 2.69083, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:20:54.128085 Training: [21 epoch,  70 batch] loss: 2.65284, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:22:32.376114 Training: [21 epoch,  80 batch] loss: 2.64026, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:24:11.828522 Training: [21 epoch,  90 batch] loss: 2.65582, the best RMSE/MAE: 0.41048 / 0.15691
<Test> RMSE：0.43206,MAE：0.20409
2021-01-07 04:29:09.742120 Training: [22 epoch,  10 batch] loss: 2.65490, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:30:53.468320 Training: [22 epoch,  20 batch] loss: 2.56569, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:32:37.095437 Training: [22 epoch,  30 batch] loss: 2.57577, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:34:17.343037 Training: [22 epoch,  40 batch] loss: 2.56316, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:35:57.030389 Training: [22 epoch,  50 batch] loss: 2.55164, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:37:36.178711 Training: [22 epoch,  60 batch] loss: 2.55010, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:39:16.069741 Training: [22 epoch,  70 batch] loss: 2.50171, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:40:53.282655 Training: [22 epoch,  80 batch] loss: 2.53045, the best RMSE/MAE: 0.41048 / 0.15691
2021-01-07 04:42:33.084115 Training: [22 epoch,  90 batch] loss: 2.47408, the best RMSE/MAE: 0.41048 / 0.15691
<Test> RMSE：0.40272,MAE：0.14250
2021-01-07 04:47:30.664420 Training: [23 epoch,  10 batch] loss: 2.48015, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:48:55.558343 Training: [23 epoch,  20 batch] loss: 2.47367, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:50:20.633922 Training: [23 epoch,  30 batch] loss: 2.40588, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:51:47.013403 Training: [23 epoch,  40 batch] loss: 2.42451, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:53:12.987606 Training: [23 epoch,  50 batch] loss: 2.43936, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:54:37.769212 Training: [23 epoch,  60 batch] loss: 2.39809, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:56:03.646162 Training: [23 epoch,  70 batch] loss: 2.38198, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:57:26.719016 Training: [23 epoch,  80 batch] loss: 2.35777, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 04:58:52.928930 Training: [23 epoch,  90 batch] loss: 2.39540, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.42616,MAE：0.19338
2021-01-07 05:03:03.463865 Training: [24 epoch,  10 batch] loss: 2.35854, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:04:18.778827 Training: [24 epoch,  20 batch] loss: 2.30421, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:05:29.973775 Training: [24 epoch,  30 batch] loss: 2.29871, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:06:40.858560 Training: [24 epoch,  40 batch] loss: 2.31594, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:07:51.050445 Training: [24 epoch,  50 batch] loss: 2.28039, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:08:59.574605 Training: [24 epoch,  60 batch] loss: 2.26925, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:10:08.142671 Training: [24 epoch,  70 batch] loss: 2.24627, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:11:16.761949 Training: [24 epoch,  80 batch] loss: 2.27205, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:12:16.791748 Training: [24 epoch,  90 batch] loss: 2.24631, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.41900,MAE：0.17721
2021-01-07 05:15:01.661994 Training: [25 epoch,  10 batch] loss: 2.19906, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:16:00.843881 Training: [25 epoch,  20 batch] loss: 2.19050, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:17:00.546091 Training: [25 epoch,  30 batch] loss: 2.19809, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:18:00.756752 Training: [25 epoch,  40 batch] loss: 2.15636, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:18:59.945818 Training: [25 epoch,  50 batch] loss: 2.16065, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:19:56.870979 Training: [25 epoch,  60 batch] loss: 2.17491, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:20:46.279143 Training: [25 epoch,  70 batch] loss: 2.17261, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:21:33.706879 Training: [25 epoch,  80 batch] loss: 2.13319, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:22:20.823558 Training: [25 epoch,  90 batch] loss: 2.09978, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.42298,MAE：0.18626
2021-01-07 05:24:29.991460 Training: [26 epoch,  10 batch] loss: 2.10124, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:25:16.722615 Training: [26 epoch,  20 batch] loss: 2.07318, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:26:03.547723 Training: [26 epoch,  30 batch] loss: 2.03940, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:26:50.443274 Training: [26 epoch,  40 batch] loss: 2.06162, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:27:36.672850 Training: [26 epoch,  50 batch] loss: 2.03438, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:28:23.001047 Training: [26 epoch,  60 batch] loss: 2.07460, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:29:10.068777 Training: [26 epoch,  70 batch] loss: 2.02628, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:29:56.983404 Training: [26 epoch,  80 batch] loss: 2.02276, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:30:43.479266 Training: [26 epoch,  90 batch] loss: 1.99811, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.41403,MAE：0.16218
2021-01-07 05:32:55.849562 Training: [27 epoch,  10 batch] loss: 1.99001, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:33:42.598053 Training: [27 epoch,  20 batch] loss: 1.95885, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:34:29.519118 Training: [27 epoch,  30 batch] loss: 1.94553, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:35:15.835562 Training: [27 epoch,  40 batch] loss: 1.96593, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:36:02.205591 Training: [27 epoch,  50 batch] loss: 1.94552, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:36:49.285732 Training: [27 epoch,  60 batch] loss: 1.90788, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:37:36.571345 Training: [27 epoch,  70 batch] loss: 1.92003, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:38:23.874151 Training: [27 epoch,  80 batch] loss: 1.88950, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:39:10.520949 Training: [27 epoch,  90 batch] loss: 1.91486, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.42148,MAE：0.18239
2021-01-07 05:41:26.857648 Training: [28 epoch,  10 batch] loss: 1.87039, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:42:17.923225 Training: [28 epoch,  20 batch] loss: 1.89119, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:43:08.401796 Training: [28 epoch,  30 batch] loss: 1.83907, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:44:01.087837 Training: [28 epoch,  40 batch] loss: 1.85011, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:44:53.689592 Training: [28 epoch,  50 batch] loss: 1.84160, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:45:46.310419 Training: [28 epoch,  60 batch] loss: 1.80501, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:46:38.710911 Training: [28 epoch,  70 batch] loss: 1.81790, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:47:30.548313 Training: [28 epoch,  80 batch] loss: 1.79997, the best RMSE/MAE: 0.40272 / 0.14250
2021-01-07 05:48:23.651768 Training: [28 epoch,  90 batch] loss: 1.79441, the best RMSE/MAE: 0.40272 / 0.14250
<Test> RMSE：0.39707,MAE：0.12830
2021-01-07 05:50:42.379861 Training: [29 epoch,  10 batch] loss: 1.75427, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:51:34.459628 Training: [29 epoch,  20 batch] loss: 1.76063, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:52:26.882256 Training: [29 epoch,  30 batch] loss: 1.75289, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:53:19.427095 Training: [29 epoch,  40 batch] loss: 1.79590, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:54:11.605772 Training: [29 epoch,  50 batch] loss: 1.73023, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:54:51.107848 Training: [29 epoch,  60 batch] loss: 1.72181, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:55:22.117819 Training: [29 epoch,  70 batch] loss: 1.68896, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:55:53.184725 Training: [29 epoch,  80 batch] loss: 1.72092, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:56:24.164097 Training: [29 epoch,  90 batch] loss: 1.70201, the best RMSE/MAE: 0.39707 / 0.12830
<Test> RMSE：0.40049,MAE：0.13548
2021-01-07 05:57:42.747873 Training: [30 epoch,  10 batch] loss: 1.66145, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:58:12.683511 Training: [30 epoch,  20 batch] loss: 1.67120, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:58:42.715515 Training: [30 epoch,  30 batch] loss: 1.71778, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:59:12.877152 Training: [30 epoch,  40 batch] loss: 1.62735, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 05:59:43.120304 Training: [30 epoch,  50 batch] loss: 1.63732, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 06:00:13.585598 Training: [30 epoch,  60 batch] loss: 1.63694, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 06:00:44.526284 Training: [30 epoch,  70 batch] loss: 1.60143, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 06:01:15.785437 Training: [30 epoch,  80 batch] loss: 1.63661, the best RMSE/MAE: 0.39707 / 0.12830
2021-01-07 06:01:46.880323 Training: [30 epoch,  90 batch] loss: 1.59678, the best RMSE/MAE: 0.39707 / 0.12830
<Test> RMSE：0.39541,MAE：0.12495
2021-01-07 06:03:06.525227 Training: [31 epoch,  10 batch] loss: 1.58009, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:03:36.948669 Training: [31 epoch,  20 batch] loss: 1.57433, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:04:07.159876 Training: [31 epoch,  30 batch] loss: 1.55413, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:04:37.391726 Training: [31 epoch,  40 batch] loss: 1.61392, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:05:07.481840 Training: [31 epoch,  50 batch] loss: 1.55655, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:05:37.692060 Training: [31 epoch,  60 batch] loss: 1.55421, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:06:08.160192 Training: [31 epoch,  70 batch] loss: 1.52741, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:06:38.599900 Training: [31 epoch,  80 batch] loss: 1.51948, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:07:09.160957 Training: [31 epoch,  90 batch] loss: 1.51504, the best RMSE/MAE: 0.39541 / 0.12495
<Test> RMSE：0.40094,MAE：0.13108
2021-01-07 06:08:29.150641 Training: [32 epoch,  10 batch] loss: 1.54458, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:08:59.588784 Training: [32 epoch,  20 batch] loss: 1.50777, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:09:30.409712 Training: [32 epoch,  30 batch] loss: 1.48335, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:10:01.071907 Training: [32 epoch,  40 batch] loss: 1.50125, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:10:31.879574 Training: [32 epoch,  50 batch] loss: 1.46179, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:11:02.630473 Training: [32 epoch,  60 batch] loss: 1.43925, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:11:33.500787 Training: [32 epoch,  70 batch] loss: 1.45055, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:12:04.416427 Training: [32 epoch,  80 batch] loss: 1.43363, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:12:35.501745 Training: [32 epoch,  90 batch] loss: 1.44651, the best RMSE/MAE: 0.39541 / 0.12495
<Test> RMSE：0.39905,MAE：0.12723
2021-01-07 06:13:59.638000 Training: [33 epoch,  10 batch] loss: 1.46039, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:14:34.157463 Training: [33 epoch,  20 batch] loss: 1.40746, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:15:08.933318 Training: [33 epoch,  30 batch] loss: 1.41317, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:15:43.769577 Training: [33 epoch,  40 batch] loss: 1.40241, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:16:18.597875 Training: [33 epoch,  50 batch] loss: 1.36750, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:16:53.536284 Training: [33 epoch,  60 batch] loss: 1.41033, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:17:28.591944 Training: [33 epoch,  70 batch] loss: 1.38627, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:18:03.841228 Training: [33 epoch,  80 batch] loss: 1.39483, the best RMSE/MAE: 0.39541 / 0.12495
2021-01-07 06:18:39.005735 Training: [33 epoch,  90 batch] loss: 1.34449, the best RMSE/MAE: 0.39541 / 0.12495
<Test> RMSE：0.39331,MAE：0.12104
2021-01-07 06:20:05.061545 Training: [34 epoch,  10 batch] loss: 1.32589, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:20:39.726272 Training: [34 epoch,  20 batch] loss: 1.30992, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:21:14.429425 Training: [34 epoch,  30 batch] loss: 1.32064, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:21:49.233457 Training: [34 epoch,  40 batch] loss: 1.33525, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:22:23.810205 Training: [34 epoch,  50 batch] loss: 1.35223, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:22:54.251883 Training: [34 epoch,  60 batch] loss: 1.36601, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:23:24.666458 Training: [34 epoch,  70 batch] loss: 1.30526, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:23:55.104737 Training: [34 epoch,  80 batch] loss: 1.28699, the best RMSE/MAE: 0.39331 / 0.12104
2021-01-07 06:24:25.688830 Training: [34 epoch,  90 batch] loss: 1.30707, the best RMSE/MAE: 0.39331 / 0.12104
<Test> RMSE：0.39191,MAE：0.11859
2021-01-07 06:25:45.433141 Training: [35 epoch,  10 batch] loss: 1.26676, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:26:15.985555 Training: [35 epoch,  20 batch] loss: 1.25514, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:26:46.733646 Training: [35 epoch,  30 batch] loss: 1.25737, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:27:17.537298 Training: [35 epoch,  40 batch] loss: 1.24555, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:27:48.546228 Training: [35 epoch,  50 batch] loss: 1.25427, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:28:19.718110 Training: [35 epoch,  60 batch] loss: 1.29111, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:28:50.637057 Training: [35 epoch,  70 batch] loss: 1.24272, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:29:21.669423 Training: [35 epoch,  80 batch] loss: 1.24954, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:29:52.708610 Training: [35 epoch,  90 batch] loss: 1.24317, the best RMSE/MAE: 0.39191 / 0.11859
<Test> RMSE：0.39839,MAE：0.12524
2021-01-07 06:31:12.428647 Training: [36 epoch,  10 batch] loss: 1.21524, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:31:42.456349 Training: [36 epoch,  20 batch] loss: 1.21899, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:32:12.854716 Training: [36 epoch,  30 batch] loss: 1.20900, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:32:43.893557 Training: [36 epoch,  40 batch] loss: 1.21297, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:33:14.896726 Training: [36 epoch,  50 batch] loss: 1.20269, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:33:45.994386 Training: [36 epoch,  60 batch] loss: 1.17649, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:34:17.160048 Training: [36 epoch,  70 batch] loss: 1.20762, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:34:48.429391 Training: [36 epoch,  80 batch] loss: 1.15675, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:35:19.827515 Training: [36 epoch,  90 batch] loss: 1.15669, the best RMSE/MAE: 0.39191 / 0.11859
<Test> RMSE：0.40303,MAE：0.12890
2021-01-07 06:36:40.301388 Training: [37 epoch,  10 batch] loss: 1.17251, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:37:11.013307 Training: [37 epoch,  20 batch] loss: 1.14883, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:37:41.917427 Training: [37 epoch,  30 batch] loss: 1.14771, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:38:12.722042 Training: [37 epoch,  40 batch] loss: 1.13503, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:38:43.478020 Training: [37 epoch,  50 batch] loss: 1.13957, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:39:14.281074 Training: [37 epoch,  60 batch] loss: 1.10785, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:39:45.083995 Training: [37 epoch,  70 batch] loss: 1.13597, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:40:16.076780 Training: [37 epoch,  80 batch] loss: 1.11444, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:40:47.177274 Training: [37 epoch,  90 batch] loss: 1.13208, the best RMSE/MAE: 0.39191 / 0.11859
<Test> RMSE：0.39403,MAE：0.11955
2021-01-07 06:42:14.523633 Training: [38 epoch,  10 batch] loss: 1.10339, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:42:44.768445 Training: [38 epoch,  20 batch] loss: 1.09518, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:43:15.165503 Training: [38 epoch,  30 batch] loss: 1.07864, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:43:45.752657 Training: [38 epoch,  40 batch] loss: 1.06700, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:44:16.172066 Training: [38 epoch,  50 batch] loss: 1.07778, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:44:46.455789 Training: [38 epoch,  60 batch] loss: 1.06369, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:45:16.836438 Training: [38 epoch,  70 batch] loss: 1.12424, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:45:47.449733 Training: [38 epoch,  80 batch] loss: 1.07444, the best RMSE/MAE: 0.39191 / 0.11859
2021-01-07 06:46:18.156911 Training: [38 epoch,  90 batch] loss: 1.04017, the best RMSE/MAE: 0.39191 / 0.11859
<Test> RMSE：0.39031,MAE：0.12764
2021-01-07 06:47:37.653743 Training: [39 epoch,  10 batch] loss: 1.03982, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:48:07.719068 Training: [39 epoch,  20 batch] loss: 1.05831, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:48:37.863768 Training: [39 epoch,  30 batch] loss: 1.03240, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:49:08.079667 Training: [39 epoch,  40 batch] loss: 1.04905, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:49:38.235302 Training: [39 epoch,  50 batch] loss: 1.00170, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:50:08.669011 Training: [39 epoch,  60 batch] loss: 1.00070, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:50:39.087803 Training: [39 epoch,  70 batch] loss: 1.02826, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:51:09.596650 Training: [39 epoch,  80 batch] loss: 1.00481, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:51:40.140673 Training: [39 epoch,  90 batch] loss: 1.02253, the best RMSE/MAE: 0.39031 / 0.12764
<Test> RMSE：0.39248,MAE：0.12004
2021-01-07 06:53:03.734506 Training: [40 epoch,  10 batch] loss: 0.99970, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:53:38.420537 Training: [40 epoch,  20 batch] loss: 0.98767, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:54:13.358022 Training: [40 epoch,  30 batch] loss: 0.97341, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:54:48.317307 Training: [40 epoch,  40 batch] loss: 0.99825, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:55:23.123110 Training: [40 epoch,  50 batch] loss: 0.97956, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:55:58.065099 Training: [40 epoch,  60 batch] loss: 0.95796, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:56:33.087474 Training: [40 epoch,  70 batch] loss: 0.96961, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:57:08.225242 Training: [40 epoch,  80 batch] loss: 0.94884, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:57:43.421436 Training: [40 epoch,  90 batch] loss: 0.99436, the best RMSE/MAE: 0.39031 / 0.12764
<Test> RMSE：0.39353,MAE：0.11321
2021-01-07 06:59:05.559989 Training: [41 epoch,  10 batch] loss: 0.94796, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 06:59:35.661022 Training: [41 epoch,  20 batch] loss: 0.94720, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:00:05.884645 Training: [41 epoch,  30 batch] loss: 0.95042, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:00:36.195134 Training: [41 epoch,  40 batch] loss: 0.93571, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:01:06.519913 Training: [41 epoch,  50 batch] loss: 0.95758, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:01:36.965630 Training: [41 epoch,  60 batch] loss: 0.91715, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:02:07.417109 Training: [41 epoch,  70 batch] loss: 0.90728, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:02:38.066982 Training: [41 epoch,  80 batch] loss: 0.91504, the best RMSE/MAE: 0.39031 / 0.12764
2021-01-07 07:03:08.576812 Training: [41 epoch,  90 batch] loss: 0.90640, the best RMSE/MAE: 0.39031 / 0.12764
<Test> RMSE：0.38841,MAE：0.13888
2021-01-07 07:04:34.485211 Training: [42 epoch,  10 batch] loss: 0.89713, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:05:09.380788 Training: [42 epoch,  20 batch] loss: 0.90418, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:05:44.135684 Training: [42 epoch,  30 batch] loss: 0.87204, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:06:18.896092 Training: [42 epoch,  40 batch] loss: 0.89565, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:06:53.633131 Training: [42 epoch,  50 batch] loss: 0.93932, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:07:28.557539 Training: [42 epoch,  60 batch] loss: 0.90150, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:08:03.502416 Training: [42 epoch,  70 batch] loss: 0.87408, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:08:38.588862 Training: [42 epoch,  80 batch] loss: 0.89025, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:09:13.333457 Training: [42 epoch,  90 batch] loss: 0.84164, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.39022,MAE：0.18202
2021-01-07 07:10:35.076930 Training: [43 epoch,  10 batch] loss: 0.86168, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:11:04.727780 Training: [43 epoch,  20 batch] loss: 0.88413, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:11:34.451174 Training: [43 epoch,  30 batch] loss: 0.84533, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:12:04.360542 Training: [43 epoch,  40 batch] loss: 0.83637, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:12:34.377659 Training: [43 epoch,  50 batch] loss: 0.84097, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:13:04.263862 Training: [43 epoch,  60 batch] loss: 0.84319, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:13:34.223237 Training: [43 epoch,  70 batch] loss: 0.85526, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:14:04.463951 Training: [43 epoch,  80 batch] loss: 0.81678, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:14:35.008024 Training: [43 epoch,  90 batch] loss: 0.86111, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.38966,MAE：0.15527
2021-01-07 07:15:58.525237 Training: [44 epoch,  10 batch] loss: 0.82115, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:16:32.773132 Training: [44 epoch,  20 batch] loss: 0.81412, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:17:07.018323 Training: [44 epoch,  30 batch] loss: 0.86170, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:17:41.321238 Training: [44 epoch,  40 batch] loss: 0.83376, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:18:15.669950 Training: [44 epoch,  50 batch] loss: 0.79308, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:18:50.148827 Training: [44 epoch,  60 batch] loss: 0.82014, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:19:24.818085 Training: [44 epoch,  70 batch] loss: 0.79709, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:19:59.643767 Training: [44 epoch,  80 batch] loss: 0.79466, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:20:34.513229 Training: [44 epoch,  90 batch] loss: 0.77889, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.39109,MAE：0.18625
2021-01-07 07:21:56.379452 Training: [45 epoch,  10 batch] loss: 0.79692, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:22:26.346613 Training: [45 epoch,  20 batch] loss: 0.80017, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:22:56.580288 Training: [45 epoch,  30 batch] loss: 0.77887, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:23:26.404384 Training: [45 epoch,  40 batch] loss: 0.77098, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:23:56.132315 Training: [45 epoch,  50 batch] loss: 0.76581, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:24:26.162934 Training: [45 epoch,  60 batch] loss: 0.81592, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:24:56.115476 Training: [45 epoch,  70 batch] loss: 0.75661, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:25:26.256895 Training: [45 epoch,  80 batch] loss: 0.76508, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:25:56.549329 Training: [45 epoch,  90 batch] loss: 0.75289, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.39007,MAE：0.17916
2021-01-07 07:27:16.003405 Training: [46 epoch,  10 batch] loss: 0.75311, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:27:46.296072 Training: [46 epoch,  20 batch] loss: 0.73857, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:28:16.629241 Training: [46 epoch,  30 batch] loss: 0.71896, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:28:46.995313 Training: [46 epoch,  40 batch] loss: 0.79182, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:29:17.362939 Training: [46 epoch,  50 batch] loss: 0.71982, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:29:47.801383 Training: [46 epoch,  60 batch] loss: 0.75630, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:30:18.268367 Training: [46 epoch,  70 batch] loss: 0.72841, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:30:48.836234 Training: [46 epoch,  80 batch] loss: 0.72605, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:31:19.427345 Training: [46 epoch,  90 batch] loss: 0.72197, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.38990,MAE：0.17632
2021-01-07 07:32:38.681841 Training: [47 epoch,  10 batch] loss: 0.76195, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:33:08.736469 Training: [47 epoch,  20 batch] loss: 0.71557, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:33:38.928859 Training: [47 epoch,  30 batch] loss: 0.72078, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:34:09.353216 Training: [47 epoch,  40 batch] loss: 0.73206, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:34:39.659455 Training: [47 epoch,  50 batch] loss: 0.69208, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:35:10.017582 Training: [47 epoch,  60 batch] loss: 0.69709, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:35:40.490569 Training: [47 epoch,  70 batch] loss: 0.68484, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:36:11.106643 Training: [47 epoch,  80 batch] loss: 0.69375, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:36:41.855657 Training: [47 epoch,  90 batch] loss: 0.70744, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.39020,MAE：0.18457
2021-01-07 07:38:08.644989 Training: [48 epoch,  10 batch] loss: 0.70032, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:38:38.793298 Training: [48 epoch,  20 batch] loss: 0.69609, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:39:09.087921 Training: [48 epoch,  30 batch] loss: 0.69045, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:39:39.408506 Training: [48 epoch,  40 batch] loss: 0.72037, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:40:09.764617 Training: [48 epoch,  50 batch] loss: 0.65039, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:40:40.121537 Training: [48 epoch,  60 batch] loss: 0.69085, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:41:10.507522 Training: [48 epoch,  70 batch] loss: 0.67826, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:41:41.064416 Training: [48 epoch,  80 batch] loss: 0.68459, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:42:11.672061 Training: [48 epoch,  90 batch] loss: 0.64617, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.39154,MAE：0.19153
2021-01-07 07:43:38.663650 Training: [49 epoch,  10 batch] loss: 0.63943, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:44:08.728561 Training: [49 epoch,  20 batch] loss: 0.65140, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:44:39.030703 Training: [49 epoch,  30 batch] loss: 0.68712, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:45:09.360174 Training: [49 epoch,  40 batch] loss: 0.65534, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:45:39.709973 Training: [49 epoch,  50 batch] loss: 0.64829, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:46:10.199165 Training: [49 epoch,  60 batch] loss: 0.63544, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:46:40.753623 Training: [49 epoch,  70 batch] loss: 0.63229, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:47:11.408966 Training: [49 epoch,  80 batch] loss: 0.67197, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:47:42.097756 Training: [49 epoch,  90 batch] loss: 0.65881, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.38922,MAE：0.14279
2021-01-07 07:49:01.203885 Training: [50 epoch,  10 batch] loss: 0.62887, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:49:31.268206 Training: [50 epoch,  20 batch] loss: 0.63536, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:50:01.657589 Training: [50 epoch,  30 batch] loss: 0.61714, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:50:31.997433 Training: [50 epoch,  40 batch] loss: 0.61222, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:51:02.392007 Training: [50 epoch,  50 batch] loss: 0.61711, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:51:32.844372 Training: [50 epoch,  60 batch] loss: 0.65850, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:52:03.309384 Training: [50 epoch,  70 batch] loss: 0.63031, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:52:33.909209 Training: [50 epoch,  80 batch] loss: 0.61913, the best RMSE/MAE: 0.38841 / 0.13888
2021-01-07 07:53:04.460913 Training: [50 epoch,  90 batch] loss: 0.63153, the best RMSE/MAE: 0.38841 / 0.13888
<Test> RMSE：0.38821,MAE：0.15990
2021-01-07 07:54:26.949922 Training: [51 epoch,  10 batch] loss: 0.60541, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:55:01.012508 Training: [51 epoch,  20 batch] loss: 0.63412, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:55:35.269685 Training: [51 epoch,  30 batch] loss: 0.61072, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:56:09.570258 Training: [51 epoch,  40 batch] loss: 0.62517, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:56:43.970482 Training: [51 epoch,  50 batch] loss: 0.58415, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:57:18.583722 Training: [51 epoch,  60 batch] loss: 0.58544, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:57:53.154008 Training: [51 epoch,  70 batch] loss: 0.59365, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:58:27.625815 Training: [51 epoch,  80 batch] loss: 0.61221, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 07:59:02.128980 Training: [51 epoch,  90 batch] loss: 0.58047, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39103,MAE：0.16808
2021-01-07 08:00:23.692316 Training: [52 epoch,  10 batch] loss: 0.57516, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:00:53.569182 Training: [52 epoch,  20 batch] loss: 0.61285, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:01:23.607517 Training: [52 epoch,  30 batch] loss: 0.56400, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:01:53.745832 Training: [52 epoch,  40 batch] loss: 0.55890, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:02:24.132686 Training: [52 epoch,  50 batch] loss: 0.59789, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:02:54.728169 Training: [52 epoch,  60 batch] loss: 0.57499, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:03:25.396913 Training: [52 epoch,  70 batch] loss: 0.56866, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:03:56.116752 Training: [52 epoch,  80 batch] loss: 0.57578, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:04:26.732545 Training: [52 epoch,  90 batch] loss: 0.59752, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39585,MAE：0.22267
2021-01-07 08:05:47.804436 Training: [53 epoch,  10 batch] loss: 0.56273, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:06:17.246013 Training: [53 epoch,  20 batch] loss: 0.56237, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:06:46.919595 Training: [53 epoch,  30 batch] loss: 0.54863, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:07:16.525290 Training: [53 epoch,  40 batch] loss: 0.55877, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:07:46.255659 Training: [53 epoch,  50 batch] loss: 0.55650, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:08:16.059736 Training: [53 epoch,  60 batch] loss: 0.53900, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:08:45.871863 Training: [53 epoch,  70 batch] loss: 0.60889, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:09:15.833095 Training: [53 epoch,  80 batch] loss: 0.55369, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:09:45.771324 Training: [53 epoch,  90 batch] loss: 0.54290, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39229,MAE：0.20397
2021-01-07 08:11:03.295942 Training: [54 epoch,  10 batch] loss: 0.53213, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:11:32.783527 Training: [54 epoch,  20 batch] loss: 0.54908, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:12:02.902981 Training: [54 epoch,  30 batch] loss: 0.56479, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:12:33.301247 Training: [54 epoch,  40 batch] loss: 0.52860, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:13:03.645012 Training: [54 epoch,  50 batch] loss: 0.52964, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:13:34.124372 Training: [54 epoch,  60 batch] loss: 0.52651, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:14:04.646853 Training: [54 epoch,  70 batch] loss: 0.52858, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:14:35.255523 Training: [54 epoch,  80 batch] loss: 0.55726, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:15:05.946903 Training: [54 epoch,  90 batch] loss: 0.53464, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39612,MAE：0.21304
2021-01-07 08:16:28.421839 Training: [55 epoch,  10 batch] loss: 0.53026, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:17:02.578852 Training: [55 epoch,  20 batch] loss: 0.53490, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:17:33.098413 Training: [55 epoch,  30 batch] loss: 0.52379, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:18:03.379336 Training: [55 epoch,  40 batch] loss: 0.49054, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:18:33.716574 Training: [55 epoch,  50 batch] loss: 0.50291, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:19:04.076807 Training: [55 epoch,  60 batch] loss: 0.51315, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:19:34.593310 Training: [55 epoch,  70 batch] loss: 0.53732, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:20:05.165009 Training: [55 epoch,  80 batch] loss: 0.49173, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:20:35.720810 Training: [55 epoch,  90 batch] loss: 0.50593, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39668,MAE：0.22082
2021-01-07 08:22:02.323359 Training: [56 epoch,  10 batch] loss: 0.51164, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:22:32.484771 Training: [56 epoch,  20 batch] loss: 0.54804, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:23:02.708409 Training: [56 epoch,  30 batch] loss: 0.51990, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:23:33.192251 Training: [56 epoch,  40 batch] loss: 0.49408, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:24:03.595876 Training: [56 epoch,  50 batch] loss: 0.48488, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:24:34.069350 Training: [56 epoch,  60 batch] loss: 0.48083, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:25:04.652810 Training: [56 epoch,  70 batch] loss: 0.52191, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:25:35.361433 Training: [56 epoch,  80 batch] loss: 0.47609, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:26:06.099890 Training: [56 epoch,  90 batch] loss: 0.49645, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.40300,MAE：0.24597
2021-01-07 08:27:25.168799 Training: [57 epoch,  10 batch] loss: 0.49061, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:27:54.763634 Training: [57 epoch,  20 batch] loss: 0.45972, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:28:24.534235 Training: [57 epoch,  30 batch] loss: 0.51552, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:28:55.115777 Training: [57 epoch,  40 batch] loss: 0.48617, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:29:25.718651 Training: [57 epoch,  50 batch] loss: 0.46923, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:29:56.402132 Training: [57 epoch,  60 batch] loss: 0.48619, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:30:31.198760 Training: [57 epoch,  70 batch] loss: 0.47519, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:31:05.994606 Training: [57 epoch,  80 batch] loss: 0.46146, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:31:39.720442 Training: [57 epoch,  90 batch] loss: 0.47831, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39134,MAE：0.18950
2021-01-07 08:33:06.740697 Training: [58 epoch,  10 batch] loss: 0.47413, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:33:36.945387 Training: [58 epoch,  20 batch] loss: 0.47287, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:34:07.296684 Training: [58 epoch,  30 batch] loss: 0.45940, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:34:37.480603 Training: [58 epoch,  40 batch] loss: 0.44875, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:35:07.712602 Training: [58 epoch,  50 batch] loss: 0.51846, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:35:37.964709 Training: [58 epoch,  60 batch] loss: 0.45312, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:36:08.371607 Training: [58 epoch,  70 batch] loss: 0.47306, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:36:38.811599 Training: [58 epoch,  80 batch] loss: 0.45952, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:37:09.316523 Training: [58 epoch,  90 batch] loss: 0.47127, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39448,MAE：0.20966
2021-01-07 08:38:27.700604 Training: [59 epoch,  10 batch] loss: 0.46844, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:38:57.751546 Training: [59 epoch,  20 batch] loss: 0.44956, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:39:28.000052 Training: [59 epoch,  30 batch] loss: 0.45360, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:39:58.235937 Training: [59 epoch,  40 batch] loss: 0.44170, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:40:28.652872 Training: [59 epoch,  50 batch] loss: 0.45911, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:40:59.100812 Training: [59 epoch,  60 batch] loss: 0.45006, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:41:29.555155 Training: [59 epoch,  70 batch] loss: 0.44794, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:42:00.199816 Training: [59 epoch,  80 batch] loss: 0.49226, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:42:30.968234 Training: [59 epoch,  90 batch] loss: 0.47344, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39281,MAE：0.17560
2021-01-07 08:43:53.149887 Training: [60 epoch,  10 batch] loss: 0.44600, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:44:27.556465 Training: [60 epoch,  20 batch] loss: 0.43711, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:45:01.945191 Training: [60 epoch,  30 batch] loss: 0.47489, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:45:36.234349 Training: [60 epoch,  40 batch] loss: 0.42962, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:46:10.654088 Training: [60 epoch,  50 batch] loss: 0.45812, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:46:45.097154 Training: [60 epoch,  60 batch] loss: 0.45993, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:47:19.630070 Training: [60 epoch,  70 batch] loss: 0.43898, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:47:54.298563 Training: [60 epoch,  80 batch] loss: 0.47598, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:48:28.904826 Training: [60 epoch,  90 batch] loss: 0.47110, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39578,MAE：0.21736
2021-01-07 08:49:49.975377 Training: [61 epoch,  10 batch] loss: 0.44087, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:50:20.369272 Training: [61 epoch,  20 batch] loss: 0.44264, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:50:50.813305 Training: [61 epoch,  30 batch] loss: 0.45957, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:51:21.324997 Training: [61 epoch,  40 batch] loss: 0.43840, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:51:51.752135 Training: [61 epoch,  50 batch] loss: 0.44491, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:52:22.201067 Training: [61 epoch,  60 batch] loss: 0.47216, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:52:52.742388 Training: [61 epoch,  70 batch] loss: 0.46994, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:53:23.396526 Training: [61 epoch,  80 batch] loss: 0.43843, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:53:54.079842 Training: [61 epoch,  90 batch] loss: 0.42574, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.40878,MAE：0.25247
2021-01-07 08:55:12.439319 Training: [62 epoch,  10 batch] loss: 0.42129, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:55:42.604800 Training: [62 epoch,  20 batch] loss: 0.52222, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:56:12.890738 Training: [62 epoch,  30 batch] loss: 0.42792, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:56:43.241103 Training: [62 epoch,  40 batch] loss: 0.44764, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:57:13.596078 Training: [62 epoch,  50 batch] loss: 0.44158, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:57:44.052893 Training: [62 epoch,  60 batch] loss: 0.43494, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:58:14.493096 Training: [62 epoch,  70 batch] loss: 0.42435, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:58:45.087136 Training: [62 epoch,  80 batch] loss: 0.43115, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 08:59:15.693377 Training: [62 epoch,  90 batch] loss: 0.43910, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39181,MAE：0.20080
2021-01-07 09:00:35.404297 Training: [63 epoch,  10 batch] loss: 0.42456, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:01:05.511961 Training: [63 epoch,  20 batch] loss: 0.43820, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:01:35.775424 Training: [63 epoch,  30 batch] loss: 0.45623, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:02:06.051324 Training: [63 epoch,  40 batch] loss: 0.43778, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:02:36.368877 Training: [63 epoch,  50 batch] loss: 0.43444, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:03:06.744501 Training: [63 epoch,  60 batch] loss: 0.45603, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:03:37.268617 Training: [63 epoch,  70 batch] loss: 0.43171, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:04:07.941193 Training: [63 epoch,  80 batch] loss: 0.45727, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:04:38.616747 Training: [63 epoch,  90 batch] loss: 0.41571, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.40986,MAE：0.26311
2021-01-07 09:05:58.167966 Training: [64 epoch,  10 batch] loss: 0.45745, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:06:28.356693 Training: [64 epoch,  20 batch] loss: 0.41409, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:06:58.637145 Training: [64 epoch,  30 batch] loss: 0.42913, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:07:28.956312 Training: [64 epoch,  40 batch] loss: 0.43992, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:07:59.280980 Training: [64 epoch,  50 batch] loss: 0.41731, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:08:29.881417 Training: [64 epoch,  60 batch] loss: 0.45681, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:09:00.612600 Training: [64 epoch,  70 batch] loss: 0.41979, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:09:31.469456 Training: [64 epoch,  80 batch] loss: 0.44123, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:10:02.389599 Training: [64 epoch,  90 batch] loss: 0.43476, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.41252,MAE：0.25653
2021-01-07 09:11:21.168908 Training: [65 epoch,  10 batch] loss: 0.41978, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:11:50.708730 Training: [65 epoch,  20 batch] loss: 0.44506, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:12:20.488680 Training: [65 epoch,  30 batch] loss: 0.42326, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:12:50.295996 Training: [65 epoch,  40 batch] loss: 0.41111, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:13:20.225027 Training: [65 epoch,  50 batch] loss: 0.44093, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:13:50.188509 Training: [65 epoch,  60 batch] loss: 0.44112, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:14:20.398303 Training: [65 epoch,  70 batch] loss: 0.43892, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:14:51.145847 Training: [65 epoch,  80 batch] loss: 0.41993, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:15:21.872875 Training: [65 epoch,  90 batch] loss: 0.41486, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39502,MAE：0.21448
2021-01-07 09:16:44.987299 Training: [66 epoch,  10 batch] loss: 0.42187, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:17:19.656573 Training: [66 epoch,  20 batch] loss: 0.41437, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:17:54.589783 Training: [66 epoch,  30 batch] loss: 0.44948, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:18:29.479495 Training: [66 epoch,  40 batch] loss: 0.41659, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:19:04.460060 Training: [66 epoch,  50 batch] loss: 0.41881, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:19:39.527703 Training: [66 epoch,  60 batch] loss: 0.47029, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:20:14.783822 Training: [66 epoch,  70 batch] loss: 0.43755, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:20:49.953202 Training: [66 epoch,  80 batch] loss: 0.41752, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:21:25.237020 Training: [66 epoch,  90 batch] loss: 0.42285, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.40214,MAE：0.23266
2021-01-07 09:22:50.453631 Training: [67 epoch,  10 batch] loss: 0.41696, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:23:25.151927 Training: [67 epoch,  20 batch] loss: 0.40078, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:24:00.145819 Training: [67 epoch,  30 batch] loss: 0.41881, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:24:35.107619 Training: [67 epoch,  40 batch] loss: 0.41375, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:25:10.160922 Training: [67 epoch,  50 batch] loss: 0.42048, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:25:45.182370 Training: [67 epoch,  60 batch] loss: 0.42571, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:26:20.338008 Training: [67 epoch,  70 batch] loss: 0.42268, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:26:55.586148 Training: [67 epoch,  80 batch] loss: 0.45930, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:27:29.000377 Training: [67 epoch,  90 batch] loss: 0.41794, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.40052,MAE：0.22532
2021-01-07 09:28:48.744204 Training: [68 epoch,  10 batch] loss: 0.41089, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:29:18.922944 Training: [68 epoch,  20 batch] loss: 0.41363, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:29:49.012363 Training: [68 epoch,  30 batch] loss: 0.39604, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:30:19.115337 Training: [68 epoch,  40 batch] loss: 0.39709, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:30:49.317970 Training: [68 epoch,  50 batch] loss: 0.41302, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:31:19.482738 Training: [68 epoch,  60 batch] loss: 0.44591, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:31:49.745172 Training: [68 epoch,  70 batch] loss: 0.41605, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:32:20.163876 Training: [68 epoch,  80 batch] loss: 0.45892, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:32:50.641042 Training: [68 epoch,  90 batch] loss: 0.45647, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.39634,MAE：0.21069
2021-01-07 09:34:14.488572 Training: [69 epoch,  10 batch] loss: 0.43071, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:34:49.133831 Training: [69 epoch,  20 batch] loss: 0.40126, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:35:23.902700 Training: [69 epoch,  30 batch] loss: 0.42482, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:35:58.690166 Training: [69 epoch,  40 batch] loss: 0.44251, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:36:33.501646 Training: [69 epoch,  50 batch] loss: 0.41943, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:37:08.402055 Training: [69 epoch,  60 batch] loss: 0.43013, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:37:43.521855 Training: [69 epoch,  70 batch] loss: 0.40564, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:38:18.635946 Training: [69 epoch,  80 batch] loss: 0.42038, the best RMSE/MAE: 0.38821 / 0.15990
2021-01-07 09:38:53.778037 Training: [69 epoch,  90 batch] loss: 0.41015, the best RMSE/MAE: 0.38821 / 0.15990
<Test> RMSE：0.41181,MAE：0.26106
