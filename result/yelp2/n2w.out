-------------------- Hyperparams --------------------
time: 2021-01-06 10:55:45.119042
Dataset: yelp
N: 20000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-06 11:11:13.615586 Training: [1 epoch,  10 batch] loss: 17.12933, the best RMSE/MAE: inf / inf
2021-01-06 11:12:20.540980 Training: [1 epoch,  20 batch] loss: 16.74279, the best RMSE/MAE: inf / inf
2021-01-06 11:13:34.202932 Training: [1 epoch,  30 batch] loss: 16.45248, the best RMSE/MAE: inf / inf
2021-01-06 11:14:45.360209 Training: [1 epoch,  40 batch] loss: 16.26123, the best RMSE/MAE: inf / inf
2021-01-06 11:15:56.223578 Training: [1 epoch,  50 batch] loss: 16.06811, the best RMSE/MAE: inf / inf
2021-01-06 11:17:08.525671 Training: [1 epoch,  60 batch] loss: 15.93443, the best RMSE/MAE: inf / inf
2021-01-06 11:18:19.413406 Training: [1 epoch,  70 batch] loss: 15.90289, the best RMSE/MAE: inf / inf
2021-01-06 11:19:30.460192 Training: [1 epoch,  80 batch] loss: 15.75683, the best RMSE/MAE: inf / inf
2021-01-06 11:20:39.459969 Training: [1 epoch,  90 batch] loss: 15.72945, the best RMSE/MAE: inf / inf
<Test> RMSE：186673104.00000,MAE：144798672.00000
2021-01-06 11:23:52.229755 Training: [2 epoch,  10 batch] loss: 15.66451, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:24:52.562721 Training: [2 epoch,  20 batch] loss: 15.62883, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:25:54.576919 Training: [2 epoch,  30 batch] loss: 15.58597, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:27:04.398214 Training: [2 epoch,  40 batch] loss: 15.54333, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:28:15.488042 Training: [2 epoch,  50 batch] loss: 15.50401, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:29:16.499542 Training: [2 epoch,  60 batch] loss: 15.44974, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:30:18.320311 Training: [2 epoch,  70 batch] loss: 15.44871, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:31:19.867879 Training: [2 epoch,  80 batch] loss: 15.45917, the best RMSE/MAE: 186673104.00000 / 144798672.00000
2021-01-06 11:32:20.917200 Training: [2 epoch,  90 batch] loss: 15.41280, the best RMSE/MAE: 186673104.00000 / 144798672.00000
<Test> RMSE：231224.62500,MAE：175778.67188
2021-01-06 11:35:10.560763 Training: [3 epoch,  10 batch] loss: 15.30701, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:36:16.560997 Training: [3 epoch,  20 batch] loss: 15.25884, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:37:21.956551 Training: [3 epoch,  30 batch] loss: 15.26323, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:38:27.984014 Training: [3 epoch,  40 batch] loss: 15.19169, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:39:31.237414 Training: [3 epoch,  50 batch] loss: 15.15944, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:40:30.819766 Training: [3 epoch,  60 batch] loss: 15.11042, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:41:47.005842 Training: [3 epoch,  70 batch] loss: 15.11448, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:43:13.489439 Training: [3 epoch,  80 batch] loss: 15.12575, the best RMSE/MAE: 231224.62500 / 175778.67188
2021-01-06 11:44:37.930200 Training: [3 epoch,  90 batch] loss: 15.03004, the best RMSE/MAE: 231224.62500 / 175778.67188
<Test> RMSE：4398.19531,MAE：3287.91406
2021-01-06 11:48:48.476925 Training: [4 epoch,  10 batch] loss: 14.98031, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:50:15.726046 Training: [4 epoch,  20 batch] loss: 14.93741, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:51:42.666257 Training: [4 epoch,  30 batch] loss: 14.92182, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:53:08.973904 Training: [4 epoch,  40 batch] loss: 14.87320, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:54:35.387078 Training: [4 epoch,  50 batch] loss: 14.81512, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:55:57.676946 Training: [4 epoch,  60 batch] loss: 14.77555, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:57:22.503575 Training: [4 epoch,  70 batch] loss: 14.76006, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 11:58:48.869871 Training: [4 epoch,  80 batch] loss: 14.74206, the best RMSE/MAE: 4398.19531 / 3287.91406
2021-01-06 12:00:13.511194 Training: [4 epoch,  90 batch] loss: 14.71139, the best RMSE/MAE: 4398.19531 / 3287.91406
<Test> RMSE：297.03180,MAE：213.87407
2021-01-06 12:04:23.849531 Training: [5 epoch,  10 batch] loss: 14.60594, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:05:49.791837 Training: [5 epoch,  20 batch] loss: 14.58292, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:07:15.526507 Training: [5 epoch,  30 batch] loss: 14.58887, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:08:42.492423 Training: [5 epoch,  40 batch] loss: 14.52261, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:10:09.719443 Training: [5 epoch,  50 batch] loss: 14.48294, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:11:32.214101 Training: [5 epoch,  60 batch] loss: 14.42088, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:12:56.305989 Training: [5 epoch,  70 batch] loss: 14.41082, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:14:22.308669 Training: [5 epoch,  80 batch] loss: 14.35500, the best RMSE/MAE: 297.03180 / 213.87407
2021-01-06 12:15:46.785179 Training: [5 epoch,  90 batch] loss: 14.30838, the best RMSE/MAE: 297.03180 / 213.87407
<Test> RMSE：49.87312,MAE：36.66168
2021-01-06 12:19:56.183545 Training: [6 epoch,  10 batch] loss: 14.21599, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:21:22.734913 Training: [6 epoch,  20 batch] loss: 14.19882, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:22:50.351339 Training: [6 epoch,  30 batch] loss: 14.15244, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:24:18.741482 Training: [6 epoch,  40 batch] loss: 14.12556, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:25:46.113607 Training: [6 epoch,  50 batch] loss: 14.06861, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:27:08.362149 Training: [6 epoch,  60 batch] loss: 14.05338, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:28:32.776533 Training: [6 epoch,  70 batch] loss: 14.04953, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:30:03.528849 Training: [6 epoch,  80 batch] loss: 13.96865, the best RMSE/MAE: 49.87312 / 36.66168
2021-01-06 12:31:29.271692 Training: [6 epoch,  90 batch] loss: 13.96013, the best RMSE/MAE: 49.87312 / 36.66168
<Test> RMSE：17.74157,MAE：13.74995
2021-01-06 12:35:38.532791 Training: [7 epoch,  10 batch] loss: 13.86067, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:37:04.553613 Training: [7 epoch,  20 batch] loss: 13.80068, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:38:30.983805 Training: [7 epoch,  30 batch] loss: 13.76396, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:39:58.097362 Training: [7 epoch,  40 batch] loss: 13.70976, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:41:25.033815 Training: [7 epoch,  50 batch] loss: 13.69375, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:42:47.909329 Training: [7 epoch,  60 batch] loss: 13.61917, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:44:12.252235 Training: [7 epoch,  70 batch] loss: 13.59060, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:45:38.636340 Training: [7 epoch,  80 batch] loss: 13.58491, the best RMSE/MAE: 17.74157 / 13.74995
2021-01-06 12:47:03.108080 Training: [7 epoch,  90 batch] loss: 13.50674, the best RMSE/MAE: 17.74157 / 13.74995
<Test> RMSE：8.87716,MAE：7.38544
2021-01-06 12:51:17.217604 Training: [8 epoch,  10 batch] loss: 13.48201, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:52:44.112271 Training: [8 epoch,  20 batch] loss: 13.38771, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:54:10.958628 Training: [8 epoch,  30 batch] loss: 13.33740, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:55:37.862973 Training: [8 epoch,  40 batch] loss: 13.29721, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:57:04.588144 Training: [8 epoch,  50 batch] loss: 13.23888, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:58:27.820901 Training: [8 epoch,  60 batch] loss: 13.20389, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 12:59:52.674050 Training: [8 epoch,  70 batch] loss: 13.14037, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 13:01:19.686256 Training: [8 epoch,  80 batch] loss: 13.07438, the best RMSE/MAE: 8.87716 / 7.38544
2021-01-06 13:02:44.859912 Training: [8 epoch,  90 batch] loss: 13.07406, the best RMSE/MAE: 8.87716 / 7.38544
<Test> RMSE：5.30981,MAE：4.53789
2021-01-06 13:06:56.069265 Training: [9 epoch,  10 batch] loss: 12.97443, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:08:23.102223 Training: [9 epoch,  20 batch] loss: 12.95761, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:09:50.134118 Training: [9 epoch,  30 batch] loss: 12.89605, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:11:17.145985 Training: [9 epoch,  40 batch] loss: 12.86182, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:12:44.103694 Training: [9 epoch,  50 batch] loss: 12.75151, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:14:06.764907 Training: [9 epoch,  60 batch] loss: 12.72412, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:15:31.947766 Training: [9 epoch,  70 batch] loss: 12.66663, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:16:58.853380 Training: [9 epoch,  80 batch] loss: 12.65375, the best RMSE/MAE: 5.30981 / 4.53789
2021-01-06 13:18:24.126903 Training: [9 epoch,  90 batch] loss: 12.62257, the best RMSE/MAE: 5.30981 / 4.53789
<Test> RMSE：3.13406,MAE：2.70040
2021-01-06 13:22:58.527387 Training: [10 epoch,  10 batch] loss: 12.48743, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:24:35.498235 Training: [10 epoch,  20 batch] loss: 12.44592, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:26:11.982907 Training: [10 epoch,  30 batch] loss: 12.41978, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:27:48.297595 Training: [10 epoch,  40 batch] loss: 12.34664, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:29:24.995660 Training: [10 epoch,  50 batch] loss: 12.30897, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:30:55.938175 Training: [10 epoch,  60 batch] loss: 12.26729, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:32:31.523890 Training: [10 epoch,  70 batch] loss: 12.28817, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:34:07.307622 Training: [10 epoch,  80 batch] loss: 12.15063, the best RMSE/MAE: 3.13406 / 2.70040
2021-01-06 13:35:42.816826 Training: [10 epoch,  90 batch] loss: 12.11388, the best RMSE/MAE: 3.13406 / 2.70040
<Test> RMSE：2.30488,MAE：1.98059
2021-01-06 13:40:19.776778 Training: [11 epoch,  10 batch] loss: 12.03616, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:41:54.917911 Training: [11 epoch,  20 batch] loss: 11.95807, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:43:32.854455 Training: [11 epoch,  30 batch] loss: 11.90761, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:45:09.386719 Training: [11 epoch,  40 batch] loss: 11.85770, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:46:45.686429 Training: [11 epoch,  50 batch] loss: 11.82227, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:48:17.587353 Training: [11 epoch,  60 batch] loss: 11.76000, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:49:51.804461 Training: [11 epoch,  70 batch] loss: 11.71414, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:51:28.906710 Training: [11 epoch,  80 batch] loss: 11.70252, the best RMSE/MAE: 2.30488 / 1.98059
2021-01-06 13:53:01.363974 Training: [11 epoch,  90 batch] loss: 11.57781, the best RMSE/MAE: 2.30488 / 1.98059
<Test> RMSE：1.46950,MAE：1.25694
2021-01-06 13:57:38.906336 Training: [12 epoch,  10 batch] loss: 11.51984, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 13:59:16.542994 Training: [12 epoch,  20 batch] loss: 11.44618, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:00:52.006036 Training: [12 epoch,  30 batch] loss: 11.40573, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:02:29.315803 Training: [12 epoch,  40 batch] loss: 11.36272, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:04:03.427062 Training: [12 epoch,  50 batch] loss: 11.33512, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:05:32.876672 Training: [12 epoch,  60 batch] loss: 11.23454, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:07:07.338124 Training: [12 epoch,  70 batch] loss: 11.20536, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:08:42.745932 Training: [12 epoch,  80 batch] loss: 11.16168, the best RMSE/MAE: 1.46950 / 1.25694
2021-01-06 14:10:18.426723 Training: [12 epoch,  90 batch] loss: 11.09730, the best RMSE/MAE: 1.46950 / 1.25694
<Test> RMSE：0.93846,MAE：0.78264
2021-01-06 14:14:54.007804 Training: [13 epoch,  10 batch] loss: 11.00330, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:16:30.056545 Training: [13 epoch,  20 batch] loss: 10.96240, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:18:05.591010 Training: [13 epoch,  30 batch] loss: 10.90339, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:19:42.023175 Training: [13 epoch,  40 batch] loss: 10.82015, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:21:16.210886 Training: [13 epoch,  50 batch] loss: 10.80403, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:22:48.745709 Training: [13 epoch,  60 batch] loss: 10.73835, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:24:26.009631 Training: [13 epoch,  70 batch] loss: 10.75489, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:26:01.514178 Training: [13 epoch,  80 batch] loss: 10.62802, the best RMSE/MAE: 0.93846 / 0.78264
2021-01-06 14:27:36.916835 Training: [13 epoch,  90 batch] loss: 10.58721, the best RMSE/MAE: 0.93846 / 0.78264
<Test> RMSE：0.75512,MAE：0.62193
2021-01-06 14:32:14.057311 Training: [14 epoch,  10 batch] loss: 10.49965, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:33:50.703886 Training: [14 epoch,  20 batch] loss: 10.42896, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:35:27.370058 Training: [14 epoch,  30 batch] loss: 10.37651, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:37:03.884805 Training: [14 epoch,  40 batch] loss: 10.29789, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:38:41.844454 Training: [14 epoch,  50 batch] loss: 10.29020, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:40:12.399997 Training: [14 epoch,  60 batch] loss: 10.21049, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:41:49.579825 Training: [14 epoch,  70 batch] loss: 10.20410, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:43:26.757474 Training: [14 epoch,  80 batch] loss: 10.14133, the best RMSE/MAE: 0.75512 / 0.62193
2021-01-06 14:45:02.813678 Training: [14 epoch,  90 batch] loss: 10.05309, the best RMSE/MAE: 0.75512 / 0.62193
<Test> RMSE：0.66166,MAE：0.53101
2021-01-06 14:49:42.030470 Training: [15 epoch,  10 batch] loss: 10.00303, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:51:17.129210 Training: [15 epoch,  20 batch] loss: 9.92592, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:52:54.566032 Training: [15 epoch,  30 batch] loss: 9.84880, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:54:31.766269 Training: [15 epoch,  40 batch] loss: 9.77672, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:56:08.044929 Training: [15 epoch,  50 batch] loss: 9.73036, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:57:39.662608 Training: [15 epoch,  60 batch] loss: 9.70351, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 14:59:13.766346 Training: [15 epoch,  70 batch] loss: 9.62813, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 15:00:50.872017 Training: [15 epoch,  80 batch] loss: 9.62419, the best RMSE/MAE: 0.66166 / 0.53101
2021-01-06 15:02:24.615201 Training: [15 epoch,  90 batch] loss: 9.57574, the best RMSE/MAE: 0.66166 / 0.53101
<Test> RMSE：0.55278,MAE：0.42054
2021-01-06 15:07:04.906990 Training: [16 epoch,  10 batch] loss: 9.46676, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:08:42.406263 Training: [16 epoch,  20 batch] loss: 9.40780, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:10:19.546743 Training: [16 epoch,  30 batch] loss: 9.35812, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:11:56.583359 Training: [16 epoch,  40 batch] loss: 9.31454, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:13:32.341882 Training: [16 epoch,  50 batch] loss: 9.26052, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:14:54.698721 Training: [16 epoch,  60 batch] loss: 9.19366, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:16:20.019228 Training: [16 epoch,  70 batch] loss: 9.11942, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:17:46.355101 Training: [16 epoch,  80 batch] loss: 9.10204, the best RMSE/MAE: 0.55278 / 0.42054
2021-01-06 15:19:11.649175 Training: [16 epoch,  90 batch] loss: 9.03567, the best RMSE/MAE: 0.55278 / 0.42054
<Test> RMSE：0.45106,MAE：0.30230
2021-01-06 15:23:21.099093 Training: [17 epoch,  10 batch] loss: 8.94127, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:24:47.689435 Training: [17 epoch,  20 batch] loss: 8.89260, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:26:14.369857 Training: [17 epoch,  30 batch] loss: 8.83861, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:27:41.127888 Training: [17 epoch,  40 batch] loss: 8.78179, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:29:07.396274 Training: [17 epoch,  50 batch] loss: 8.73634, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:30:30.592828 Training: [17 epoch,  60 batch] loss: 8.70437, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:31:57.721825 Training: [17 epoch,  70 batch] loss: 8.69026, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:33:23.513899 Training: [17 epoch,  80 batch] loss: 8.60071, the best RMSE/MAE: 0.45106 / 0.30230
2021-01-06 15:34:47.983040 Training: [17 epoch,  90 batch] loss: 8.54651, the best RMSE/MAE: 0.45106 / 0.30230
<Test> RMSE：0.41254,MAE：0.23685
2021-01-06 15:38:56.690747 Training: [18 epoch,  10 batch] loss: 8.44128, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:40:22.623303 Training: [18 epoch,  20 batch] loss: 8.41398, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:41:48.719092 Training: [18 epoch,  30 batch] loss: 8.38074, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:43:14.899597 Training: [18 epoch,  40 batch] loss: 8.34857, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:44:40.362447 Training: [18 epoch,  50 batch] loss: 8.25119, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:46:03.988654 Training: [18 epoch,  60 batch] loss: 8.22463, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:47:42.579634 Training: [18 epoch,  70 batch] loss: 8.14659, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:49:18.370707 Training: [18 epoch,  80 batch] loss: 8.11535, the best RMSE/MAE: 0.41254 / 0.23685
2021-01-06 15:50:52.557531 Training: [18 epoch,  90 batch] loss: 8.06271, the best RMSE/MAE: 0.41254 / 0.23685
<Test> RMSE：0.41218,MAE：0.23894
2021-01-06 15:55:32.152607 Training: [19 epoch,  10 batch] loss: 8.00496, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 15:57:07.282846 Training: [19 epoch,  20 batch] loss: 7.94327, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 15:58:45.975938 Training: [19 epoch,  30 batch] loss: 7.86645, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:00:24.356589 Training: [19 epoch,  40 batch] loss: 7.85421, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:02:01.028281 Training: [19 epoch,  50 batch] loss: 7.75989, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:03:52.095531 Training: [19 epoch,  60 batch] loss: 7.75243, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:05:27.819744 Training: [19 epoch,  70 batch] loss: 7.71523, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:07:05.990812 Training: [19 epoch,  80 batch] loss: 7.65143, the best RMSE/MAE: 0.41218 / 0.23894
2021-01-06 16:08:40.104709 Training: [19 epoch,  90 batch] loss: 7.58691, the best RMSE/MAE: 0.41218 / 0.23894
<Test> RMSE：0.39190,MAE：0.18114
2021-01-06 16:13:21.955053 Training: [20 epoch,  10 batch] loss: 7.51704, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:14:59.984416 Training: [20 epoch,  20 batch] loss: 7.49873, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:16:36.923708 Training: [20 epoch,  30 batch] loss: 7.41968, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:18:15.027764 Training: [20 epoch,  40 batch] loss: 7.38384, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:19:51.060576 Training: [20 epoch,  50 batch] loss: 7.34496, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:21:27.578663 Training: [20 epoch,  60 batch] loss: 7.35824, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:23:05.435775 Training: [20 epoch,  70 batch] loss: 7.24472, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:24:44.367585 Training: [20 epoch,  80 batch] loss: 7.20076, the best RMSE/MAE: 0.39190 / 0.18114
2021-01-06 16:26:19.606092 Training: [20 epoch,  90 batch] loss: 7.13827, the best RMSE/MAE: 0.39190 / 0.18114
<Test> RMSE：0.38975,MAE：0.15741
2021-01-06 16:31:04.196524 Training: [21 epoch,  10 batch] loss: 7.07728, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:32:43.919646 Training: [21 epoch,  20 batch] loss: 7.02013, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:34:22.932171 Training: [21 epoch,  30 batch] loss: 7.01521, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:36:02.355750 Training: [21 epoch,  40 batch] loss: 6.96854, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:37:35.767622 Training: [21 epoch,  50 batch] loss: 6.89651, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:39:09.169478 Training: [21 epoch,  60 batch] loss: 6.84012, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:40:46.312787 Training: [21 epoch,  70 batch] loss: 6.83916, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:42:23.556715 Training: [21 epoch,  80 batch] loss: 6.79716, the best RMSE/MAE: 0.38975 / 0.15741
2021-01-06 16:43:58.115337 Training: [21 epoch,  90 batch] loss: 6.75046, the best RMSE/MAE: 0.38975 / 0.15741
<Test> RMSE：0.38455,MAE：0.16026
2021-01-06 16:48:41.986234 Training: [22 epoch,  10 batch] loss: 6.66500, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:50:19.991063 Training: [22 epoch,  20 batch] loss: 6.63265, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:51:56.831557 Training: [22 epoch,  30 batch] loss: 6.62159, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:53:34.541772 Training: [22 epoch,  40 batch] loss: 6.52478, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:55:09.034171 Training: [22 epoch,  50 batch] loss: 6.49953, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:56:41.340920 Training: [22 epoch,  60 batch] loss: 6.46700, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:58:19.042866 Training: [22 epoch,  70 batch] loss: 6.42641, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 16:59:57.214494 Training: [22 epoch,  80 batch] loss: 6.37613, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:01:34.446949 Training: [22 epoch,  90 batch] loss: 6.34892, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38507,MAE：0.15062
2021-01-06 17:06:15.681110 Training: [23 epoch,  10 batch] loss: 6.27725, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:07:54.049896 Training: [23 epoch,  20 batch] loss: 6.24586, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:09:31.451161 Training: [23 epoch,  30 batch] loss: 6.17417, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:11:09.721044 Training: [23 epoch,  40 batch] loss: 6.14538, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:12:45.271021 Training: [23 epoch,  50 batch] loss: 6.12385, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:14:18.425171 Training: [23 epoch,  60 batch] loss: 6.04345, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:15:56.660678 Training: [23 epoch,  70 batch] loss: 6.08254, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:17:32.810754 Training: [23 epoch,  80 batch] loss: 6.02803, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:19:08.777737 Training: [23 epoch,  90 batch] loss: 5.96747, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38609,MAE：0.13961
2021-01-06 17:23:53.606224 Training: [24 epoch,  10 batch] loss: 5.90315, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:25:30.510978 Training: [24 epoch,  20 batch] loss: 5.89452, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:27:08.697522 Training: [24 epoch,  30 batch] loss: 5.78920, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:28:45.605473 Training: [24 epoch,  40 batch] loss: 5.78656, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:30:21.209771 Training: [24 epoch,  50 batch] loss: 5.75701, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:31:53.671886 Training: [24 epoch,  60 batch] loss: 5.76014, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:33:31.951941 Training: [24 epoch,  70 batch] loss: 5.68844, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:35:10.997382 Training: [24 epoch,  80 batch] loss: 5.63070, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:36:38.766123 Training: [24 epoch,  90 batch] loss: 5.61327, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38911,MAE：0.13883
2021-01-06 17:40:50.059821 Training: [25 epoch,  10 batch] loss: 5.55500, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:42:16.307019 Training: [25 epoch,  20 batch] loss: 5.50665, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:43:42.681563 Training: [25 epoch,  30 batch] loss: 5.45491, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:45:09.054837 Training: [25 epoch,  40 batch] loss: 5.45473, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:46:32.518718 Training: [25 epoch,  50 batch] loss: 5.42356, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:47:55.135220 Training: [25 epoch,  60 batch] loss: 5.40474, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:49:21.894323 Training: [25 epoch,  70 batch] loss: 5.33572, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:50:48.769962 Training: [25 epoch,  80 batch] loss: 5.27494, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:52:12.490682 Training: [25 epoch,  90 batch] loss: 5.29316, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38677,MAE：0.13698
2021-01-06 17:56:21.051606 Training: [26 epoch,  10 batch] loss: 5.20804, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:57:47.033762 Training: [26 epoch,  20 batch] loss: 5.16947, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 17:59:13.227174 Training: [26 epoch,  30 batch] loss: 5.17141, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:00:39.635272 Training: [26 epoch,  40 batch] loss: 5.15556, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:02:03.078978 Training: [26 epoch,  50 batch] loss: 5.10473, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:03:25.197399 Training: [26 epoch,  60 batch] loss: 5.06916, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:04:51.835382 Training: [26 epoch,  70 batch] loss: 5.03988, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:06:18.190158 Training: [26 epoch,  80 batch] loss: 4.96996, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:07:42.157130 Training: [26 epoch,  90 batch] loss: 4.95302, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38681,MAE：0.14751
2021-01-06 18:11:50.328645 Training: [27 epoch,  10 batch] loss: 4.93048, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:13:19.316491 Training: [27 epoch,  20 batch] loss: 4.91395, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:14:51.268185 Training: [27 epoch,  30 batch] loss: 4.84269, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:16:17.834070 Training: [27 epoch,  40 batch] loss: 4.81071, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:17:42.121981 Training: [27 epoch,  50 batch] loss: 4.81942, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:19:04.844749 Training: [27 epoch,  60 batch] loss: 4.77781, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:20:31.795603 Training: [27 epoch,  70 batch] loss: 4.75108, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:21:58.982162 Training: [27 epoch,  80 batch] loss: 4.68576, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:23:23.595972 Training: [27 epoch,  90 batch] loss: 4.65267, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.38960,MAE：0.14774
2021-01-06 18:27:32.655872 Training: [28 epoch,  10 batch] loss: 4.61501, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:29:03.529089 Training: [28 epoch,  20 batch] loss: 4.58434, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:30:44.562298 Training: [28 epoch,  30 batch] loss: 4.58443, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:32:25.963897 Training: [28 epoch,  40 batch] loss: 4.57162, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:34:05.254020 Training: [28 epoch,  50 batch] loss: 4.50911, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:35:42.154141 Training: [28 epoch,  60 batch] loss: 4.46884, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:37:22.107651 Training: [28 epoch,  70 batch] loss: 4.44147, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:39:03.543376 Training: [28 epoch,  80 batch] loss: 4.43382, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:40:30.837136 Training: [28 epoch,  90 batch] loss: 4.43556, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.39138,MAE：0.13915
2021-01-06 18:44:40.385850 Training: [29 epoch,  10 batch] loss: 4.38515, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:46:06.586509 Training: [29 epoch,  20 batch] loss: 4.33678, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:47:32.709469 Training: [29 epoch,  30 batch] loss: 4.29328, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:48:59.179346 Training: [29 epoch,  40 batch] loss: 4.27084, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:50:24.196811 Training: [29 epoch,  50 batch] loss: 4.24686, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:51:47.427828 Training: [29 epoch,  60 batch] loss: 4.22532, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:53:24.131067 Training: [29 epoch,  70 batch] loss: 4.18316, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:55:04.062882 Training: [29 epoch,  80 batch] loss: 4.19046, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 18:56:42.702623 Training: [29 epoch,  90 batch] loss: 4.14419, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.39143,MAE：0.14941
2021-01-06 19:01:35.611472 Training: [30 epoch,  10 batch] loss: 4.08816, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:03:15.199284 Training: [30 epoch,  20 batch] loss: 4.08455, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:04:54.550247 Training: [30 epoch,  30 batch] loss: 4.04078, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:06:34.683055 Training: [30 epoch,  40 batch] loss: 4.05358, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:08:13.344316 Training: [30 epoch,  50 batch] loss: 3.99659, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:09:51.916764 Training: [30 epoch,  60 batch] loss: 4.01682, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:11:32.227054 Training: [30 epoch,  70 batch] loss: 3.97242, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:13:13.251623 Training: [30 epoch,  80 batch] loss: 3.94552, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:14:51.571691 Training: [30 epoch,  90 batch] loss: 3.89059, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.39309,MAE：0.14810
2021-01-06 19:19:45.766108 Training: [31 epoch,  10 batch] loss: 3.86207, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:21:25.397925 Training: [31 epoch,  20 batch] loss: 3.84419, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:23:05.546610 Training: [31 epoch,  30 batch] loss: 3.82756, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:24:46.362063 Training: [31 epoch,  40 batch] loss: 3.79527, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:26:25.465155 Training: [31 epoch,  50 batch] loss: 3.80410, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:28:03.555351 Training: [31 epoch,  60 batch] loss: 3.74180, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:29:42.193672 Training: [31 epoch,  70 batch] loss: 3.73464, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:31:22.351871 Training: [31 epoch,  80 batch] loss: 3.70677, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:33:01.672153 Training: [31 epoch,  90 batch] loss: 3.73012, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.39629,MAE：0.17922
2021-01-06 19:37:57.558600 Training: [32 epoch,  10 batch] loss: 3.66513, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:39:37.545654 Training: [32 epoch,  20 batch] loss: 3.65146, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:41:18.566285 Training: [32 epoch,  30 batch] loss: 3.61547, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:42:59.567881 Training: [32 epoch,  40 batch] loss: 3.60014, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:44:39.583897 Training: [32 epoch,  50 batch] loss: 3.55022, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:46:18.046716 Training: [32 epoch,  60 batch] loss: 3.53563, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:47:56.295056 Training: [32 epoch,  70 batch] loss: 3.51618, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:49:36.411695 Training: [32 epoch,  80 batch] loss: 3.49547, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:51:14.013569 Training: [32 epoch,  90 batch] loss: 3.48806, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.40996,MAE：0.23956
2021-01-06 19:56:07.582023 Training: [33 epoch,  10 batch] loss: 3.47044, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:57:48.376278 Training: [33 epoch,  20 batch] loss: 3.44369, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 19:59:29.440434 Training: [33 epoch,  30 batch] loss: 3.41506, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:01:10.218424 Training: [33 epoch,  40 batch] loss: 3.39085, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:02:48.408527 Training: [33 epoch,  50 batch] loss: 3.34899, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:04:26.867506 Training: [33 epoch,  60 batch] loss: 3.33570, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:06:06.365692 Training: [33 epoch,  70 batch] loss: 3.32806, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:07:47.238733 Training: [33 epoch,  80 batch] loss: 3.30980, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:09:25.614298 Training: [33 epoch,  90 batch] loss: 3.30623, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.40419,MAE：0.23377
2021-01-06 20:14:20.564996 Training: [34 epoch,  10 batch] loss: 3.25204, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:16:00.888383 Training: [34 epoch,  20 batch] loss: 3.27544, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:17:41.066575 Training: [34 epoch,  30 batch] loss: 3.22899, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:19:20.510350 Training: [34 epoch,  40 batch] loss: 3.18123, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:20:59.872051 Training: [34 epoch,  50 batch] loss: 3.19331, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:22:37.502017 Training: [34 epoch,  60 batch] loss: 3.16187, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:24:16.787334 Training: [34 epoch,  70 batch] loss: 3.15680, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:25:56.895427 Training: [34 epoch,  80 batch] loss: 3.11760, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:27:34.778420 Training: [34 epoch,  90 batch] loss: 3.12584, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.40572,MAE：0.25057
2021-01-06 20:32:30.434309 Training: [35 epoch,  10 batch] loss: 3.08884, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:34:08.696391 Training: [35 epoch,  20 batch] loss: 3.02041, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:35:48.263172 Training: [35 epoch,  30 batch] loss: 3.05174, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:37:28.075322 Training: [35 epoch,  40 batch] loss: 3.05970, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:39:07.304336 Training: [35 epoch,  50 batch] loss: 3.00023, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:40:45.159001 Training: [35 epoch,  60 batch] loss: 3.05312, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:42:22.083116 Training: [35 epoch,  70 batch] loss: 2.95600, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:44:02.563403 Training: [35 epoch,  80 batch] loss: 2.95759, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:45:41.762830 Training: [35 epoch,  90 batch] loss: 2.94049, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.40296,MAE：0.24518
2021-01-06 20:50:36.249809 Training: [36 epoch,  10 batch] loss: 2.92739, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:52:16.306844 Training: [36 epoch,  20 batch] loss: 2.93475, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:53:56.813052 Training: [36 epoch,  30 batch] loss: 2.87181, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:55:38.048960 Training: [36 epoch,  40 batch] loss: 2.86734, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:57:19.163133 Training: [36 epoch,  50 batch] loss: 2.84507, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 20:58:58.031403 Training: [36 epoch,  60 batch] loss: 2.81614, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:00:37.076236 Training: [36 epoch,  70 batch] loss: 2.81308, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:02:18.204716 Training: [36 epoch,  80 batch] loss: 2.78829, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:03:57.790391 Training: [36 epoch,  90 batch] loss: 2.81794, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.39787,MAE：0.22833
2021-01-06 21:09:56.425608 Training: [37 epoch,  10 batch] loss: 2.75333, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:12:04.655233 Training: [37 epoch,  20 batch] loss: 2.75985, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:14:13.471138 Training: [37 epoch,  30 batch] loss: 2.70612, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:16:18.164371 Training: [37 epoch,  40 batch] loss: 2.70449, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:18:26.376857 Training: [37 epoch,  50 batch] loss: 2.70220, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:20:32.133563 Training: [37 epoch,  60 batch] loss: 2.68665, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:22:37.938573 Training: [37 epoch,  70 batch] loss: 2.70680, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:24:46.061749 Training: [37 epoch,  80 batch] loss: 2.65261, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:26:49.843999 Training: [37 epoch,  90 batch] loss: 2.62641, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.43783,MAE：0.32941
2021-01-06 21:33:19.049973 Training: [38 epoch,  10 batch] loss: 2.61863, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:35:24.984551 Training: [38 epoch,  20 batch] loss: 2.59820, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:37:33.934464 Training: [38 epoch,  30 batch] loss: 2.62121, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:39:42.523476 Training: [38 epoch,  40 batch] loss: 2.57767, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:41:51.356663 Training: [38 epoch,  50 batch] loss: 2.56527, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:43:57.269222 Training: [38 epoch,  60 batch] loss: 2.52898, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:45:57.704557 Training: [38 epoch,  70 batch] loss: 2.54330, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:48:05.583702 Training: [38 epoch,  80 batch] loss: 2.49875, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:50:10.406219 Training: [38 epoch,  90 batch] loss: 2.51874, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.41278,MAE：0.27561
2021-01-06 21:56:34.035947 Training: [39 epoch,  10 batch] loss: 2.46617, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 21:58:42.234758 Training: [39 epoch,  20 batch] loss: 2.46033, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:00:50.550541 Training: [39 epoch,  30 batch] loss: 2.46136, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:02:59.054183 Training: [39 epoch,  40 batch] loss: 2.43487, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:05:03.769589 Training: [39 epoch,  50 batch] loss: 2.41426, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:07:10.086057 Training: [39 epoch,  60 batch] loss: 2.41652, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:09:14.681291 Training: [39 epoch,  70 batch] loss: 2.39612, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:11:20.690732 Training: [39 epoch,  80 batch] loss: 2.40685, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:13:25.908376 Training: [39 epoch,  90 batch] loss: 2.40098, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.42568,MAE：0.30553
2021-01-06 22:19:50.427551 Training: [40 epoch,  10 batch] loss: 2.35409, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:21:58.776291 Training: [40 epoch,  20 batch] loss: 2.34010, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:24:04.516898 Training: [40 epoch,  30 batch] loss: 2.33578, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:26:11.883488 Training: [40 epoch,  40 batch] loss: 2.29564, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:28:20.160636 Training: [40 epoch,  50 batch] loss: 2.29971, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:30:26.672176 Training: [40 epoch,  60 batch] loss: 2.29931, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:32:32.380892 Training: [40 epoch,  70 batch] loss: 2.27057, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:34:34.102269 Training: [40 epoch,  80 batch] loss: 2.25161, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:36:38.022484 Training: [40 epoch,  90 batch] loss: 2.30230, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.41466,MAE：0.28088
2021-01-06 22:43:06.063766 Training: [41 epoch,  10 batch] loss: 2.23821, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:45:11.746543 Training: [41 epoch,  20 batch] loss: 2.20713, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:47:19.703236 Training: [41 epoch,  30 batch] loss: 2.23254, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:49:28.205171 Training: [41 epoch,  40 batch] loss: 2.18614, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:51:36.824323 Training: [41 epoch,  50 batch] loss: 2.19150, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:53:40.903570 Training: [41 epoch,  60 batch] loss: 2.17476, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:55:46.359024 Training: [41 epoch,  70 batch] loss: 2.20130, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:57:52.227575 Training: [41 epoch,  80 batch] loss: 2.14084, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 22:59:55.927229 Training: [41 epoch,  90 batch] loss: 2.14374, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.40482,MAE：0.25434
2021-01-06 23:06:20.286758 Training: [42 epoch,  10 batch] loss: 2.13400, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:08:28.796366 Training: [42 epoch,  20 batch] loss: 2.11336, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:10:37.592777 Training: [42 epoch,  30 batch] loss: 2.09172, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:12:44.955176 Training: [42 epoch,  40 batch] loss: 2.08944, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:14:51.103326 Training: [42 epoch,  50 batch] loss: 2.08301, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:16:57.563948 Training: [42 epoch,  60 batch] loss: 2.07801, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:19:02.862389 Training: [42 epoch,  70 batch] loss: 2.04581, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:21:08.190717 Training: [42 epoch,  80 batch] loss: 2.06775, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:23:10.129578 Training: [42 epoch,  90 batch] loss: 2.04284, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.41655,MAE：0.28548
2021-01-06 23:29:38.647085 Training: [43 epoch,  10 batch] loss: 2.03606, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:31:47.800439 Training: [43 epoch,  20 batch] loss: 1.99268, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:33:52.979777 Training: [43 epoch,  30 batch] loss: 2.01861, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:36:01.230520 Training: [43 epoch,  40 batch] loss: 2.01398, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:38:09.188278 Training: [43 epoch,  50 batch] loss: 1.96656, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:40:16.036159 Training: [43 epoch,  60 batch] loss: 1.95023, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:42:19.589789 Training: [43 epoch,  70 batch] loss: 1.96068, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:44:22.773116 Training: [43 epoch,  80 batch] loss: 1.95957, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:46:26.056766 Training: [43 epoch,  90 batch] loss: 1.92563, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.49981,MAE：0.42691
2021-01-06 23:52:08.837809 Training: [44 epoch,  10 batch] loss: 1.93727, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:54:01.691136 Training: [44 epoch,  20 batch] loss: 1.88812, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:55:55.763790 Training: [44 epoch,  30 batch] loss: 1.92583, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:57:49.395100 Training: [44 epoch,  40 batch] loss: 1.90135, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-06 23:59:43.017842 Training: [44 epoch,  50 batch] loss: 1.86575, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:01:33.254752 Training: [44 epoch,  60 batch] loss: 1.89031, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:03:24.882510 Training: [44 epoch,  70 batch] loss: 1.88620, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:05:16.285471 Training: [44 epoch,  80 batch] loss: 1.85267, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:07:08.252557 Training: [44 epoch,  90 batch] loss: 1.83092, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.45211,MAE：0.35502
2021-01-07 00:12:46.333921 Training: [45 epoch,  10 batch] loss: 1.82894, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:14:38.694935 Training: [45 epoch,  20 batch] loss: 1.81180, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:16:32.091471 Training: [45 epoch,  30 batch] loss: 1.80150, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:18:22.992467 Training: [45 epoch,  40 batch] loss: 1.84442, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:20:16.197400 Training: [45 epoch,  50 batch] loss: 1.80544, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:22:09.041967 Training: [45 epoch,  60 batch] loss: 1.79800, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:23:59.971302 Training: [45 epoch,  70 batch] loss: 1.77644, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:25:50.895516 Training: [45 epoch,  80 batch] loss: 1.75265, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:27:38.845875 Training: [45 epoch,  90 batch] loss: 1.75910, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.42657,MAE：0.30760
2021-01-07 00:33:19.758137 Training: [46 epoch,  10 batch] loss: 1.73056, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:35:11.227616 Training: [46 epoch,  20 batch] loss: 1.72922, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:37:02.938019 Training: [46 epoch,  30 batch] loss: 1.72385, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:38:56.711818 Training: [46 epoch,  40 batch] loss: 1.71522, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:40:50.864206 Training: [46 epoch,  50 batch] loss: 1.70530, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:42:43.973093 Training: [46 epoch,  60 batch] loss: 1.68929, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:44:34.113415 Training: [46 epoch,  70 batch] loss: 1.71545, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:46:26.431620 Training: [46 epoch,  80 batch] loss: 1.67157, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:48:20.428092 Training: [46 epoch,  90 batch] loss: 1.67459, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.46721,MAE：0.37940
2021-01-07 00:54:00.602280 Training: [47 epoch,  10 batch] loss: 1.65316, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:55:55.624937 Training: [47 epoch,  20 batch] loss: 1.65443, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:57:51.254619 Training: [47 epoch,  30 batch] loss: 1.62821, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 00:59:46.342703 Training: [47 epoch,  40 batch] loss: 1.61745, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:01:39.621194 Training: [47 epoch,  50 batch] loss: 1.62815, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:03:30.313420 Training: [47 epoch,  60 batch] loss: 1.65630, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:05:21.781508 Training: [47 epoch,  70 batch] loss: 1.60187, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:07:14.492363 Training: [47 epoch,  80 batch] loss: 1.62376, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:09:08.743734 Training: [47 epoch,  90 batch] loss: 1.60457, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.52621,MAE：0.46206
2021-01-07 01:14:47.439885 Training: [48 epoch,  10 batch] loss: 1.61194, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:16:40.767923 Training: [48 epoch,  20 batch] loss: 1.56194, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:18:35.589941 Training: [48 epoch,  30 batch] loss: 1.57310, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:20:27.042892 Training: [48 epoch,  40 batch] loss: 1.56158, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:22:20.934009 Training: [48 epoch,  50 batch] loss: 1.54154, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:24:13.741988 Training: [48 epoch,  60 batch] loss: 1.56814, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:26:05.290445 Training: [48 epoch,  70 batch] loss: 1.51444, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:27:55.532550 Training: [48 epoch,  80 batch] loss: 1.55712, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:29:45.223189 Training: [48 epoch,  90 batch] loss: 1.52376, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.49308,MAE：0.41762
2021-01-07 01:35:24.905155 Training: [49 epoch,  10 batch] loss: 1.49854, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:37:15.482654 Training: [49 epoch,  20 batch] loss: 1.49282, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:39:07.278069 Training: [49 epoch,  30 batch] loss: 1.51178, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:41:01.108552 Training: [49 epoch,  40 batch] loss: 1.51444, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:42:54.798263 Training: [49 epoch,  50 batch] loss: 1.48593, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:44:46.790620 Training: [49 epoch,  60 batch] loss: 1.46175, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:46:35.692053 Training: [49 epoch,  70 batch] loss: 1.47772, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:48:26.496085 Training: [49 epoch,  80 batch] loss: 1.45353, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:50:19.070533 Training: [49 epoch,  90 batch] loss: 1.49401, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.47231,MAE：0.38736
2021-01-07 01:55:59.368615 Training: [50 epoch,  10 batch] loss: 1.43655, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:57:54.204207 Training: [50 epoch,  20 batch] loss: 1.44422, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 01:59:54.177490 Training: [50 epoch,  30 batch] loss: 1.43237, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:01:49.421462 Training: [50 epoch,  40 batch] loss: 1.41339, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:03:44.727118 Training: [50 epoch,  50 batch] loss: 1.44789, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:05:36.959546 Training: [50 epoch,  60 batch] loss: 1.40469, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:07:29.550618 Training: [50 epoch,  70 batch] loss: 1.38955, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:09:20.666888 Training: [50 epoch,  80 batch] loss: 1.38682, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:11:12.439545 Training: [50 epoch,  90 batch] loss: 1.39003, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.51509,MAE：0.44768
2021-01-07 02:16:51.980540 Training: [51 epoch,  10 batch] loss: 1.38239, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:18:50.410198 Training: [51 epoch,  20 batch] loss: 1.37415, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:20:46.552822 Training: [51 epoch,  30 batch] loss: 1.35011, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:22:43.554558 Training: [51 epoch,  40 batch] loss: 1.37054, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:24:37.998023 Training: [51 epoch,  50 batch] loss: 1.33021, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:26:30.038816 Training: [51 epoch,  60 batch] loss: 1.33786, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:28:22.799784 Training: [51 epoch,  70 batch] loss: 1.33199, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:30:11.532966 Training: [51 epoch,  80 batch] loss: 1.34577, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:32:03.459384 Training: [51 epoch,  90 batch] loss: 1.41268, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.44444,MAE：0.34181
2021-01-07 02:37:42.633897 Training: [52 epoch,  10 batch] loss: 1.30817, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:39:34.555886 Training: [52 epoch,  20 batch] loss: 1.32126, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:41:26.318435 Training: [52 epoch,  30 batch] loss: 1.28917, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:43:21.985306 Training: [52 epoch,  40 batch] loss: 1.28060, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:45:22.719426 Training: [52 epoch,  50 batch] loss: 1.29227, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:47:18.208882 Training: [52 epoch,  60 batch] loss: 1.34898, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:49:13.579309 Training: [52 epoch,  70 batch] loss: 1.27846, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:51:03.846262 Training: [52 epoch,  80 batch] loss: 1.28459, the best RMSE/MAE: 0.38455 / 0.16026
2021-01-07 02:52:57.740469 Training: [52 epoch,  90 batch] loss: 1.28034, the best RMSE/MAE: 0.38455 / 0.16026
<Test> RMSE：0.49456,MAE：0.41977
The best RMSE/MAE：0.38455/0.16026
