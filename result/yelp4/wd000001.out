-------------------- Hyperparams --------------------
time: 2021-01-10 10:12:15.650235
Dataset: yelp
N: 30000
weight decay: 1e-05
dropout rate: 0.5
learning rate: 0.0005
dimension of embedding: 32
use_cuda: True
2021-01-10 10:22:16.298922 Training: [1 epoch,  10 batch] loss: 0.80719, the best RMSE/MAE: inf / inf
2021-01-10 10:23:15.244817 Training: [1 epoch,  20 batch] loss: 0.76924, the best RMSE/MAE: inf / inf
2021-01-10 10:24:17.728254 Training: [1 epoch,  30 batch] loss: 0.71197, the best RMSE/MAE: inf / inf
2021-01-10 10:25:27.180156 Training: [1 epoch,  40 batch] loss: 0.57618, the best RMSE/MAE: inf / inf
2021-01-10 10:26:34.959124 Training: [1 epoch,  50 batch] loss: 0.47318, the best RMSE/MAE: inf / inf
2021-01-10 10:27:42.997137 Training: [1 epoch,  60 batch] loss: 0.46322, the best RMSE/MAE: inf / inf
2021-01-10 10:28:51.920683 Training: [1 epoch,  70 batch] loss: 0.48496, the best RMSE/MAE: inf / inf
2021-01-10 10:29:59.343227 Training: [1 epoch,  80 batch] loss: 0.40910, the best RMSE/MAE: inf / inf
2021-01-10 10:31:06.164567 Training: [1 epoch,  90 batch] loss: 0.33304, the best RMSE/MAE: inf / inf
<Test> RMSE：159628160.00000,MAE：124163424.00000
2021-01-10 10:34:13.975679 Training: [2 epoch,  10 batch] loss: 0.34966, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:35:19.867037 Training: [2 epoch,  20 batch] loss: 0.34206, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:36:25.944745 Training: [2 epoch,  30 batch] loss: 0.39630, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:37:34.304462 Training: [2 epoch,  40 batch] loss: 0.35017, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:38:44.845519 Training: [2 epoch,  50 batch] loss: 0.27599, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:39:53.055397 Training: [2 epoch,  60 batch] loss: 0.30831, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:41:00.792186 Training: [2 epoch,  70 batch] loss: 0.26893, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:42:07.940000 Training: [2 epoch,  80 batch] loss: 0.31804, the best RMSE/MAE: 159628160.00000 / 124163424.00000
2021-01-10 10:43:15.337259 Training: [2 epoch,  90 batch] loss: 0.37922, the best RMSE/MAE: 159628160.00000 / 124163424.00000
<Test> RMSE：250072.85938,MAE：195506.34375
2021-01-10 10:46:23.454481 Training: [3 epoch,  10 batch] loss: 0.27185, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:47:29.229143 Training: [3 epoch,  20 batch] loss: 0.25582, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:48:33.990617 Training: [3 epoch,  30 batch] loss: 0.23440, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:49:40.787121 Training: [3 epoch,  40 batch] loss: 0.27476, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:50:47.567328 Training: [3 epoch,  50 batch] loss: 0.27831, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:51:54.172826 Training: [3 epoch,  60 batch] loss: 0.35549, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:53:01.925263 Training: [3 epoch,  70 batch] loss: 0.23957, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:54:09.278918 Training: [3 epoch,  80 batch] loss: 0.22391, the best RMSE/MAE: 250072.85938 / 195506.34375
2021-01-10 10:55:16.605845 Training: [3 epoch,  90 batch] loss: 0.25157, the best RMSE/MAE: 250072.85938 / 195506.34375
<Test> RMSE：5586.46338,MAE：4154.00635
2021-01-10 10:58:27.728651 Training: [4 epoch,  10 batch] loss: 0.21743, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 10:59:35.721538 Training: [4 epoch,  20 batch] loss: 0.21002, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:00:42.517650 Training: [4 epoch,  30 batch] loss: 0.20816, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:01:50.358658 Training: [4 epoch,  40 batch] loss: 0.32851, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:02:59.001130 Training: [4 epoch,  50 batch] loss: 0.19822, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:04:09.601355 Training: [4 epoch,  60 batch] loss: 0.30250, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:05:19.039209 Training: [4 epoch,  70 batch] loss: 0.22194, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:06:26.572930 Training: [4 epoch,  80 batch] loss: 0.19561, the best RMSE/MAE: 5586.46338 / 4154.00635
2021-01-10 11:07:33.995026 Training: [4 epoch,  90 batch] loss: 0.21939, the best RMSE/MAE: 5586.46338 / 4154.00635
<Test> RMSE：380.09866,MAE：277.66528
2021-01-10 11:10:44.743915 Training: [5 epoch,  10 batch] loss: 0.25135, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:11:50.847660 Training: [5 epoch,  20 batch] loss: 0.28824, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:12:55.858716 Training: [5 epoch,  30 batch] loss: 0.19630, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:14:02.563917 Training: [5 epoch,  40 batch] loss: 0.23448, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:15:09.832343 Training: [5 epoch,  50 batch] loss: 0.16221, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:16:17.040595 Training: [5 epoch,  60 batch] loss: 0.18237, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:17:24.702331 Training: [5 epoch,  70 batch] loss: 0.22149, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:18:32.370925 Training: [5 epoch,  80 batch] loss: 0.19367, the best RMSE/MAE: 380.09866 / 277.66528
2021-01-10 11:19:40.127868 Training: [5 epoch,  90 batch] loss: 0.18648, the best RMSE/MAE: 380.09866 / 277.66528
<Test> RMSE：70.61855,MAE：51.48721
2021-01-10 11:22:50.698599 Training: [6 epoch,  10 batch] loss: 0.30638, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:23:58.766030 Training: [6 epoch,  20 batch] loss: 0.18209, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:25:07.176047 Training: [6 epoch,  30 batch] loss: 0.19872, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:26:15.846133 Training: [6 epoch,  40 batch] loss: 0.17523, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:27:29.411599 Training: [6 epoch,  50 batch] loss: 0.18483, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:28:39.912895 Training: [6 epoch,  60 batch] loss: 0.16540, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:29:48.970649 Training: [6 epoch,  70 batch] loss: 0.20549, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:30:56.826355 Training: [6 epoch,  80 batch] loss: 0.18409, the best RMSE/MAE: 70.61855 / 51.48721
2021-01-10 11:32:04.100615 Training: [6 epoch,  90 batch] loss: 0.23750, the best RMSE/MAE: 70.61855 / 51.48721
<Test> RMSE：30.42412,MAE：21.23200
2021-01-10 11:35:17.591923 Training: [7 epoch,  10 batch] loss: 0.20140, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:36:26.689546 Training: [7 epoch,  20 batch] loss: 0.23840, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:37:32.784089 Training: [7 epoch,  30 batch] loss: 0.16072, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:38:41.671497 Training: [7 epoch,  40 batch] loss: 0.22744, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:39:50.740964 Training: [7 epoch,  50 batch] loss: 0.17577, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:40:58.842240 Training: [7 epoch,  60 batch] loss: 0.20272, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:42:06.825384 Training: [7 epoch,  70 batch] loss: 0.17653, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:43:13.437016 Training: [7 epoch,  80 batch] loss: 0.19439, the best RMSE/MAE: 30.42412 / 21.23200
2021-01-10 11:44:20.387913 Training: [7 epoch,  90 batch] loss: 0.18450, the best RMSE/MAE: 30.42412 / 21.23200
<Test> RMSE：14.41205,MAE：9.90313
2021-01-10 11:47:30.109683 Training: [8 epoch,  10 batch] loss: 0.17925, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:48:36.138078 Training: [8 epoch,  20 batch] loss: 0.18754, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:49:40.736578 Training: [8 epoch,  30 batch] loss: 0.21047, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:50:45.888844 Training: [8 epoch,  40 batch] loss: 0.15511, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:51:53.653300 Training: [8 epoch,  50 batch] loss: 0.16277, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:53:01.457927 Training: [8 epoch,  60 batch] loss: 0.15355, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:54:08.472689 Training: [8 epoch,  70 batch] loss: 0.18768, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:55:15.608338 Training: [8 epoch,  80 batch] loss: 0.14406, the best RMSE/MAE: 14.41205 / 9.90313
2021-01-10 11:56:23.375427 Training: [8 epoch,  90 batch] loss: 0.16382, the best RMSE/MAE: 14.41205 / 9.90313
<Test> RMSE：8.83155,MAE：6.37638
2021-01-10 11:59:36.653044 Training: [9 epoch,  10 batch] loss: 0.14269, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:00:45.160241 Training: [9 epoch,  20 batch] loss: 0.16088, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:01:52.520445 Training: [9 epoch,  30 batch] loss: 0.18410, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:02:58.528275 Training: [9 epoch,  40 batch] loss: 0.21124, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:04:06.548796 Training: [9 epoch,  50 batch] loss: 0.23871, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:05:14.540795 Training: [9 epoch,  60 batch] loss: 0.14121, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:06:23.178394 Training: [9 epoch,  70 batch] loss: 0.18566, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:07:31.711930 Training: [9 epoch,  80 batch] loss: 0.16987, the best RMSE/MAE: 8.83155 / 6.37638
2021-01-10 12:08:39.767185 Training: [9 epoch,  90 batch] loss: 0.18266, the best RMSE/MAE: 8.83155 / 6.37638
<Test> RMSE：5.73192,MAE：4.11258
2021-01-10 12:12:00.095408 Training: [10 epoch,  10 batch] loss: 0.16979, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:13:09.984914 Training: [10 epoch,  20 batch] loss: 0.19480, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:14:16.232617 Training: [10 epoch,  30 batch] loss: 0.14222, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:15:22.624636 Training: [10 epoch,  40 batch] loss: 0.19861, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:16:31.247243 Training: [10 epoch,  50 batch] loss: 0.23380, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:17:39.281837 Training: [10 epoch,  60 batch] loss: 0.14810, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:18:46.250003 Training: [10 epoch,  70 batch] loss: 0.17359, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:19:54.900312 Training: [10 epoch,  80 batch] loss: 0.14710, the best RMSE/MAE: 5.73192 / 4.11258
2021-01-10 12:21:02.016194 Training: [10 epoch,  90 batch] loss: 0.12986, the best RMSE/MAE: 5.73192 / 4.11258
<Test> RMSE：2.75711,MAE：2.10111
2021-01-10 12:24:13.493837 Training: [11 epoch,  10 batch] loss: 0.15127, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:25:19.881661 Training: [11 epoch,  20 batch] loss: 0.16172, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:26:27.897220 Training: [11 epoch,  30 batch] loss: 0.15348, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:27:36.137318 Training: [11 epoch,  40 batch] loss: 0.23925, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:28:43.680683 Training: [11 epoch,  50 batch] loss: 0.17180, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:29:50.904633 Training: [11 epoch,  60 batch] loss: 0.16787, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:30:58.365359 Training: [11 epoch,  70 batch] loss: 0.14292, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:32:05.811499 Training: [11 epoch,  80 batch] loss: 0.18067, the best RMSE/MAE: 2.75711 / 2.10111
2021-01-10 12:33:13.458779 Training: [11 epoch,  90 batch] loss: 0.15670, the best RMSE/MAE: 2.75711 / 2.10111
<Test> RMSE：2.15342,MAE：1.60289
2021-01-10 12:36:28.958105 Training: [12 epoch,  10 batch] loss: 0.16227, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:37:41.336319 Training: [12 epoch,  20 batch] loss: 0.13267, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:38:56.273658 Training: [12 epoch,  30 batch] loss: 0.12901, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:40:06.671784 Training: [12 epoch,  40 batch] loss: 0.14122, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:41:16.240418 Training: [12 epoch,  50 batch] loss: 0.12454, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:42:27.812890 Training: [12 epoch,  60 batch] loss: 0.19608, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:43:44.041806 Training: [12 epoch,  70 batch] loss: 0.14718, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:44:59.865335 Training: [12 epoch,  80 batch] loss: 0.21921, the best RMSE/MAE: 2.15342 / 1.60289
2021-01-10 12:46:17.077333 Training: [12 epoch,  90 batch] loss: 0.18502, the best RMSE/MAE: 2.15342 / 1.60289
<Test> RMSE：1.84304,MAE：1.34808
2021-01-10 12:49:47.342170 Training: [13 epoch,  10 batch] loss: 0.12494, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:51:01.874536 Training: [13 epoch,  20 batch] loss: 0.13872, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:52:14.790225 Training: [13 epoch,  30 batch] loss: 0.14216, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:53:26.621697 Training: [13 epoch,  40 batch] loss: 0.20559, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:54:41.582159 Training: [13 epoch,  50 batch] loss: 0.17273, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:55:56.295926 Training: [13 epoch,  60 batch] loss: 0.16775, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:57:11.236064 Training: [13 epoch,  70 batch] loss: 0.14036, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:58:26.403759 Training: [13 epoch,  80 batch] loss: 0.22700, the best RMSE/MAE: 1.84304 / 1.34808
2021-01-10 12:59:44.366513 Training: [13 epoch,  90 batch] loss: 0.12850, the best RMSE/MAE: 1.84304 / 1.34808
<Test> RMSE：1.05960,MAE：0.83200
2021-01-10 13:03:12.680130 Training: [14 epoch,  10 batch] loss: 0.14387, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:04:26.987966 Training: [14 epoch,  20 batch] loss: 0.19575, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:05:42.579383 Training: [14 epoch,  30 batch] loss: 0.11950, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:06:55.201736 Training: [14 epoch,  40 batch] loss: 0.13405, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:08:10.424706 Training: [14 epoch,  50 batch] loss: 0.20187, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:09:23.751734 Training: [14 epoch,  60 batch] loss: 0.16490, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:10:38.558174 Training: [14 epoch,  70 batch] loss: 0.17126, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:11:53.637168 Training: [14 epoch,  80 batch] loss: 0.13897, the best RMSE/MAE: 1.05960 / 0.83200
2021-01-10 13:13:08.192429 Training: [14 epoch,  90 batch] loss: 0.15257, the best RMSE/MAE: 1.05960 / 0.83200
<Test> RMSE：0.80703,MAE：0.67446
2021-01-10 13:16:38.232213 Training: [15 epoch,  10 batch] loss: 0.13769, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:17:53.295067 Training: [15 epoch,  20 batch] loss: 0.12949, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:19:07.758578 Training: [15 epoch,  30 batch] loss: 0.14874, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:20:20.215752 Training: [15 epoch,  40 batch] loss: 0.24908, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:21:36.873341 Training: [15 epoch,  50 batch] loss: 0.13531, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:22:51.367721 Training: [15 epoch,  60 batch] loss: 0.12353, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:24:05.117422 Training: [15 epoch,  70 batch] loss: 0.18960, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:25:21.463857 Training: [15 epoch,  80 batch] loss: 0.13365, the best RMSE/MAE: 0.80703 / 0.67446
2021-01-10 13:26:36.757870 Training: [15 epoch,  90 batch] loss: 0.13162, the best RMSE/MAE: 0.80703 / 0.67446
<Test> RMSE：0.54740,MAE：0.44807
2021-01-10 13:30:04.354467 Training: [16 epoch,  10 batch] loss: 0.13151, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:31:19.291179 Training: [16 epoch,  20 batch] loss: 0.21640, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:32:32.823974 Training: [16 epoch,  30 batch] loss: 0.15132, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:33:47.044549 Training: [16 epoch,  40 batch] loss: 0.11376, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:35:03.868780 Training: [16 epoch,  50 batch] loss: 0.14081, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:36:18.054658 Training: [16 epoch,  60 batch] loss: 0.15061, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:37:33.846635 Training: [16 epoch,  70 batch] loss: 0.19909, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:38:49.047248 Training: [16 epoch,  80 batch] loss: 0.14763, the best RMSE/MAE: 0.54740 / 0.44807
2021-01-10 13:40:04.398704 Training: [16 epoch,  90 batch] loss: 0.12016, the best RMSE/MAE: 0.54740 / 0.44807
<Test> RMSE：0.41831,MAE：0.34491
2021-01-10 13:43:33.012314 Training: [17 epoch,  10 batch] loss: 0.11821, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:44:44.532477 Training: [17 epoch,  20 batch] loss: 0.14339, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:45:51.758815 Training: [17 epoch,  30 batch] loss: 0.12907, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:46:57.849828 Training: [17 epoch,  40 batch] loss: 0.16617, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:48:07.292155 Training: [17 epoch,  50 batch] loss: 0.12998, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:49:21.527877 Training: [17 epoch,  60 batch] loss: 0.17719, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:50:33.362448 Training: [17 epoch,  70 batch] loss: 0.13311, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:51:41.741288 Training: [17 epoch,  80 batch] loss: 0.14747, the best RMSE/MAE: 0.41831 / 0.34491
2021-01-10 13:52:49.351402 Training: [17 epoch,  90 batch] loss: 0.13551, the best RMSE/MAE: 0.41831 / 0.34491
<Test> RMSE：0.37002,MAE：0.22692
2021-01-10 13:56:10.519516 Training: [18 epoch,  10 batch] loss: 0.18097, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 13:57:17.991383 Training: [18 epoch,  20 batch] loss: 0.14779, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 13:58:26.029673 Training: [18 epoch,  30 batch] loss: 0.12493, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 13:59:32.386051 Training: [18 epoch,  40 batch] loss: 0.21515, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:00:40.434033 Training: [18 epoch,  50 batch] loss: 0.12710, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:01:58.901889 Training: [18 epoch,  60 batch] loss: 0.13013, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:03:17.781589 Training: [18 epoch,  70 batch] loss: 0.15588, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:04:36.993621 Training: [18 epoch,  80 batch] loss: 0.15176, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:05:55.620064 Training: [18 epoch,  90 batch] loss: 0.11907, the best RMSE/MAE: 0.37002 / 0.22692
<Test> RMSE：0.37352,MAE：0.23675
2021-01-10 14:09:43.814884 Training: [19 epoch,  10 batch] loss: 0.15359, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:11:04.702946 Training: [19 epoch,  20 batch] loss: 0.13488, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:12:26.984081 Training: [19 epoch,  30 batch] loss: 0.16612, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:13:43.618274 Training: [19 epoch,  40 batch] loss: 0.13295, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:15:02.289331 Training: [19 epoch,  50 batch] loss: 0.14676, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:16:21.247146 Training: [19 epoch,  60 batch] loss: 0.11419, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:17:40.374708 Training: [19 epoch,  70 batch] loss: 0.13562, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:18:59.723144 Training: [19 epoch,  80 batch] loss: 0.22173, the best RMSE/MAE: 0.37002 / 0.22692
2021-01-10 14:20:21.269880 Training: [19 epoch,  90 batch] loss: 0.10898, the best RMSE/MAE: 0.37002 / 0.22692
<Test> RMSE：0.35417,MAE：0.19820
2021-01-10 14:24:10.668932 Training: [20 epoch,  10 batch] loss: 0.11051, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:25:30.043200 Training: [20 epoch,  20 batch] loss: 0.12050, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:26:48.144150 Training: [20 epoch,  30 batch] loss: 0.12206, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:28:05.227816 Training: [20 epoch,  40 batch] loss: 0.13290, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:29:23.368630 Training: [20 epoch,  50 batch] loss: 0.25475, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:30:42.174116 Training: [20 epoch,  60 batch] loss: 0.13012, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:32:00.546697 Training: [20 epoch,  70 batch] loss: 0.15556, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:33:19.248898 Training: [20 epoch,  80 batch] loss: 0.10597, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:34:37.891990 Training: [20 epoch,  90 batch] loss: 0.19654, the best RMSE/MAE: 0.35417 / 0.19820
<Test> RMSE：0.35752,MAE：0.20597
2021-01-10 14:38:30.011351 Training: [21 epoch,  10 batch] loss: 0.13153, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:39:49.310412 Training: [21 epoch,  20 batch] loss: 0.10916, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:41:08.127379 Training: [21 epoch,  30 batch] loss: 0.12705, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:42:25.302303 Training: [21 epoch,  40 batch] loss: 0.13363, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:43:44.640719 Training: [21 epoch,  50 batch] loss: 0.19120, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:45:03.188592 Training: [21 epoch,  60 batch] loss: 0.12275, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:46:23.910338 Training: [21 epoch,  70 batch] loss: 0.15946, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:47:47.864003 Training: [21 epoch,  80 batch] loss: 0.13460, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:49:06.062679 Training: [21 epoch,  90 batch] loss: 0.20838, the best RMSE/MAE: 0.35417 / 0.19820
<Test> RMSE：0.36006,MAE：0.20496
2021-01-10 14:52:53.606418 Training: [22 epoch,  10 batch] loss: 0.12515, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:54:16.153238 Training: [22 epoch,  20 batch] loss: 0.11543, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:55:40.968972 Training: [22 epoch,  30 batch] loss: 0.12742, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:57:04.247059 Training: [22 epoch,  40 batch] loss: 0.10820, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:58:28.804669 Training: [22 epoch,  50 batch] loss: 0.18084, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 14:59:46.551246 Training: [22 epoch,  60 batch] loss: 0.17064, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 15:01:04.660810 Training: [22 epoch,  70 batch] loss: 0.10836, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 15:02:20.833788 Training: [22 epoch,  80 batch] loss: 0.14964, the best RMSE/MAE: 0.35417 / 0.19820
2021-01-10 15:03:35.530768 Training: [22 epoch,  90 batch] loss: 0.17316, the best RMSE/MAE: 0.35417 / 0.19820
<Test> RMSE：0.35273,MAE：0.17058
2021-01-10 15:07:18.501937 Training: [23 epoch,  10 batch] loss: 0.12244, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:08:38.776141 Training: [23 epoch,  20 batch] loss: 0.14090, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:09:57.171620 Training: [23 epoch,  30 batch] loss: 0.11161, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:11:17.323667 Training: [23 epoch,  40 batch] loss: 0.17553, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:12:36.789576 Training: [23 epoch,  50 batch] loss: 0.14132, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:13:56.003891 Training: [23 epoch,  60 batch] loss: 0.14933, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:15:15.219671 Training: [23 epoch,  70 batch] loss: 0.11177, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:16:35.160181 Training: [23 epoch,  80 batch] loss: 0.14926, the best RMSE/MAE: 0.35273 / 0.17058
2021-01-10 15:17:54.106886 Training: [23 epoch,  90 batch] loss: 0.11265, the best RMSE/MAE: 0.35273 / 0.17058
<Test> RMSE：0.35060,MAE：0.15052
2021-01-10 15:21:43.753513 Training: [24 epoch,  10 batch] loss: 0.14095, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:23:01.534635 Training: [24 epoch,  20 batch] loss: 0.14440, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:24:19.867400 Training: [24 epoch,  30 batch] loss: 0.10118, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:25:36.717299 Training: [24 epoch,  40 batch] loss: 0.15483, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:27:00.892488 Training: [24 epoch,  50 batch] loss: 0.17807, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:28:20.190148 Training: [24 epoch,  60 batch] loss: 0.18511, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:29:38.657950 Training: [24 epoch,  70 batch] loss: 0.11733, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:30:57.725801 Training: [24 epoch,  80 batch] loss: 0.12006, the best RMSE/MAE: 0.35060 / 0.15052
2021-01-10 15:32:15.299849 Training: [24 epoch,  90 batch] loss: 0.11693, the best RMSE/MAE: 0.35060 / 0.15052
<Test> RMSE：0.35176,MAE：0.14111
2021-01-10 15:36:07.982264 Training: [25 epoch,  10 batch] loss: 0.15291, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:37:27.525028 Training: [25 epoch,  20 batch] loss: 0.09800, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:38:46.028479 Training: [25 epoch,  30 batch] loss: 0.15475, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:40:02.105554 Training: [25 epoch,  40 batch] loss: 0.10345, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:41:20.328505 Training: [25 epoch,  50 batch] loss: 0.11517, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:42:38.414071 Training: [25 epoch,  60 batch] loss: 0.13797, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:43:58.813421 Training: [25 epoch,  70 batch] loss: 0.13720, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:45:17.799097 Training: [25 epoch,  80 batch] loss: 0.10947, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:46:37.987094 Training: [25 epoch,  90 batch] loss: 0.21584, the best RMSE/MAE: 0.35176 / 0.14111
<Test> RMSE：0.35709,MAE：0.14539
2021-01-10 15:50:31.911403 Training: [26 epoch,  10 batch] loss: 0.14391, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:51:53.351793 Training: [26 epoch,  20 batch] loss: 0.16139, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:53:12.566605 Training: [26 epoch,  30 batch] loss: 0.11077, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:54:29.348229 Training: [26 epoch,  40 batch] loss: 0.11275, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:55:51.598222 Training: [26 epoch,  50 batch] loss: 0.12793, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:57:12.427803 Training: [26 epoch,  60 batch] loss: 0.10389, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:58:33.146334 Training: [26 epoch,  70 batch] loss: 0.15782, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 15:59:54.118589 Training: [26 epoch,  80 batch] loss: 0.13133, the best RMSE/MAE: 0.35176 / 0.14111
2021-01-10 16:01:10.959243 Training: [26 epoch,  90 batch] loss: 0.09336, the best RMSE/MAE: 0.35176 / 0.14111
<Test> RMSE：0.35510,MAE：0.14043
2021-01-10 16:05:00.649830 Training: [27 epoch,  10 batch] loss: 0.11065, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:06:16.636552 Training: [27 epoch,  20 batch] loss: 0.17494, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:07:33.067318 Training: [27 epoch,  30 batch] loss: 0.10920, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:08:49.509876 Training: [27 epoch,  40 batch] loss: 0.14878, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:10:07.120871 Training: [27 epoch,  50 batch] loss: 0.13479, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:11:23.618358 Training: [27 epoch,  60 batch] loss: 0.11734, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:12:42.360450 Training: [27 epoch,  70 batch] loss: 0.18960, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:14:01.236995 Training: [27 epoch,  80 batch] loss: 0.12422, the best RMSE/MAE: 0.35510 / 0.14043
2021-01-10 16:15:18.538083 Training: [27 epoch,  90 batch] loss: 0.10380, the best RMSE/MAE: 0.35510 / 0.14043
<Test> RMSE：0.35612,MAE：0.13917
2021-01-10 16:19:12.059270 Training: [28 epoch,  10 batch] loss: 0.11411, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:20:32.321004 Training: [28 epoch,  20 batch] loss: 0.14352, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:21:54.369333 Training: [28 epoch,  30 batch] loss: 0.10967, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:23:10.622402 Training: [28 epoch,  40 batch] loss: 0.10043, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:24:29.396810 Training: [28 epoch,  50 batch] loss: 0.16191, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:25:48.074642 Training: [28 epoch,  60 batch] loss: 0.12339, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:27:08.550720 Training: [28 epoch,  70 batch] loss: 0.10947, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:28:28.498198 Training: [28 epoch,  80 batch] loss: 0.15641, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:29:44.226333 Training: [28 epoch,  90 batch] loss: 0.12316, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35630,MAE：0.15762
2021-01-10 16:33:34.354541 Training: [29 epoch,  10 batch] loss: 0.12891, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:34:55.562114 Training: [29 epoch,  20 batch] loss: 0.10881, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:36:14.046165 Training: [29 epoch,  30 batch] loss: 0.12532, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:37:31.091308 Training: [29 epoch,  40 batch] loss: 0.12656, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:38:48.292054 Training: [29 epoch,  50 batch] loss: 0.26081, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:40:06.929968 Training: [29 epoch,  60 batch] loss: 0.11354, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:41:25.652205 Training: [29 epoch,  70 batch] loss: 0.11750, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:42:44.534423 Training: [29 epoch,  80 batch] loss: 0.12505, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:44:03.889490 Training: [29 epoch,  90 batch] loss: 0.09596, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35747,MAE：0.15441
2021-01-10 16:47:55.984046 Training: [30 epoch,  10 batch] loss: 0.12203, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:49:14.164632 Training: [30 epoch,  20 batch] loss: 0.12510, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:50:32.540230 Training: [30 epoch,  30 batch] loss: 0.10546, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:51:49.314799 Training: [30 epoch,  40 batch] loss: 0.09426, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:53:07.075645 Training: [30 epoch,  50 batch] loss: 0.10078, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:54:26.520575 Training: [30 epoch,  60 batch] loss: 0.13364, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:55:47.452720 Training: [30 epoch,  70 batch] loss: 0.12859, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:57:08.064414 Training: [30 epoch,  80 batch] loss: 0.11483, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 16:58:30.133373 Training: [30 epoch,  90 batch] loss: 0.10731, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35682,MAE：0.15228
2021-01-10 17:02:27.882502 Training: [31 epoch,  10 batch] loss: 0.12039, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:03:50.467248 Training: [31 epoch,  20 batch] loss: 0.12810, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:05:10.267532 Training: [31 epoch,  30 batch] loss: 0.18172, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:06:27.450783 Training: [31 epoch,  40 batch] loss: 0.14551, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:07:45.677212 Training: [31 epoch,  50 batch] loss: 0.15209, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:09:09.504463 Training: [31 epoch,  60 batch] loss: 0.10209, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:10:28.837022 Training: [31 epoch,  70 batch] loss: 0.10210, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:11:46.871543 Training: [31 epoch,  80 batch] loss: 0.12065, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:13:04.496134 Training: [31 epoch,  90 batch] loss: 0.11122, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.36350,MAE：0.17005
2021-01-10 17:16:55.830955 Training: [32 epoch,  10 batch] loss: 0.12613, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:18:16.248237 Training: [32 epoch,  20 batch] loss: 0.11122, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:19:38.194443 Training: [32 epoch,  30 batch] loss: 0.10414, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:20:58.275596 Training: [32 epoch,  40 batch] loss: 0.24422, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:22:16.384662 Training: [32 epoch,  50 batch] loss: 0.10976, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:23:36.088093 Training: [32 epoch,  60 batch] loss: 0.12033, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:24:54.865627 Training: [32 epoch,  70 batch] loss: 0.10133, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:26:14.033397 Training: [32 epoch,  80 batch] loss: 0.09981, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:27:29.303469 Training: [32 epoch,  90 batch] loss: 0.16334, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.36024,MAE：0.14710
2021-01-10 17:31:20.430738 Training: [33 epoch,  10 batch] loss: 0.11061, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:32:40.549307 Training: [33 epoch,  20 batch] loss: 0.20250, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:33:59.504908 Training: [33 epoch,  30 batch] loss: 0.14079, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:35:17.947199 Training: [33 epoch,  40 batch] loss: 0.13820, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:36:34.532096 Training: [33 epoch,  50 batch] loss: 0.14269, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:37:54.352465 Training: [33 epoch,  60 batch] loss: 0.12952, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:39:14.372783 Training: [33 epoch,  70 batch] loss: 0.11626, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:40:33.452035 Training: [33 epoch,  80 batch] loss: 0.10865, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:41:50.056914 Training: [33 epoch,  90 batch] loss: 0.08876, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.36239,MAE：0.15813
2021-01-10 17:45:43.435173 Training: [34 epoch,  10 batch] loss: 0.10888, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:47:04.366639 Training: [34 epoch,  20 batch] loss: 0.15974, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:48:25.086368 Training: [34 epoch,  30 batch] loss: 0.17059, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:49:44.187543 Training: [34 epoch,  40 batch] loss: 0.12823, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:51:01.270480 Training: [34 epoch,  50 batch] loss: 0.09782, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:52:19.851727 Training: [34 epoch,  60 batch] loss: 0.09974, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:53:39.765282 Training: [34 epoch,  70 batch] loss: 0.17768, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:55:00.718577 Training: [34 epoch,  80 batch] loss: 0.11471, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 17:56:16.982655 Training: [34 epoch,  90 batch] loss: 0.08523, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35926,MAE：0.15181
2021-01-10 18:00:10.071891 Training: [35 epoch,  10 batch] loss: 0.10712, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:01:28.235565 Training: [35 epoch,  20 batch] loss: 0.11170, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:02:47.869461 Training: [35 epoch,  30 batch] loss: 0.14380, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:04:05.549694 Training: [35 epoch,  40 batch] loss: 0.09490, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:05:22.404685 Training: [35 epoch,  50 batch] loss: 0.18439, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:06:41.704495 Training: [35 epoch,  60 batch] loss: 0.15284, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:08:00.567231 Training: [35 epoch,  70 batch] loss: 0.09772, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:09:20.776551 Training: [35 epoch,  80 batch] loss: 0.12751, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:10:36.919699 Training: [35 epoch,  90 batch] loss: 0.10694, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35296,MAE：0.14994
2021-01-10 18:14:28.809739 Training: [36 epoch,  10 batch] loss: 0.08801, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:15:46.960394 Training: [36 epoch,  20 batch] loss: 0.11017, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:17:04.557144 Training: [36 epoch,  30 batch] loss: 0.11404, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:18:20.826485 Training: [36 epoch,  40 batch] loss: 0.13749, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:19:35.258808 Training: [36 epoch,  50 batch] loss: 0.12162, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:20:52.265955 Training: [36 epoch,  60 batch] loss: 0.09982, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:22:10.495902 Training: [36 epoch,  70 batch] loss: 0.12185, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:23:29.757341 Training: [36 epoch,  80 batch] loss: 0.11786, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:24:46.426168 Training: [36 epoch,  90 batch] loss: 0.19745, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35437,MAE：0.15195
2021-01-10 18:28:41.869009 Training: [37 epoch,  10 batch] loss: 0.13691, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:30:01.894015 Training: [37 epoch,  20 batch] loss: 0.10978, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:31:22.097150 Training: [37 epoch,  30 batch] loss: 0.13759, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:32:40.732762 Training: [37 epoch,  40 batch] loss: 0.10591, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:33:58.192635 Training: [37 epoch,  50 batch] loss: 0.10840, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:35:16.612874 Training: [37 epoch,  60 batch] loss: 0.11351, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:36:36.503427 Training: [37 epoch,  70 batch] loss: 0.10836, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:37:56.034143 Training: [37 epoch,  80 batch] loss: 0.12770, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:39:11.961483 Training: [37 epoch,  90 batch] loss: 0.10872, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.36120,MAE：0.16100
2021-01-10 18:43:07.057366 Training: [38 epoch,  10 batch] loss: 0.08523, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:44:26.265887 Training: [38 epoch,  20 batch] loss: 0.10186, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:45:45.480079 Training: [38 epoch,  30 batch] loss: 0.15080, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:47:05.952811 Training: [38 epoch,  40 batch] loss: 0.19199, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:48:23.643907 Training: [38 epoch,  50 batch] loss: 0.11518, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:49:42.181456 Training: [38 epoch,  60 batch] loss: 0.10211, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:51:00.915034 Training: [38 epoch,  70 batch] loss: 0.12964, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:52:20.057719 Training: [38 epoch,  80 batch] loss: 0.13031, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:53:34.544860 Training: [38 epoch,  90 batch] loss: 0.09204, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35147,MAE：0.16211
2021-01-10 18:57:26.733558 Training: [39 epoch,  10 batch] loss: 0.10308, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 18:58:44.942411 Training: [39 epoch,  20 batch] loss: 0.16288, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:00:03.730568 Training: [39 epoch,  30 batch] loss: 0.08678, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:01:23.238634 Training: [39 epoch,  40 batch] loss: 0.11135, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:02:40.053043 Training: [39 epoch,  50 batch] loss: 0.17719, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:03:58.095474 Training: [39 epoch,  60 batch] loss: 0.09921, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:05:16.980097 Training: [39 epoch,  70 batch] loss: 0.14181, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:06:35.674199 Training: [39 epoch,  80 batch] loss: 0.09086, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:07:52.674337 Training: [39 epoch,  90 batch] loss: 0.11994, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35472,MAE：0.15800
2021-01-10 19:11:44.845275 Training: [40 epoch,  10 batch] loss: 0.13102, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:13:02.815437 Training: [40 epoch,  20 batch] loss: 0.15172, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:14:22.436226 Training: [40 epoch,  30 batch] loss: 0.11888, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:15:42.615819 Training: [40 epoch,  40 batch] loss: 0.19519, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:17:03.444721 Training: [40 epoch,  50 batch] loss: 0.07831, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:18:22.596651 Training: [40 epoch,  60 batch] loss: 0.10656, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:19:42.229968 Training: [40 epoch,  70 batch] loss: 0.10455, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:21:00.827941 Training: [40 epoch,  80 batch] loss: 0.13108, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:22:14.046245 Training: [40 epoch,  90 batch] loss: 0.09168, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35539,MAE：0.14498
2021-01-10 19:25:56.612683 Training: [41 epoch,  10 batch] loss: 0.16184, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:27:11.919952 Training: [41 epoch,  20 batch] loss: 0.13578, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:28:29.582015 Training: [41 epoch,  30 batch] loss: 0.10685, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:29:48.104480 Training: [41 epoch,  40 batch] loss: 0.09969, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:31:06.688752 Training: [41 epoch,  50 batch] loss: 0.08745, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:32:22.598746 Training: [41 epoch,  60 batch] loss: 0.11801, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:33:41.590076 Training: [41 epoch,  70 batch] loss: 0.09803, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:35:00.320121 Training: [41 epoch,  80 batch] loss: 0.08597, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:36:17.559663 Training: [41 epoch,  90 batch] loss: 0.10745, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35119,MAE：0.14165
2021-01-10 19:40:09.632974 Training: [42 epoch,  10 batch] loss: 0.09040, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:41:28.224356 Training: [42 epoch,  20 batch] loss: 0.09462, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:42:49.057054 Training: [42 epoch,  30 batch] loss: 0.08269, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:44:09.627963 Training: [42 epoch,  40 batch] loss: 0.11286, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:45:30.639464 Training: [42 epoch,  50 batch] loss: 0.10869, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:46:48.648543 Training: [42 epoch,  60 batch] loss: 0.24176, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:48:07.704820 Training: [42 epoch,  70 batch] loss: 0.12016, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:49:27.491010 Training: [42 epoch,  80 batch] loss: 0.11665, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:50:44.130097 Training: [42 epoch,  90 batch] loss: 0.09236, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35256,MAE：0.14220
2021-01-10 19:54:38.091666 Training: [43 epoch,  10 batch] loss: 0.15108, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:55:56.557998 Training: [43 epoch,  20 batch] loss: 0.09582, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:57:16.103305 Training: [43 epoch,  30 batch] loss: 0.11360, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:58:36.102449 Training: [43 epoch,  40 batch] loss: 0.11052, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 19:59:56.885159 Training: [43 epoch,  50 batch] loss: 0.09425, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:01:14.724795 Training: [43 epoch,  60 batch] loss: 0.12451, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:02:34.835628 Training: [43 epoch,  70 batch] loss: 0.09400, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:03:55.720726 Training: [43 epoch,  80 batch] loss: 0.18890, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:05:13.818482 Training: [43 epoch,  90 batch] loss: 0.11073, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35125,MAE：0.16472
2021-01-10 20:09:07.174923 Training: [44 epoch,  10 batch] loss: 0.19173, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:10:26.894121 Training: [44 epoch,  20 batch] loss: 0.10531, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:11:46.719655 Training: [44 epoch,  30 batch] loss: 0.08600, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:13:06.422676 Training: [44 epoch,  40 batch] loss: 0.10843, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:14:24.359824 Training: [44 epoch,  50 batch] loss: 0.11037, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:15:41.795317 Training: [44 epoch,  60 batch] loss: 0.12155, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:17:00.309182 Training: [44 epoch,  70 batch] loss: 0.12297, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:18:19.956769 Training: [44 epoch,  80 batch] loss: 0.10070, the best RMSE/MAE: 0.35612 / 0.13917
2021-01-10 20:19:36.829029 Training: [44 epoch,  90 batch] loss: 0.13306, the best RMSE/MAE: 0.35612 / 0.13917
<Test> RMSE：0.35232,MAE：0.13819
2021-01-10 20:23:26.669473 Training: [45 epoch,  10 batch] loss: 0.11454, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:24:44.421946 Training: [45 epoch,  20 batch] loss: 0.11272, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:26:03.532807 Training: [45 epoch,  30 batch] loss: 0.12426, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:27:20.884683 Training: [45 epoch,  40 batch] loss: 0.09416, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:28:37.045938 Training: [45 epoch,  50 batch] loss: 0.10366, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:29:51.468496 Training: [45 epoch,  60 batch] loss: 0.09453, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:31:07.272442 Training: [45 epoch,  70 batch] loss: 0.08789, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:32:24.515027 Training: [45 epoch,  80 batch] loss: 0.16525, the best RMSE/MAE: 0.35232 / 0.13819
2021-01-10 20:33:41.588852 Training: [45 epoch,  90 batch] loss: 0.12583, the best RMSE/MAE: 0.35232 / 0.13819
<Test> RMSE：0.35229,MAE：0.13563
2021-01-10 20:37:33.779922 Training: [46 epoch,  10 batch] loss: 0.10571, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:38:52.556882 Training: [46 epoch,  20 batch] loss: 0.10777, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:40:12.590891 Training: [46 epoch,  30 batch] loss: 0.09476, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:41:32.081550 Training: [46 epoch,  40 batch] loss: 0.09349, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:42:52.248239 Training: [46 epoch,  50 batch] loss: 0.09974, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:44:10.410083 Training: [46 epoch,  60 batch] loss: 0.11068, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:45:30.896662 Training: [46 epoch,  70 batch] loss: 0.11554, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:46:51.862536 Training: [46 epoch,  80 batch] loss: 0.12007, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:48:09.570310 Training: [46 epoch,  90 batch] loss: 0.21476, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.35479,MAE：0.14124
2021-01-10 20:52:00.446622 Training: [47 epoch,  10 batch] loss: 0.09655, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:53:18.592913 Training: [47 epoch,  20 batch] loss: 0.17338, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:54:39.256417 Training: [47 epoch,  30 batch] loss: 0.08862, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:55:59.625340 Training: [47 epoch,  40 batch] loss: 0.09054, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:57:20.861895 Training: [47 epoch,  50 batch] loss: 0.08618, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:58:39.010754 Training: [47 epoch,  60 batch] loss: 0.09664, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 20:59:57.285338 Training: [47 epoch,  70 batch] loss: 0.17902, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:01:18.015806 Training: [47 epoch,  80 batch] loss: 0.10249, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:02:36.972155 Training: [47 epoch,  90 batch] loss: 0.10810, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.35278,MAE：0.13582
2021-01-10 21:06:31.059999 Training: [48 epoch,  10 batch] loss: 0.10849, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:07:50.456128 Training: [48 epoch,  20 batch] loss: 0.11991, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:09:10.146175 Training: [48 epoch,  30 batch] loss: 0.18063, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:10:30.268487 Training: [48 epoch,  40 batch] loss: 0.09482, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:11:50.360689 Training: [48 epoch,  50 batch] loss: 0.11356, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:13:08.539825 Training: [48 epoch,  60 batch] loss: 0.12055, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:14:27.816262 Training: [48 epoch,  70 batch] loss: 0.10091, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:15:47.537713 Training: [48 epoch,  80 batch] loss: 0.09045, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:17:04.462294 Training: [48 epoch,  90 batch] loss: 0.09109, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.35023,MAE：0.13817
2021-01-10 21:20:56.357169 Training: [49 epoch,  10 batch] loss: 0.20468, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:22:16.224313 Training: [49 epoch,  20 batch] loss: 0.10588, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:23:36.509016 Training: [49 epoch,  30 batch] loss: 0.11855, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:24:56.850138 Training: [49 epoch,  40 batch] loss: 0.12033, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:26:16.499740 Training: [49 epoch,  50 batch] loss: 0.08376, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:27:34.090827 Training: [49 epoch,  60 batch] loss: 0.11664, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:28:51.749981 Training: [49 epoch,  70 batch] loss: 0.10615, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:30:10.464749 Training: [49 epoch,  80 batch] loss: 0.10675, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:31:27.180671 Training: [49 epoch,  90 batch] loss: 0.08681, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.34951,MAE：0.13851
2021-01-10 21:35:12.043657 Training: [50 epoch,  10 batch] loss: 0.10951, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:36:25.816168 Training: [50 epoch,  20 batch] loss: 0.10195, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:37:42.267064 Training: [50 epoch,  30 batch] loss: 0.10200, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:38:58.213094 Training: [50 epoch,  40 batch] loss: 0.11538, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:40:16.738895 Training: [50 epoch,  50 batch] loss: 0.09225, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:41:34.756736 Training: [50 epoch,  60 batch] loss: 0.12390, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:42:54.687124 Training: [50 epoch,  70 batch] loss: 0.08579, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:44:15.193336 Training: [50 epoch,  80 batch] loss: 0.15131, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:45:33.290793 Training: [50 epoch,  90 batch] loss: 0.10525, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.34976,MAE：0.13831
2021-01-10 21:49:27.055611 Training: [51 epoch,  10 batch] loss: 0.10252, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:50:44.871926 Training: [51 epoch,  20 batch] loss: 0.11259, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:52:03.520389 Training: [51 epoch,  30 batch] loss: 0.08052, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:53:23.356401 Training: [51 epoch,  40 batch] loss: 0.10377, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:54:42.381692 Training: [51 epoch,  50 batch] loss: 0.08623, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:55:59.926730 Training: [51 epoch,  60 batch] loss: 0.12872, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:57:17.390502 Training: [51 epoch,  70 batch] loss: 0.11429, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:58:36.016004 Training: [51 epoch,  80 batch] loss: 0.09757, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 21:59:52.547893 Training: [51 epoch,  90 batch] loss: 0.21420, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.34765,MAE：0.15067
2021-01-10 22:03:44.176061 Training: [52 epoch,  10 batch] loss: 0.09737, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:05:01.928721 Training: [52 epoch,  20 batch] loss: 0.10707, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:06:20.506895 Training: [52 epoch,  30 batch] loss: 0.20483, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:07:39.549994 Training: [52 epoch,  40 batch] loss: 0.11512, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:08:59.265273 Training: [52 epoch,  50 batch] loss: 0.11077, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:10:16.843775 Training: [52 epoch,  60 batch] loss: 0.08250, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:11:35.415507 Training: [52 epoch,  70 batch] loss: 0.11513, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:12:55.627555 Training: [52 epoch,  80 batch] loss: 0.10528, the best RMSE/MAE: 0.35229 / 0.13563
2021-01-10 22:14:13.314789 Training: [52 epoch,  90 batch] loss: 0.12535, the best RMSE/MAE: 0.35229 / 0.13563
<Test> RMSE：0.35105,MAE：0.13373
2021-01-10 22:18:15.687767 Training: [53 epoch,  10 batch] loss: 0.14166, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:19:44.430261 Training: [53 epoch,  20 batch] loss: 0.13311, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:21:14.661349 Training: [53 epoch,  30 batch] loss: 0.10172, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:22:45.779621 Training: [53 epoch,  40 batch] loss: 0.09474, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:24:18.825719 Training: [53 epoch,  50 batch] loss: 0.07392, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:25:51.578441 Training: [53 epoch,  60 batch] loss: 0.14402, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:27:24.081401 Training: [53 epoch,  70 batch] loss: 0.08837, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:28:59.673698 Training: [53 epoch,  80 batch] loss: 0.10180, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:30:31.813453 Training: [53 epoch,  90 batch] loss: 0.16556, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34926,MAE：0.13930
2021-01-10 22:35:04.558402 Training: [54 epoch,  10 batch] loss: 0.10300, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:36:33.364919 Training: [54 epoch,  20 batch] loss: 0.06677, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:38:07.096641 Training: [54 epoch,  30 batch] loss: 0.09593, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:39:43.655983 Training: [54 epoch,  40 batch] loss: 0.13288, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:41:16.110762 Training: [54 epoch,  50 batch] loss: 0.08338, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:42:44.775506 Training: [54 epoch,  60 batch] loss: 0.17516, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:44:12.552347 Training: [54 epoch,  70 batch] loss: 0.09242, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:45:32.543605 Training: [54 epoch,  80 batch] loss: 0.13300, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:46:47.013082 Training: [54 epoch,  90 batch] loss: 0.11713, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34918,MAE：0.14128
2021-01-10 22:50:35.808113 Training: [55 epoch,  10 batch] loss: 0.09290, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:51:54.468147 Training: [55 epoch,  20 batch] loss: 0.07038, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:53:13.111947 Training: [55 epoch,  30 batch] loss: 0.09531, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:54:35.747029 Training: [55 epoch,  40 batch] loss: 0.09909, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:55:57.578863 Training: [55 epoch,  50 batch] loss: 0.13765, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:57:19.144425 Training: [55 epoch,  60 batch] loss: 0.12085, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 22:58:39.472141 Training: [55 epoch,  70 batch] loss: 0.10201, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:00:00.253967 Training: [55 epoch,  80 batch] loss: 0.10463, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:01:16.762789 Training: [55 epoch,  90 batch] loss: 0.10570, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34847,MAE：0.14252
2021-01-10 23:05:14.834451 Training: [56 epoch,  10 batch] loss: 0.17330, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:06:35.056563 Training: [56 epoch,  20 batch] loss: 0.10569, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:07:57.402051 Training: [56 epoch,  30 batch] loss: 0.09081, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:09:18.505442 Training: [56 epoch,  40 batch] loss: 0.11336, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:10:38.996896 Training: [56 epoch,  50 batch] loss: 0.07041, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:11:58.393778 Training: [56 epoch,  60 batch] loss: 0.11851, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:13:17.021957 Training: [56 epoch,  70 batch] loss: 0.14490, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:14:37.125935 Training: [56 epoch,  80 batch] loss: 0.10793, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:15:52.675987 Training: [56 epoch,  90 batch] loss: 0.08384, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34914,MAE：0.13768
2021-01-10 23:19:48.564759 Training: [57 epoch,  10 batch] loss: 0.11708, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:21:06.521852 Training: [57 epoch,  20 batch] loss: 0.13090, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:22:27.959584 Training: [57 epoch,  30 batch] loss: 0.09213, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:23:49.321564 Training: [57 epoch,  40 batch] loss: 0.10468, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:25:10.506527 Training: [57 epoch,  50 batch] loss: 0.10199, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:26:27.689311 Training: [57 epoch,  60 batch] loss: 0.08868, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:27:45.069206 Training: [57 epoch,  70 batch] loss: 0.08201, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:29:05.453842 Training: [57 epoch,  80 batch] loss: 0.10559, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:30:20.953361 Training: [57 epoch,  90 batch] loss: 0.12683, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34887,MAE：0.13669
2021-01-10 23:34:18.684819 Training: [58 epoch,  10 batch] loss: 0.07447, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:35:35.159627 Training: [58 epoch,  20 batch] loss: 0.11088, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:36:53.699803 Training: [58 epoch,  30 batch] loss: 0.09906, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:38:13.400275 Training: [58 epoch,  40 batch] loss: 0.16079, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:39:33.213178 Training: [58 epoch,  50 batch] loss: 0.12477, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:40:53.052914 Training: [58 epoch,  60 batch] loss: 0.12329, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:42:12.489718 Training: [58 epoch,  70 batch] loss: 0.09810, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:43:31.441398 Training: [58 epoch,  80 batch] loss: 0.09565, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:44:46.058824 Training: [58 epoch,  90 batch] loss: 0.10809, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34831,MAE：0.15028
2021-01-10 23:48:39.214712 Training: [59 epoch,  10 batch] loss: 0.08177, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:49:54.311755 Training: [59 epoch,  20 batch] loss: 0.10265, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:51:10.778445 Training: [59 epoch,  30 batch] loss: 0.11300, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:52:27.645898 Training: [59 epoch,  40 batch] loss: 0.11730, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:53:44.354472 Training: [59 epoch,  50 batch] loss: 0.13178, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:54:59.919209 Training: [59 epoch,  60 batch] loss: 0.10555, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:56:17.196293 Training: [59 epoch,  70 batch] loss: 0.08593, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:57:37.650741 Training: [59 epoch,  80 batch] loss: 0.17818, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-10 23:58:53.568003 Training: [59 epoch,  90 batch] loss: 0.09875, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.34801,MAE：0.14416
2021-01-11 00:02:49.359212 Training: [60 epoch,  10 batch] loss: 0.11615, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:04:05.369083 Training: [60 epoch,  20 batch] loss: 0.09251, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:05:23.185355 Training: [60 epoch,  30 batch] loss: 0.09340, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:06:41.871218 Training: [60 epoch,  40 batch] loss: 0.09054, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:08:00.400452 Training: [60 epoch,  50 batch] loss: 0.11658, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:09:18.777617 Training: [60 epoch,  60 batch] loss: 0.09494, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:10:35.850874 Training: [60 epoch,  70 batch] loss: 0.16290, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:11:53.256480 Training: [60 epoch,  80 batch] loss: 0.12419, the best RMSE/MAE: 0.35105 / 0.13373
2021-01-11 00:13:07.330835 Training: [60 epoch,  90 batch] loss: 0.09905, the best RMSE/MAE: 0.35105 / 0.13373
<Test> RMSE：0.35113,MAE：0.13053
2021-01-11 00:17:02.400709 Training: [61 epoch,  10 batch] loss: 0.09381, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:18:19.962857 Training: [61 epoch,  20 batch] loss: 0.14822, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:19:38.951005 Training: [61 epoch,  30 batch] loss: 0.09296, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:20:58.765344 Training: [61 epoch,  40 batch] loss: 0.12870, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:22:18.059796 Training: [61 epoch,  50 batch] loss: 0.11044, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:23:36.510862 Training: [61 epoch,  60 batch] loss: 0.12364, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:24:53.460311 Training: [61 epoch,  70 batch] loss: 0.10411, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:26:10.936412 Training: [61 epoch,  80 batch] loss: 0.10905, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:27:26.052954 Training: [61 epoch,  90 batch] loss: 0.10147, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34870,MAE：0.13517
2021-01-11 00:31:22.501173 Training: [62 epoch,  10 batch] loss: 0.14146, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:32:39.332915 Training: [62 epoch,  20 batch] loss: 0.12220, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:33:58.360574 Training: [62 epoch,  30 batch] loss: 0.09886, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:35:17.745408 Training: [62 epoch,  40 batch] loss: 0.08182, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:36:39.124515 Training: [62 epoch,  50 batch] loss: 0.08587, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:38:00.944201 Training: [62 epoch,  60 batch] loss: 0.13515, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:39:18.228368 Training: [62 epoch,  70 batch] loss: 0.09415, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:40:36.110809 Training: [62 epoch,  80 batch] loss: 0.11086, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:41:51.476615 Training: [62 epoch,  90 batch] loss: 0.10274, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35067,MAE：0.15192
2021-01-11 00:45:48.596690 Training: [63 epoch,  10 batch] loss: 0.08521, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:47:06.005076 Training: [63 epoch,  20 batch] loss: 0.11129, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:48:24.720618 Training: [63 epoch,  30 batch] loss: 0.15477, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:49:45.269410 Training: [63 epoch,  40 batch] loss: 0.10456, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:51:06.273841 Training: [63 epoch,  50 batch] loss: 0.13030, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:52:27.524852 Training: [63 epoch,  60 batch] loss: 0.09789, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:53:45.014160 Training: [63 epoch,  70 batch] loss: 0.12081, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:55:01.994786 Training: [63 epoch,  80 batch] loss: 0.09417, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 00:56:16.978879 Training: [63 epoch,  90 batch] loss: 0.09244, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35086,MAE：0.14265
2021-01-11 01:00:04.797298 Training: [64 epoch,  10 batch] loss: 0.10117, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:01:18.642679 Training: [64 epoch,  20 batch] loss: 0.08038, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:02:35.904028 Training: [64 epoch,  30 batch] loss: 0.08294, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:03:54.276566 Training: [64 epoch,  40 batch] loss: 0.16747, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:05:12.623516 Training: [64 epoch,  50 batch] loss: 0.08252, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:06:31.537700 Training: [64 epoch,  60 batch] loss: 0.10828, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:07:48.628491 Training: [64 epoch,  70 batch] loss: 0.09625, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:09:07.198650 Training: [64 epoch,  80 batch] loss: 0.10951, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:10:24.178926 Training: [64 epoch,  90 batch] loss: 0.09531, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35053,MAE：0.14914
2021-01-11 01:14:21.849846 Training: [65 epoch,  10 batch] loss: 0.08712, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:15:39.417482 Training: [65 epoch,  20 batch] loss: 0.10034, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:16:58.553070 Training: [65 epoch,  30 batch] loss: 0.07881, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:18:19.010940 Training: [65 epoch,  40 batch] loss: 0.10979, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:19:38.248963 Training: [65 epoch,  50 batch] loss: 0.14360, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:20:59.212507 Training: [65 epoch,  60 batch] loss: 0.10250, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:22:18.603446 Training: [65 epoch,  70 batch] loss: 0.09743, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:23:37.062789 Training: [65 epoch,  80 batch] loss: 0.15100, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:24:52.442388 Training: [65 epoch,  90 batch] loss: 0.11230, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35219,MAE：0.16113
2021-01-11 01:28:47.852147 Training: [66 epoch,  10 batch] loss: 0.08454, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:30:04.702495 Training: [66 epoch,  20 batch] loss: 0.10191, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:31:23.663824 Training: [66 epoch,  30 batch] loss: 0.10267, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:32:43.630871 Training: [66 epoch,  40 batch] loss: 0.13735, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:34:03.286610 Training: [66 epoch,  50 batch] loss: 0.11293, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:35:23.360969 Training: [66 epoch,  60 batch] loss: 0.08981, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:36:42.133987 Training: [66 epoch,  70 batch] loss: 0.09141, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:38:01.996248 Training: [66 epoch,  80 batch] loss: 0.16774, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:39:17.908509 Training: [66 epoch,  90 batch] loss: 0.11375, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35111,MAE：0.16360
2021-01-11 01:43:14.302578 Training: [67 epoch,  10 batch] loss: 0.20057, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:44:31.255381 Training: [67 epoch,  20 batch] loss: 0.08644, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:45:49.793076 Training: [67 epoch,  30 batch] loss: 0.08773, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:47:09.224556 Training: [67 epoch,  40 batch] loss: 0.09239, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:48:30.233505 Training: [67 epoch,  50 batch] loss: 0.09021, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:49:51.045098 Training: [67 epoch,  60 batch] loss: 0.08326, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:51:09.018327 Training: [67 epoch,  70 batch] loss: 0.09565, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:52:26.657770 Training: [67 epoch,  80 batch] loss: 0.10592, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:53:41.108206 Training: [67 epoch,  90 batch] loss: 0.11928, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34736,MAE：0.14837
2021-01-11 01:57:37.192560 Training: [68 epoch,  10 batch] loss: 0.12151, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 01:58:55.635991 Training: [68 epoch,  20 batch] loss: 0.10892, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:00:12.764037 Training: [68 epoch,  30 batch] loss: 0.08558, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:01:29.420456 Training: [68 epoch,  40 batch] loss: 0.08777, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:02:45.358594 Training: [68 epoch,  50 batch] loss: 0.11891, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:04:01.326980 Training: [68 epoch,  60 batch] loss: 0.11573, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:05:14.997800 Training: [68 epoch,  70 batch] loss: 0.12097, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:06:30.301729 Training: [68 epoch,  80 batch] loss: 0.08839, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:07:45.703623 Training: [68 epoch,  90 batch] loss: 0.17782, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34652,MAE：0.16420
2021-01-11 02:11:42.258527 Training: [69 epoch,  10 batch] loss: 0.08262, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:12:59.402496 Training: [69 epoch,  20 batch] loss: 0.10539, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:14:20.626447 Training: [69 epoch,  30 batch] loss: 0.13260, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:15:41.356755 Training: [69 epoch,  40 batch] loss: 0.09875, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:17:01.152058 Training: [69 epoch,  50 batch] loss: 0.18712, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:18:20.828008 Training: [69 epoch,  60 batch] loss: 0.09621, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:19:38.277841 Training: [69 epoch,  70 batch] loss: 0.09692, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:20:55.337772 Training: [69 epoch,  80 batch] loss: 0.09186, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:22:11.906584 Training: [69 epoch,  90 batch] loss: 0.08612, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34570,MAE：0.14326
2021-01-11 02:26:06.191740 Training: [70 epoch,  10 batch] loss: 0.12248, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:27:22.073486 Training: [70 epoch,  20 batch] loss: 0.13779, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:28:40.193233 Training: [70 epoch,  30 batch] loss: 0.11429, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:29:58.579020 Training: [70 epoch,  40 batch] loss: 0.11220, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:31:17.290661 Training: [70 epoch,  50 batch] loss: 0.09449, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:32:35.944982 Training: [70 epoch,  60 batch] loss: 0.09318, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:33:52.109593 Training: [70 epoch,  70 batch] loss: 0.09337, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:35:09.614158 Training: [70 epoch,  80 batch] loss: 0.08676, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:36:25.809707 Training: [70 epoch,  90 batch] loss: 0.11211, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34889,MAE：0.15179
2021-01-11 02:40:22.051469 Training: [71 epoch,  10 batch] loss: 0.10685, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:41:40.862619 Training: [71 epoch,  20 batch] loss: 0.07349, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:42:59.961439 Training: [71 epoch,  30 batch] loss: 0.13229, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:44:18.957424 Training: [71 epoch,  40 batch] loss: 0.09051, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:45:38.717174 Training: [71 epoch,  50 batch] loss: 0.11585, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:46:57.940816 Training: [71 epoch,  60 batch] loss: 0.10002, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:48:16.110492 Training: [71 epoch,  70 batch] loss: 0.09992, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:49:34.548497 Training: [71 epoch,  80 batch] loss: 0.10714, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:50:51.231218 Training: [71 epoch,  90 batch] loss: 0.15750, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34909,MAE：0.14689
2021-01-11 02:54:47.199387 Training: [72 epoch,  10 batch] loss: 0.10343, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:56:04.192608 Training: [72 epoch,  20 batch] loss: 0.12558, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:57:22.385206 Training: [72 epoch,  30 batch] loss: 0.11674, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 02:58:41.455215 Training: [72 epoch,  40 batch] loss: 0.08927, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:00:00.834359 Training: [72 epoch,  50 batch] loss: 0.12951, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:01:20.586037 Training: [72 epoch,  60 batch] loss: 0.13532, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:02:37.856624 Training: [72 epoch,  70 batch] loss: 0.09759, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:03:55.858350 Training: [72 epoch,  80 batch] loss: 0.09542, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:05:12.304586 Training: [72 epoch,  90 batch] loss: 0.07956, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34767,MAE：0.14737
2021-01-11 03:08:59.412511 Training: [73 epoch,  10 batch] loss: 0.15594, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:10:15.308131 Training: [73 epoch,  20 batch] loss: 0.08183, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:11:33.274826 Training: [73 epoch,  30 batch] loss: 0.13759, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:12:51.389925 Training: [73 epoch,  40 batch] loss: 0.09531, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:14:10.812670 Training: [73 epoch,  50 batch] loss: 0.10715, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:15:29.899954 Training: [73 epoch,  60 batch] loss: 0.10093, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:16:46.732573 Training: [73 epoch,  70 batch] loss: 0.11564, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:18:03.346154 Training: [73 epoch,  80 batch] loss: 0.10500, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:19:18.679527 Training: [73 epoch,  90 batch] loss: 0.10577, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34914,MAE：0.15500
2021-01-11 03:23:15.147329 Training: [74 epoch,  10 batch] loss: 0.15335, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:24:33.183069 Training: [74 epoch,  20 batch] loss: 0.07758, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:25:52.579240 Training: [74 epoch,  30 batch] loss: 0.08626, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:27:13.440890 Training: [74 epoch,  40 batch] loss: 0.09688, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:28:34.802620 Training: [74 epoch,  50 batch] loss: 0.08883, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:29:54.943568 Training: [74 epoch,  60 batch] loss: 0.10064, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:31:13.081914 Training: [74 epoch,  70 batch] loss: 0.13645, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:32:29.339404 Training: [74 epoch,  80 batch] loss: 0.10365, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:33:44.568129 Training: [74 epoch,  90 batch] loss: 0.10107, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34752,MAE：0.15040
2021-01-11 03:37:39.338533 Training: [75 epoch,  10 batch] loss: 0.10488, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:38:57.387612 Training: [75 epoch,  20 batch] loss: 0.11478, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:40:17.482568 Training: [75 epoch,  30 batch] loss: 0.10473, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:41:37.074864 Training: [75 epoch,  40 batch] loss: 0.10429, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:42:56.891542 Training: [75 epoch,  50 batch] loss: 0.12020, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:44:16.475965 Training: [75 epoch,  60 batch] loss: 0.09349, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:45:35.916136 Training: [75 epoch,  70 batch] loss: 0.08563, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:46:53.792940 Training: [75 epoch,  80 batch] loss: 0.16167, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:48:10.209145 Training: [75 epoch,  90 batch] loss: 0.10387, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35033,MAE：0.15525
2021-01-11 03:52:04.950692 Training: [76 epoch,  10 batch] loss: 0.11134, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:53:22.478092 Training: [76 epoch,  20 batch] loss: 0.13386, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:54:42.259405 Training: [76 epoch,  30 batch] loss: 0.11529, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:56:02.716167 Training: [76 epoch,  40 batch] loss: 0.09065, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:57:23.725765 Training: [76 epoch,  50 batch] loss: 0.10344, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 03:58:44.211245 Training: [76 epoch,  60 batch] loss: 0.09331, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:00:01.971243 Training: [76 epoch,  70 batch] loss: 0.09134, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:01:19.478691 Training: [76 epoch,  80 batch] loss: 0.09235, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:02:36.487562 Training: [76 epoch,  90 batch] loss: 0.10761, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34569,MAE：0.14546
2021-01-11 04:06:30.917780 Training: [77 epoch,  10 batch] loss: 0.08128, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:07:49.017668 Training: [77 epoch,  20 batch] loss: 0.08082, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:09:08.621635 Training: [77 epoch,  30 batch] loss: 0.12293, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:10:29.353613 Training: [77 epoch,  40 batch] loss: 0.13901, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:11:48.963942 Training: [77 epoch,  50 batch] loss: 0.10863, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:13:06.200019 Training: [77 epoch,  60 batch] loss: 0.08501, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:14:20.755297 Training: [77 epoch,  70 batch] loss: 0.10276, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:15:34.778825 Training: [77 epoch,  80 batch] loss: 0.11320, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:16:48.150243 Training: [77 epoch,  90 batch] loss: 0.11550, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34791,MAE：0.14916
2021-01-11 04:20:40.520798 Training: [78 epoch,  10 batch] loss: 0.07958, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:21:59.537193 Training: [78 epoch,  20 batch] loss: 0.12217, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:23:19.778825 Training: [78 epoch,  30 batch] loss: 0.14192, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:24:40.182062 Training: [78 epoch,  40 batch] loss: 0.08176, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:25:59.720911 Training: [78 epoch,  50 batch] loss: 0.09770, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:27:19.194761 Training: [78 epoch,  60 batch] loss: 0.10080, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:28:36.768874 Training: [78 epoch,  70 batch] loss: 0.08989, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:29:54.144261 Training: [78 epoch,  80 batch] loss: 0.11745, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:31:10.597894 Training: [78 epoch,  90 batch] loss: 0.12578, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34787,MAE：0.13439
2021-01-11 04:35:03.525592 Training: [79 epoch,  10 batch] loss: 0.09119, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:36:19.949545 Training: [79 epoch,  20 batch] loss: 0.09134, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:37:38.273797 Training: [79 epoch,  30 batch] loss: 0.07066, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:38:56.576637 Training: [79 epoch,  40 batch] loss: 0.09609, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:40:15.184633 Training: [79 epoch,  50 batch] loss: 0.10057, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:41:33.784242 Training: [79 epoch,  60 batch] loss: 0.09610, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:42:51.368837 Training: [79 epoch,  70 batch] loss: 0.11008, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:44:08.149703 Training: [79 epoch,  80 batch] loss: 0.11916, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:45:24.892930 Training: [79 epoch,  90 batch] loss: 0.13498, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35082,MAE：0.15884
2021-01-11 04:49:19.172511 Training: [80 epoch,  10 batch] loss: 0.14171, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:50:36.724350 Training: [80 epoch,  20 batch] loss: 0.12245, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:51:55.261081 Training: [80 epoch,  30 batch] loss: 0.08989, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:53:14.843309 Training: [80 epoch,  40 batch] loss: 0.08773, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:54:33.712595 Training: [80 epoch,  50 batch] loss: 0.10215, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:55:52.491388 Training: [80 epoch,  60 batch] loss: 0.08098, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:57:09.721699 Training: [80 epoch,  70 batch] loss: 0.08377, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:58:27.782621 Training: [80 epoch,  80 batch] loss: 0.13618, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 04:59:45.655095 Training: [80 epoch,  90 batch] loss: 0.08879, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34783,MAE：0.14142
2021-01-11 05:03:38.355537 Training: [81 epoch,  10 batch] loss: 0.08894, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:04:55.946965 Training: [81 epoch,  20 batch] loss: 0.08449, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:06:15.962072 Training: [81 epoch,  30 batch] loss: 0.13948, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:07:36.571403 Training: [81 epoch,  40 batch] loss: 0.10108, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:08:58.006207 Training: [81 epoch,  50 batch] loss: 0.11195, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:10:18.498400 Training: [81 epoch,  60 batch] loss: 0.08069, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:11:36.491501 Training: [81 epoch,  70 batch] loss: 0.13165, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:12:53.967478 Training: [81 epoch,  80 batch] loss: 0.09569, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:14:11.114277 Training: [81 epoch,  90 batch] loss: 0.10413, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34817,MAE：0.14545
2021-01-11 05:17:57.740381 Training: [82 epoch,  10 batch] loss: 0.09790, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:19:12.726472 Training: [82 epoch,  20 batch] loss: 0.11335, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:20:29.426801 Training: [82 epoch,  30 batch] loss: 0.10072, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:21:45.957384 Training: [82 epoch,  40 batch] loss: 0.14614, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:23:02.983602 Training: [82 epoch,  50 batch] loss: 0.10342, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:24:21.039071 Training: [82 epoch,  60 batch] loss: 0.09553, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:25:39.421462 Training: [82 epoch,  70 batch] loss: 0.09816, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:26:56.343134 Training: [82 epoch,  80 batch] loss: 0.08355, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:28:15.082706 Training: [82 epoch,  90 batch] loss: 0.10719, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34876,MAE：0.15262
2021-01-11 05:32:08.454191 Training: [83 epoch,  10 batch] loss: 0.13119, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:33:25.277060 Training: [83 epoch,  20 batch] loss: 0.12384, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:34:43.479687 Training: [83 epoch,  30 batch] loss: 0.08460, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:36:02.282592 Training: [83 epoch,  40 batch] loss: 0.08443, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:37:21.729745 Training: [83 epoch,  50 batch] loss: 0.10190, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:38:42.642425 Training: [83 epoch,  60 batch] loss: 0.07456, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:40:02.518039 Training: [83 epoch,  70 batch] loss: 0.14407, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:41:21.075573 Training: [83 epoch,  80 batch] loss: 0.08791, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:42:40.297866 Training: [83 epoch,  90 batch] loss: 0.10681, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34667,MAE：0.15678
2021-01-11 05:46:33.268872 Training: [84 epoch,  10 batch] loss: 0.16175, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:47:51.854445 Training: [84 epoch,  20 batch] loss: 0.08681, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:49:10.922900 Training: [84 epoch,  30 batch] loss: 0.09049, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:50:29.967218 Training: [84 epoch,  40 batch] loss: 0.08120, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:51:49.160390 Training: [84 epoch,  50 batch] loss: 0.09868, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:53:07.523625 Training: [84 epoch,  60 batch] loss: 0.10703, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:54:26.172510 Training: [84 epoch,  70 batch] loss: 0.12407, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:55:44.685170 Training: [84 epoch,  80 batch] loss: 0.10688, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 05:57:05.875317 Training: [84 epoch,  90 batch] loss: 0.09224, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35164,MAE：0.15735
2021-01-11 06:00:35.954549 Training: [85 epoch,  10 batch] loss: 0.09338, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:01:42.469902 Training: [85 epoch,  20 batch] loss: 0.07591, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:02:52.588628 Training: [85 epoch,  30 batch] loss: 0.10032, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:04:00.199427 Training: [85 epoch,  40 batch] loss: 0.07795, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:05:07.221052 Training: [85 epoch,  50 batch] loss: 0.07859, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:06:13.997251 Training: [85 epoch,  60 batch] loss: 0.09203, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:07:24.689273 Training: [85 epoch,  70 batch] loss: 0.15881, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:08:32.861205 Training: [85 epoch,  80 batch] loss: 0.08859, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:09:41.790984 Training: [85 epoch,  90 batch] loss: 0.11739, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34831,MAE：0.13852
2021-01-11 06:13:07.930262 Training: [86 epoch,  10 batch] loss: 0.09609, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:14:12.455961 Training: [86 epoch,  20 batch] loss: 0.07629, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:15:19.693541 Training: [86 epoch,  30 batch] loss: 0.09905, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:16:26.850896 Training: [86 epoch,  40 batch] loss: 0.10763, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:17:35.233691 Training: [86 epoch,  50 batch] loss: 0.10044, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:18:42.241536 Training: [86 epoch,  60 batch] loss: 0.08078, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:19:49.348175 Training: [86 epoch,  70 batch] loss: 0.07993, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:21:01.917950 Training: [86 epoch,  80 batch] loss: 0.09725, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:22:14.937016 Training: [86 epoch,  90 batch] loss: 0.17196, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34997,MAE：0.14774
2021-01-11 06:25:31.250672 Training: [87 epoch,  10 batch] loss: 0.09435, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:26:38.615796 Training: [87 epoch,  20 batch] loss: 0.09365, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:27:45.943614 Training: [87 epoch,  30 batch] loss: 0.11231, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:28:54.268634 Training: [87 epoch,  40 batch] loss: 0.08457, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:30:06.884855 Training: [87 epoch,  50 batch] loss: 0.09475, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:31:15.827939 Training: [87 epoch,  60 batch] loss: 0.14978, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:32:22.063251 Training: [87 epoch,  70 batch] loss: 0.08928, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:33:27.971673 Training: [87 epoch,  80 batch] loss: 0.12227, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:34:34.315586 Training: [87 epoch,  90 batch] loss: 0.09479, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34840,MAE：0.13886
2021-01-11 06:37:51.367899 Training: [88 epoch,  10 batch] loss: 0.08439, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:38:57.281516 Training: [88 epoch,  20 batch] loss: 0.14081, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:40:05.023390 Training: [88 epoch,  30 batch] loss: 0.11181, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:41:13.100519 Training: [88 epoch,  40 batch] loss: 0.08407, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:42:22.413917 Training: [88 epoch,  50 batch] loss: 0.10449, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:43:33.578071 Training: [88 epoch,  60 batch] loss: 0.09093, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:44:41.588446 Training: [88 epoch,  70 batch] loss: 0.11666, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:45:49.840525 Training: [88 epoch,  80 batch] loss: 0.09028, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:46:57.663041 Training: [88 epoch,  90 batch] loss: 0.07598, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.34769,MAE：0.14507
2021-01-11 06:50:25.108336 Training: [89 epoch,  10 batch] loss: 0.10758, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:51:31.877568 Training: [89 epoch,  20 batch] loss: 0.10135, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:52:39.781508 Training: [89 epoch,  30 batch] loss: 0.07113, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:53:48.054876 Training: [89 epoch,  40 batch] loss: 0.09496, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:54:56.128908 Training: [89 epoch,  50 batch] loss: 0.12959, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:56:04.261824 Training: [89 epoch,  60 batch] loss: 0.12395, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:57:11.174711 Training: [89 epoch,  70 batch] loss: 0.10111, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:58:13.621732 Training: [89 epoch,  80 batch] loss: 0.08840, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 06:59:13.098123 Training: [89 epoch,  90 batch] loss: 0.09970, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.36031,MAE：0.15686
2021-01-11 07:02:07.808456 Training: [90 epoch,  10 batch] loss: 0.08598, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:03:06.688227 Training: [90 epoch,  20 batch] loss: 0.15073, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:04:04.963882 Training: [90 epoch,  30 batch] loss: 0.09937, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:05:03.748896 Training: [90 epoch,  40 batch] loss: 0.08395, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:06:02.534566 Training: [90 epoch,  50 batch] loss: 0.08919, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:07:01.037200 Training: [90 epoch,  60 batch] loss: 0.15138, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:07:59.584712 Training: [90 epoch,  70 batch] loss: 0.09456, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:08:58.345156 Training: [90 epoch,  80 batch] loss: 0.10206, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:09:55.196395 Training: [90 epoch,  90 batch] loss: 0.09562, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35598,MAE：0.14902
2021-01-11 07:12:15.284661 Training: [91 epoch,  10 batch] loss: 0.12851, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:12:59.454919 Training: [91 epoch,  20 batch] loss: 0.09016, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:13:42.912873 Training: [91 epoch,  30 batch] loss: 0.11891, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:14:27.785425 Training: [91 epoch,  40 batch] loss: 0.12072, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:15:12.487099 Training: [91 epoch,  50 batch] loss: 0.09066, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:15:56.772499 Training: [91 epoch,  60 batch] loss: 0.11791, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:16:41.328713 Training: [91 epoch,  70 batch] loss: 0.06405, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:17:28.546163 Training: [91 epoch,  80 batch] loss: 0.10325, the best RMSE/MAE: 0.35113 / 0.13053
2021-01-11 07:18:15.709272 Training: [91 epoch,  90 batch] loss: 0.08707, the best RMSE/MAE: 0.35113 / 0.13053
<Test> RMSE：0.35106,MAE：0.14482
The best RMSE/MAE：0.35113/0.13053
