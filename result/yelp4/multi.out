-------------------- Hyperparams --------------------
time: 2021-01-09 13:44:53.926559
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-09 13:59:45.410141 Training: [1 epoch,  10 batch] loss: 11.44110, the best RMSE/MAE: inf / inf
2021-01-09 14:00:19.619658 Training: [1 epoch,  20 batch] loss: 11.10526, the best RMSE/MAE: inf / inf
2021-01-09 14:00:53.508837 Training: [1 epoch,  30 batch] loss: 10.88403, the best RMSE/MAE: inf / inf
2021-01-09 14:01:27.452882 Training: [1 epoch,  40 batch] loss: 10.83676, the best RMSE/MAE: inf / inf
2021-01-09 14:02:02.016854 Training: [1 epoch,  50 batch] loss: 10.78185, the best RMSE/MAE: inf / inf
2021-01-09 14:02:36.085550 Training: [1 epoch,  60 batch] loss: 10.62373, the best RMSE/MAE: inf / inf
2021-01-09 14:03:10.338229 Training: [1 epoch,  70 batch] loss: 10.63189, the best RMSE/MAE: inf / inf
2021-01-09 14:03:44.560157 Training: [1 epoch,  80 batch] loss: 10.59171, the best RMSE/MAE: inf / inf
2021-01-09 14:04:20.576468 Training: [1 epoch,  90 batch] loss: 10.54557, the best RMSE/MAE: inf / inf
<Test> RMSE：81851264.00000,MAE：59958376.00000
2021-01-09 14:05:50.356954 Training: [2 epoch,  10 batch] loss: 10.49670, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:06:25.390311 Training: [2 epoch,  20 batch] loss: 10.48614, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:07:00.282039 Training: [2 epoch,  30 batch] loss: 10.44889, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:07:35.487364 Training: [2 epoch,  40 batch] loss: 10.41361, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:08:10.214360 Training: [2 epoch,  50 batch] loss: 10.41180, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:08:45.386449 Training: [2 epoch,  60 batch] loss: 10.40680, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:09:20.681460 Training: [2 epoch,  70 batch] loss: 10.35574, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:09:55.933852 Training: [2 epoch,  80 batch] loss: 10.44165, the best RMSE/MAE: 81851264.00000 / 59958376.00000
2021-01-09 14:10:39.041021 Training: [2 epoch,  90 batch] loss: 10.29828, the best RMSE/MAE: 81851264.00000 / 59958376.00000
<Test> RMSE：174035.35938,MAE：124029.21875
2021-01-09 14:12:40.617543 Training: [3 epoch,  10 batch] loss: 10.22606, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:13:28.657062 Training: [3 epoch,  20 batch] loss: 10.27568, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:14:14.269778 Training: [3 epoch,  30 batch] loss: 10.17248, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:14:54.978721 Training: [3 epoch,  40 batch] loss: 10.21734, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:15:30.903807 Training: [3 epoch,  50 batch] loss: 10.18802, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:16:05.659821 Training: [3 epoch,  60 batch] loss: 10.14037, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:16:40.738754 Training: [3 epoch,  70 batch] loss: 10.17597, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:17:15.093377 Training: [3 epoch,  80 batch] loss: 10.07368, the best RMSE/MAE: 174035.35938 / 124029.21875
2021-01-09 14:17:49.508412 Training: [3 epoch,  90 batch] loss: 10.03260, the best RMSE/MAE: 174035.35938 / 124029.21875
<Test> RMSE：4211.53760,MAE：2875.58618
2021-01-09 14:19:17.947026 Training: [4 epoch,  10 batch] loss: 9.97519, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:19:52.369996 Training: [4 epoch,  20 batch] loss: 10.01731, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:20:27.074099 Training: [4 epoch,  30 batch] loss: 10.03670, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:21:01.781136 Training: [4 epoch,  40 batch] loss: 9.91859, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:21:36.659347 Training: [4 epoch,  50 batch] loss: 9.94878, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:22:11.649390 Training: [4 epoch,  60 batch] loss: 9.88685, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:22:46.266031 Training: [4 epoch,  70 batch] loss: 9.87357, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:23:20.848431 Training: [4 epoch,  80 batch] loss: 9.82609, the best RMSE/MAE: 4211.53760 / 2875.58618
2021-01-09 14:23:55.572898 Training: [4 epoch,  90 batch] loss: 9.78206, the best RMSE/MAE: 4211.53760 / 2875.58618
<Test> RMSE：418.32825,MAE：296.80322
2021-01-09 14:25:25.626765 Training: [5 epoch,  10 batch] loss: 9.78382, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:26:00.403717 Training: [5 epoch,  20 batch] loss: 9.73839, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:26:35.306374 Training: [5 epoch,  30 batch] loss: 9.79039, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:27:10.297521 Training: [5 epoch,  40 batch] loss: 9.65688, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:27:45.433545 Training: [5 epoch,  50 batch] loss: 9.64275, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:28:20.695082 Training: [5 epoch,  60 batch] loss: 9.73912, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:28:57.294169 Training: [5 epoch,  70 batch] loss: 9.59780, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:29:32.887310 Training: [5 epoch,  80 batch] loss: 9.57134, the best RMSE/MAE: 418.32825 / 296.80322
2021-01-09 14:30:08.564246 Training: [5 epoch,  90 batch] loss: 9.57735, the best RMSE/MAE: 418.32825 / 296.80322
<Test> RMSE：57.55256,MAE：43.14963
2021-01-09 14:32:14.550133 Training: [6 epoch,  10 batch] loss: 9.54201, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:33:03.310547 Training: [6 epoch,  20 batch] loss: 9.51081, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:33:52.243366 Training: [6 epoch,  30 batch] loss: 9.48995, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:34:40.515612 Training: [6 epoch,  40 batch] loss: 9.42198, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:35:29.242690 Training: [6 epoch,  50 batch] loss: 9.37646, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:36:18.702207 Training: [6 epoch,  60 batch] loss: 9.44829, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:37:08.695682 Training: [6 epoch,  70 batch] loss: 9.35865, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:37:57.751026 Training: [6 epoch,  80 batch] loss: 9.32770, the best RMSE/MAE: 57.55256 / 43.14963
2021-01-09 14:38:46.621606 Training: [6 epoch,  90 batch] loss: 9.31364, the best RMSE/MAE: 57.55256 / 43.14963
<Test> RMSE：12.94111,MAE：10.03138
2021-01-09 14:41:00.843121 Training: [7 epoch,  10 batch] loss: 9.24619, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:41:48.574298 Training: [7 epoch,  20 batch] loss: 9.20813, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:42:36.747241 Training: [7 epoch,  30 batch] loss: 9.19117, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:43:25.246484 Training: [7 epoch,  40 batch] loss: 9.17011, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:44:13.738506 Training: [7 epoch,  50 batch] loss: 9.13266, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:45:01.809667 Training: [7 epoch,  60 batch] loss: 9.14049, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:45:51.111790 Training: [7 epoch,  70 batch] loss: 9.04390, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:46:39.367836 Training: [7 epoch,  80 batch] loss: 9.03756, the best RMSE/MAE: 12.94111 / 10.03138
2021-01-09 14:47:27.478575 Training: [7 epoch,  90 batch] loss: 9.07962, the best RMSE/MAE: 12.94111 / 10.03138
<Test> RMSE：5.64941,MAE：4.52521
2021-01-09 14:49:38.643176 Training: [8 epoch,  10 batch] loss: 8.95064, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:50:26.943179 Training: [8 epoch,  20 batch] loss: 8.94290, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:51:15.673298 Training: [8 epoch,  30 batch] loss: 8.97176, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:52:04.574586 Training: [8 epoch,  40 batch] loss: 8.87027, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:52:53.896269 Training: [8 epoch,  50 batch] loss: 8.82617, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:53:42.410550 Training: [8 epoch,  60 batch] loss: 8.77944, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:54:31.195438 Training: [8 epoch,  70 batch] loss: 8.75713, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:55:20.305828 Training: [8 epoch,  80 batch] loss: 8.75976, the best RMSE/MAE: 5.64941 / 4.52521
2021-01-09 14:56:08.571691 Training: [8 epoch,  90 batch] loss: 8.79675, the best RMSE/MAE: 5.64941 / 4.52521
<Test> RMSE：1.81815,MAE：1.39365
2021-01-09 14:58:21.622733 Training: [9 epoch,  10 batch] loss: 8.68056, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 14:59:09.417160 Training: [9 epoch,  20 batch] loss: 8.61532, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 14:59:57.869140 Training: [9 epoch,  30 batch] loss: 8.64482, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:00:46.172419 Training: [9 epoch,  40 batch] loss: 8.58124, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:01:34.018272 Training: [9 epoch,  50 batch] loss: 8.50150, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:02:21.442020 Training: [9 epoch,  60 batch] loss: 8.49361, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:03:07.779701 Training: [9 epoch,  70 batch] loss: 8.47293, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:03:54.288565 Training: [9 epoch,  80 batch] loss: 8.52916, the best RMSE/MAE: 1.81815 / 1.39365
2021-01-09 15:04:40.673128 Training: [9 epoch,  90 batch] loss: 8.37577, the best RMSE/MAE: 1.81815 / 1.39365
<Test> RMSE：1.25143,MAE：0.99980
2021-01-09 15:06:48.517716 Training: [10 epoch,  10 batch] loss: 8.31871, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:07:35.356157 Training: [10 epoch,  20 batch] loss: 8.29840, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:08:23.106151 Training: [10 epoch,  30 batch] loss: 8.23697, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:09:11.265876 Training: [10 epoch,  40 batch] loss: 8.24194, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:09:59.203297 Training: [10 epoch,  50 batch] loss: 8.21164, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:10:47.158704 Training: [10 epoch,  60 batch] loss: 8.25513, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:11:35.187966 Training: [10 epoch,  70 batch] loss: 8.15063, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:12:22.865245 Training: [10 epoch,  80 batch] loss: 8.12249, the best RMSE/MAE: 1.25143 / 0.99980
2021-01-09 15:13:10.677319 Training: [10 epoch,  90 batch] loss: 8.12195, the best RMSE/MAE: 1.25143 / 0.99980
<Test> RMSE：0.97020,MAE：0.84768
2021-01-09 15:15:38.298475 Training: [11 epoch,  10 batch] loss: 8.02327, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:16:39.431745 Training: [11 epoch,  20 batch] loss: 7.97140, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:17:42.931463 Training: [11 epoch,  30 batch] loss: 7.96007, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:18:45.639084 Training: [11 epoch,  40 batch] loss: 7.87163, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:19:51.735831 Training: [11 epoch,  50 batch] loss: 7.85640, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:20:54.468626 Training: [11 epoch,  60 batch] loss: 7.87676, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:21:58.106079 Training: [11 epoch,  70 batch] loss: 7.85203, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:23:03.077511 Training: [11 epoch,  80 batch] loss: 7.76685, the best RMSE/MAE: 0.97020 / 0.84768
2021-01-09 15:24:12.577602 Training: [11 epoch,  90 batch] loss: 7.81206, the best RMSE/MAE: 0.97020 / 0.84768
<Test> RMSE：0.70944,MAE：0.62098
2021-01-09 15:27:00.639874 Training: [12 epoch,  10 batch] loss: 7.71195, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:28:00.636344 Training: [12 epoch,  20 batch] loss: 7.66655, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:28:49.465439 Training: [12 epoch,  30 batch] loss: 7.61141, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:29:37.083364 Training: [12 epoch,  40 batch] loss: 7.57557, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:30:25.192160 Training: [12 epoch,  50 batch] loss: 7.53171, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:31:13.232265 Training: [12 epoch,  60 batch] loss: 7.60800, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:32:02.101580 Training: [12 epoch,  70 batch] loss: 7.43310, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:32:50.370082 Training: [12 epoch,  80 batch] loss: 7.40322, the best RMSE/MAE: 0.70944 / 0.62098
2021-01-09 15:33:40.256182 Training: [12 epoch,  90 batch] loss: 7.37353, the best RMSE/MAE: 0.70944 / 0.62098
<Test> RMSE：0.56989,MAE：0.50680
2021-01-09 15:36:07.207314 Training: [13 epoch,  10 batch] loss: 7.35945, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:36:55.043300 Training: [13 epoch,  20 batch] loss: 7.37120, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:37:42.856143 Training: [13 epoch,  30 batch] loss: 7.32949, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:38:30.670153 Training: [13 epoch,  40 batch] loss: 7.20399, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:39:18.474338 Training: [13 epoch,  50 batch] loss: 7.16255, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:40:15.319146 Training: [13 epoch,  60 batch] loss: 7.13756, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:41:11.578810 Training: [13 epoch,  70 batch] loss: 7.09131, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:42:13.765676 Training: [13 epoch,  80 batch] loss: 7.08427, the best RMSE/MAE: 0.56989 / 0.50680
2021-01-09 15:43:18.745060 Training: [13 epoch,  90 batch] loss: 7.06229, the best RMSE/MAE: 0.56989 / 0.50680
<Test> RMSE：0.49374,MAE：0.44608
2021-01-09 15:46:07.041244 Training: [14 epoch,  10 batch] loss: 7.01260, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:47:13.088567 Training: [14 epoch,  20 batch] loss: 6.98410, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:48:16.580402 Training: [14 epoch,  30 batch] loss: 6.93229, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:49:19.191623 Training: [14 epoch,  40 batch] loss: 6.94088, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:50:21.484080 Training: [14 epoch,  50 batch] loss: 6.83556, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:51:29.314590 Training: [14 epoch,  60 batch] loss: 6.77281, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:52:35.200401 Training: [14 epoch,  70 batch] loss: 6.78708, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:53:42.089040 Training: [14 epoch,  80 batch] loss: 6.72182, the best RMSE/MAE: 0.49374 / 0.44608
2021-01-09 15:54:45.995248 Training: [14 epoch,  90 batch] loss: 6.68777, the best RMSE/MAE: 0.49374 / 0.44608
<Test> RMSE：0.41133,MAE：0.33113
2021-01-09 15:57:13.688578 Training: [15 epoch,  10 batch] loss: 6.65700, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 15:58:11.992207 Training: [15 epoch,  20 batch] loss: 6.60923, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 15:59:10.645458 Training: [15 epoch,  30 batch] loss: 6.62869, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:00:09.391776 Training: [15 epoch,  40 batch] loss: 6.51603, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:01:10.890432 Training: [15 epoch,  50 batch] loss: 6.47475, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:02:11.508311 Training: [15 epoch,  60 batch] loss: 6.45389, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:03:10.401607 Training: [15 epoch,  70 batch] loss: 6.42397, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:03:57.724814 Training: [15 epoch,  80 batch] loss: 6.41104, the best RMSE/MAE: 0.41133 / 0.33113
2021-01-09 16:04:45.248046 Training: [15 epoch,  90 batch] loss: 6.32998, the best RMSE/MAE: 0.41133 / 0.33113
<Test> RMSE：0.39277,MAE：0.31017
2021-01-09 16:06:56.367311 Training: [16 epoch,  10 batch] loss: 6.29424, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:07:44.667186 Training: [16 epoch,  20 batch] loss: 6.25855, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:08:36.958288 Training: [16 epoch,  30 batch] loss: 6.27684, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:09:34.480191 Training: [16 epoch,  40 batch] loss: 6.19925, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:10:32.537730 Training: [16 epoch,  50 batch] loss: 6.14912, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:11:31.215786 Training: [16 epoch,  60 batch] loss: 6.09385, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:12:30.007746 Training: [16 epoch,  70 batch] loss: 6.14724, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:13:28.645249 Training: [16 epoch,  80 batch] loss: 6.04527, the best RMSE/MAE: 0.39277 / 0.31017
2021-01-09 16:14:19.708407 Training: [16 epoch,  90 batch] loss: 6.03521, the best RMSE/MAE: 0.39277 / 0.31017
<Test> RMSE：0.35584,MAE：0.21804
2021-01-09 16:16:56.497595 Training: [17 epoch,  10 batch] loss: 6.00160, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:17:47.818352 Training: [17 epoch,  20 batch] loss: 5.89420, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:18:33.739344 Training: [17 epoch,  30 batch] loss: 5.88506, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:19:18.068495 Training: [17 epoch,  40 batch] loss: 5.85800, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:20:04.661746 Training: [17 epoch,  50 batch] loss: 5.86777, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:20:47.517813 Training: [17 epoch,  60 batch] loss: 5.78988, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:21:21.154804 Training: [17 epoch,  70 batch] loss: 5.76314, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:21:55.114119 Training: [17 epoch,  80 batch] loss: 5.74589, the best RMSE/MAE: 0.35584 / 0.21804
2021-01-09 16:22:29.160656 Training: [17 epoch,  90 batch] loss: 5.67707, the best RMSE/MAE: 0.35584 / 0.21804
<Test> RMSE：0.35243,MAE：0.18895
2021-01-09 16:23:57.057281 Training: [18 epoch,  10 batch] loss: 5.62233, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:24:30.666865 Training: [18 epoch,  20 batch] loss: 5.58204, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:25:04.615986 Training: [18 epoch,  30 batch] loss: 5.56967, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:25:38.389713 Training: [18 epoch,  40 batch] loss: 5.59974, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:26:12.442165 Training: [18 epoch,  50 batch] loss: 5.52025, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:26:46.448550 Training: [18 epoch,  60 batch] loss: 5.45453, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:27:20.434801 Training: [18 epoch,  70 batch] loss: 5.42842, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:27:54.817481 Training: [18 epoch,  80 batch] loss: 5.40951, the best RMSE/MAE: 0.35243 / 0.18895
2021-01-09 16:28:29.463817 Training: [18 epoch,  90 batch] loss: 5.38500, the best RMSE/MAE: 0.35243 / 0.18895
<Test> RMSE：0.34624,MAE：0.16680
2021-01-09 16:29:56.225045 Training: [19 epoch,  10 batch] loss: 5.30724, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:30:30.394835 Training: [19 epoch,  20 batch] loss: 5.30885, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:31:04.483300 Training: [19 epoch,  30 batch] loss: 5.32602, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:31:38.474352 Training: [19 epoch,  40 batch] loss: 5.22570, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:32:12.507040 Training: [19 epoch,  50 batch] loss: 5.18777, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:32:55.355631 Training: [19 epoch,  60 batch] loss: 5.13607, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:33:45.299993 Training: [19 epoch,  70 batch] loss: 5.10680, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:34:30.234934 Training: [19 epoch,  80 batch] loss: 5.12529, the best RMSE/MAE: 0.34624 / 0.16680
2021-01-09 16:35:02.416452 Training: [19 epoch,  90 batch] loss: 5.04691, the best RMSE/MAE: 0.34624 / 0.16680
<Test> RMSE：0.34693,MAE：0.15211
2021-01-09 16:36:25.781171 Training: [20 epoch,  10 batch] loss: 5.01829, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:36:58.162098 Training: [20 epoch,  20 batch] loss: 4.95751, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:37:30.615169 Training: [20 epoch,  30 batch] loss: 5.01828, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:38:03.165826 Training: [20 epoch,  40 batch] loss: 4.91112, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:38:36.106181 Training: [20 epoch,  50 batch] loss: 4.87694, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:39:08.911754 Training: [20 epoch,  60 batch] loss: 4.89878, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:39:41.983823 Training: [20 epoch,  70 batch] loss: 4.81601, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:40:15.096253 Training: [20 epoch,  80 batch] loss: 4.82831, the best RMSE/MAE: 0.34693 / 0.15211
2021-01-09 16:40:48.113750 Training: [20 epoch,  90 batch] loss: 4.76627, the best RMSE/MAE: 0.34693 / 0.15211
<Test> RMSE：0.35448,MAE：0.13259
2021-01-09 16:42:15.063536 Training: [21 epoch,  10 batch] loss: 4.70984, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:42:47.868113 Training: [21 epoch,  20 batch] loss: 4.77467, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:43:21.060219 Training: [21 epoch,  30 batch] loss: 4.64171, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:43:55.116409 Training: [21 epoch,  40 batch] loss: 4.61281, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:44:29.321022 Training: [21 epoch,  50 batch] loss: 4.57600, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:45:03.365203 Training: [21 epoch,  60 batch] loss: 4.63592, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:45:37.239808 Training: [21 epoch,  70 batch] loss: 4.56537, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:46:11.663213 Training: [21 epoch,  80 batch] loss: 4.50897, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:46:46.566068 Training: [21 epoch,  90 batch] loss: 4.49491, the best RMSE/MAE: 0.35448 / 0.13259
<Test> RMSE：0.35321,MAE：0.13845
2021-01-09 16:48:17.690032 Training: [22 epoch,  10 batch] loss: 4.41756, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:48:51.513897 Training: [22 epoch,  20 batch] loss: 4.42821, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:49:25.334226 Training: [22 epoch,  30 batch] loss: 4.48194, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:49:58.766388 Training: [22 epoch,  40 batch] loss: 4.38126, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:50:32.591245 Training: [22 epoch,  50 batch] loss: 4.32223, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:51:06.306607 Training: [22 epoch,  60 batch] loss: 4.28256, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:51:40.381109 Training: [22 epoch,  70 batch] loss: 4.25243, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:52:14.413674 Training: [22 epoch,  80 batch] loss: 4.28144, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:52:48.489164 Training: [22 epoch,  90 batch] loss: 4.22267, the best RMSE/MAE: 0.35448 / 0.13259
<Test> RMSE：0.36506,MAE：0.14617
2021-01-09 16:54:16.219080 Training: [23 epoch,  10 batch] loss: 4.26376, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:54:50.153447 Training: [23 epoch,  20 batch] loss: 4.14574, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:55:24.200063 Training: [23 epoch,  30 batch] loss: 4.13091, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:55:58.387234 Training: [23 epoch,  40 batch] loss: 4.09573, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:56:32.065439 Training: [23 epoch,  50 batch] loss: 4.08450, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:57:05.937705 Training: [23 epoch,  60 batch] loss: 4.02314, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:57:40.263725 Training: [23 epoch,  70 batch] loss: 4.03668, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:58:25.795313 Training: [23 epoch,  80 batch] loss: 3.97370, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 16:59:13.025023 Training: [23 epoch,  90 batch] loss: 3.97008, the best RMSE/MAE: 0.35448 / 0.13259
<Test> RMSE：0.36276,MAE：0.14569
2021-01-09 17:01:21.761086 Training: [24 epoch,  10 batch] loss: 3.94951, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:02:08.408587 Training: [24 epoch,  20 batch] loss: 3.92925, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:02:54.820848 Training: [24 epoch,  30 batch] loss: 3.93871, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:03:41.594490 Training: [24 epoch,  40 batch] loss: 3.83669, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:04:27.934420 Training: [24 epoch,  50 batch] loss: 3.85061, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:05:14.865647 Training: [24 epoch,  60 batch] loss: 3.80661, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:06:02.198872 Training: [24 epoch,  70 batch] loss: 3.77486, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:06:49.712404 Training: [24 epoch,  80 batch] loss: 3.73212, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:07:36.802466 Training: [24 epoch,  90 batch] loss: 3.82311, the best RMSE/MAE: 0.35448 / 0.13259
<Test> RMSE：0.36271,MAE：0.14519
2021-01-09 17:09:44.693507 Training: [25 epoch,  10 batch] loss: 3.66988, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:10:31.184886 Training: [25 epoch,  20 batch] loss: 3.67462, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:11:16.074750 Training: [25 epoch,  30 batch] loss: 3.70256, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:12:02.144850 Training: [25 epoch,  40 batch] loss: 3.64281, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:12:49.249273 Training: [25 epoch,  50 batch] loss: 3.59953, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:13:36.340336 Training: [25 epoch,  60 batch] loss: 3.56510, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:14:24.010393 Training: [25 epoch,  70 batch] loss: 3.58798, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:15:11.458961 Training: [25 epoch,  80 batch] loss: 3.61208, the best RMSE/MAE: 0.35448 / 0.13259
2021-01-09 17:15:58.865085 Training: [25 epoch,  90 batch] loss: 3.52831, the best RMSE/MAE: 0.35448 / 0.13259
<Test> RMSE：0.36200,MAE：0.12359
2021-01-09 17:18:06.266031 Training: [26 epoch,  10 batch] loss: 3.47790, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:18:52.296869 Training: [26 epoch,  20 batch] loss: 3.43821, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:19:39.210023 Training: [26 epoch,  30 batch] loss: 3.41767, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:20:26.178547 Training: [26 epoch,  40 batch] loss: 3.40291, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:21:13.506805 Training: [26 epoch,  50 batch] loss: 3.41645, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:22:00.750363 Training: [26 epoch,  60 batch] loss: 3.35202, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:22:48.489470 Training: [26 epoch,  70 batch] loss: 3.37880, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:23:35.937890 Training: [26 epoch,  80 batch] loss: 3.34109, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:24:23.272242 Training: [26 epoch,  90 batch] loss: 3.34768, the best RMSE/MAE: 0.36200 / 0.12359
<Test> RMSE：0.36042,MAE：0.12551
2021-01-09 17:26:32.205296 Training: [27 epoch,  10 batch] loss: 3.29451, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:27:18.885527 Training: [27 epoch,  20 batch] loss: 3.24902, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:28:05.905180 Training: [27 epoch,  30 batch] loss: 3.21931, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:28:53.456490 Training: [27 epoch,  40 batch] loss: 3.20767, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:29:40.894616 Training: [27 epoch,  50 batch] loss: 3.19604, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:30:27.592282 Training: [27 epoch,  60 batch] loss: 3.20674, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:31:14.583080 Training: [27 epoch,  70 batch] loss: 3.18423, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:32:00.747614 Training: [27 epoch,  80 batch] loss: 3.13773, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:32:49.193497 Training: [27 epoch,  90 batch] loss: 3.10492, the best RMSE/MAE: 0.36200 / 0.12359
<Test> RMSE：0.36393,MAE：0.12962
2021-01-09 17:35:02.043468 Training: [28 epoch,  10 batch] loss: 3.09473, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:35:50.285406 Training: [28 epoch,  20 batch] loss: 3.07356, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:36:38.705108 Training: [28 epoch,  30 batch] loss: 3.05777, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:37:27.202266 Training: [28 epoch,  40 batch] loss: 3.02532, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:38:16.446744 Training: [28 epoch,  50 batch] loss: 3.01800, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:39:03.719546 Training: [28 epoch,  60 batch] loss: 2.99104, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:39:53.548807 Training: [28 epoch,  70 batch] loss: 2.99278, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:40:42.765814 Training: [28 epoch,  80 batch] loss: 2.95222, the best RMSE/MAE: 0.36200 / 0.12359
2021-01-09 17:41:31.335228 Training: [28 epoch,  90 batch] loss: 2.95310, the best RMSE/MAE: 0.36200 / 0.12359
<Test> RMSE：0.36188,MAE：0.11756
2021-01-09 17:43:45.486190 Training: [29 epoch,  10 batch] loss: 2.99456, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:44:34.766697 Training: [29 epoch,  20 batch] loss: 2.90513, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:45:23.711009 Training: [29 epoch,  30 batch] loss: 2.86161, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:46:11.488651 Training: [29 epoch,  40 batch] loss: 2.86721, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:47:00.062592 Training: [29 epoch,  50 batch] loss: 2.83294, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:47:48.599507 Training: [29 epoch,  60 batch] loss: 2.83192, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:48:37.722277 Training: [29 epoch,  70 batch] loss: 2.81674, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:49:26.734624 Training: [29 epoch,  80 batch] loss: 2.78779, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:50:15.569226 Training: [29 epoch,  90 batch] loss: 2.77712, the best RMSE/MAE: 0.36188 / 0.11756
<Test> RMSE：0.34978,MAE：0.14278
2021-01-09 17:52:28.955820 Training: [30 epoch,  10 batch] loss: 2.76556, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:53:15.682898 Training: [30 epoch,  20 batch] loss: 2.75454, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:54:03.992971 Training: [30 epoch,  30 batch] loss: 2.72583, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:54:51.830856 Training: [30 epoch,  40 batch] loss: 2.69338, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:55:40.449403 Training: [30 epoch,  50 batch] loss: 2.68273, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:56:29.272907 Training: [30 epoch,  60 batch] loss: 2.65824, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:57:17.983555 Training: [30 epoch,  70 batch] loss: 2.66620, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:58:06.615647 Training: [30 epoch,  80 batch] loss: 2.63408, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 17:58:55.626564 Training: [30 epoch,  90 batch] loss: 2.68216, the best RMSE/MAE: 0.36188 / 0.11756
<Test> RMSE：0.35124,MAE：0.12911
2021-01-09 18:01:07.019853 Training: [31 epoch,  10 batch] loss: 2.58397, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:01:55.346021 Training: [31 epoch,  20 batch] loss: 2.62568, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:02:43.937218 Training: [31 epoch,  30 batch] loss: 2.54725, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:03:32.363549 Training: [31 epoch,  40 batch] loss: 2.55252, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:04:21.276144 Training: [31 epoch,  50 batch] loss: 2.58224, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:05:10.105533 Training: [31 epoch,  60 batch] loss: 2.52389, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:05:59.227499 Training: [31 epoch,  70 batch] loss: 2.49488, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:06:47.167119 Training: [31 epoch,  80 batch] loss: 2.50670, the best RMSE/MAE: 0.36188 / 0.11756
2021-01-09 18:07:34.965246 Training: [31 epoch,  90 batch] loss: 2.45082, the best RMSE/MAE: 0.36188 / 0.11756
<Test> RMSE：0.35337,MAE：0.10628
2021-01-09 18:09:49.311309 Training: [32 epoch,  10 batch] loss: 2.42199, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:10:37.832762 Training: [32 epoch,  20 batch] loss: 2.42630, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:11:26.750240 Training: [32 epoch,  30 batch] loss: 2.43015, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:12:15.698249 Training: [32 epoch,  40 batch] loss: 2.38606, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:13:04.337092 Training: [32 epoch,  50 batch] loss: 2.37752, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:13:52.102188 Training: [32 epoch,  60 batch] loss: 2.42871, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:14:39.514403 Training: [32 epoch,  70 batch] loss: 2.36612, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:15:27.682491 Training: [32 epoch,  80 batch] loss: 2.40134, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:16:16.255950 Training: [32 epoch,  90 batch] loss: 2.40225, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35354,MAE：0.10731
2021-01-09 18:18:28.294142 Training: [33 epoch,  10 batch] loss: 2.32553, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:19:16.534509 Training: [33 epoch,  20 batch] loss: 2.31184, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:20:04.969974 Training: [33 epoch,  30 batch] loss: 2.30400, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:20:53.069787 Training: [33 epoch,  40 batch] loss: 2.27224, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:21:40.550525 Training: [33 epoch,  50 batch] loss: 2.26950, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:22:29.125328 Training: [33 epoch,  60 batch] loss: 2.27505, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:23:17.862495 Training: [33 epoch,  70 batch] loss: 2.21832, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:24:06.239529 Training: [33 epoch,  80 batch] loss: 2.21326, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:24:54.717531 Training: [33 epoch,  90 batch] loss: 2.27871, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34734,MAE：0.16401
2021-01-09 18:27:09.082874 Training: [34 epoch,  10 batch] loss: 2.17556, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:27:56.992992 Training: [34 epoch,  20 batch] loss: 2.16929, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:28:44.749095 Training: [34 epoch,  30 batch] loss: 2.13546, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:29:33.598867 Training: [34 epoch,  40 batch] loss: 2.16485, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:30:22.433325 Training: [34 epoch,  50 batch] loss: 2.16043, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:31:11.358756 Training: [34 epoch,  60 batch] loss: 2.21084, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:32:00.444835 Training: [34 epoch,  70 batch] loss: 2.14196, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:32:49.480463 Training: [34 epoch,  80 batch] loss: 2.12386, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:33:39.023571 Training: [34 epoch,  90 batch] loss: 2.09732, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34737,MAE：0.16915
2021-01-09 18:35:50.457089 Training: [35 epoch,  10 batch] loss: 2.10316, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:36:39.519201 Training: [35 epoch,  20 batch] loss: 2.09706, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:37:29.428129 Training: [35 epoch,  30 batch] loss: 2.02811, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:38:18.450574 Training: [35 epoch,  40 batch] loss: 2.05895, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:39:07.557209 Training: [35 epoch,  50 batch] loss: 2.03533, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:39:56.827881 Training: [35 epoch,  60 batch] loss: 2.00444, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:40:46.138916 Training: [35 epoch,  70 batch] loss: 1.99307, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:41:35.411549 Training: [35 epoch,  80 batch] loss: 2.00540, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:42:24.106596 Training: [35 epoch,  90 batch] loss: 2.03702, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34786,MAE：0.14633
2021-01-09 18:44:37.099319 Training: [36 epoch,  10 batch] loss: 1.93826, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:45:26.206530 Training: [36 epoch,  20 batch] loss: 1.96697, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:46:15.141114 Training: [36 epoch,  30 batch] loss: 1.97828, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:47:04.582403 Training: [36 epoch,  40 batch] loss: 1.93724, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:47:53.538103 Training: [36 epoch,  50 batch] loss: 1.90303, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:48:41.960849 Training: [36 epoch,  60 batch] loss: 1.96813, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:49:30.271406 Training: [36 epoch,  70 batch] loss: 1.90074, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:50:17.073368 Training: [36 epoch,  80 batch] loss: 1.90669, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:51:08.474545 Training: [36 epoch,  90 batch] loss: 1.90127, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34846,MAE：0.13858
2021-01-09 18:53:54.260432 Training: [37 epoch,  10 batch] loss: 1.85135, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:54:55.885005 Training: [37 epoch,  20 batch] loss: 1.85187, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:55:58.900783 Training: [37 epoch,  30 batch] loss: 1.86411, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:56:58.491006 Training: [37 epoch,  40 batch] loss: 1.91592, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:58:01.808620 Training: [37 epoch,  50 batch] loss: 1.84980, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 18:59:02.913607 Training: [37 epoch,  60 batch] loss: 1.79613, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:00:06.945312 Training: [37 epoch,  70 batch] loss: 1.81577, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:01:20.574739 Training: [37 epoch,  80 batch] loss: 1.79104, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:02:27.375015 Training: [37 epoch,  90 batch] loss: 1.77439, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34742,MAE：0.17272
2021-01-09 19:05:09.519550 Training: [38 epoch,  10 batch] loss: 1.77151, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:06:07.876161 Training: [38 epoch,  20 batch] loss: 1.75987, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:07:06.401752 Training: [38 epoch,  30 batch] loss: 1.74996, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:08:04.467976 Training: [38 epoch,  40 batch] loss: 1.75626, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:09:01.746200 Training: [38 epoch,  50 batch] loss: 1.80648, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:10:00.110640 Training: [38 epoch,  60 batch] loss: 1.72896, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:11:01.056827 Training: [38 epoch,  70 batch] loss: 1.69037, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:12:01.365757 Training: [38 epoch,  80 batch] loss: 1.66694, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:13:01.428387 Training: [38 epoch,  90 batch] loss: 1.74665, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34774,MAE：0.14766
2021-01-09 19:15:44.817603 Training: [39 epoch,  10 batch] loss: 1.67017, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:16:44.141598 Training: [39 epoch,  20 batch] loss: 1.69518, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:17:42.031760 Training: [39 epoch,  30 batch] loss: 1.63495, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:18:39.113461 Training: [39 epoch,  40 batch] loss: 1.67727, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:19:38.019562 Training: [39 epoch,  50 batch] loss: 1.64758, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:20:36.939309 Training: [39 epoch,  60 batch] loss: 1.64855, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:21:35.870974 Training: [39 epoch,  70 batch] loss: 1.68139, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:22:35.055631 Training: [39 epoch,  80 batch] loss: 1.64096, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:23:34.402598 Training: [39 epoch,  90 batch] loss: 1.58820, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34755,MAE：0.17597
2021-01-09 19:26:15.562132 Training: [40 epoch,  10 batch] loss: 1.57339, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:27:14.257790 Training: [40 epoch,  20 batch] loss: 1.58385, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:28:12.464765 Training: [40 epoch,  30 batch] loss: 1.56216, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:29:12.416615 Training: [40 epoch,  40 batch] loss: 1.59440, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:30:12.052685 Training: [40 epoch,  50 batch] loss: 1.56258, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:31:11.816978 Training: [40 epoch,  60 batch] loss: 1.58480, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:32:11.087709 Training: [40 epoch,  70 batch] loss: 1.64046, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:33:10.932072 Training: [40 epoch,  80 batch] loss: 1.52197, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:34:10.524214 Training: [40 epoch,  90 batch] loss: 1.54009, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34797,MAE：0.14458
2021-01-09 19:36:46.393061 Training: [41 epoch,  10 batch] loss: 1.50027, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:37:42.638657 Training: [41 epoch,  20 batch] loss: 1.51221, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:38:39.906089 Training: [41 epoch,  30 batch] loss: 1.50055, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:39:37.533528 Training: [41 epoch,  40 batch] loss: 1.50181, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:40:35.115083 Training: [41 epoch,  50 batch] loss: 1.59449, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:41:32.553830 Training: [41 epoch,  60 batch] loss: 1.51389, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:42:30.569771 Training: [41 epoch,  70 batch] loss: 1.45297, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:43:29.187038 Training: [41 epoch,  80 batch] loss: 1.44664, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:44:25.949127 Training: [41 epoch,  90 batch] loss: 1.43766, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34924,MAE：0.13169
2021-01-09 19:47:02.266522 Training: [42 epoch,  10 batch] loss: 1.46836, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:47:58.897755 Training: [42 epoch,  20 batch] loss: 1.43421, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:48:56.530342 Training: [42 epoch,  30 batch] loss: 1.44177, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:49:54.049977 Training: [42 epoch,  40 batch] loss: 1.46352, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:50:51.890536 Training: [42 epoch,  50 batch] loss: 1.40144, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:51:50.382142 Training: [42 epoch,  60 batch] loss: 1.37835, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:52:47.925332 Training: [42 epoch,  70 batch] loss: 1.38930, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:53:45.131016 Training: [42 epoch,  80 batch] loss: 1.41593, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:54:43.466979 Training: [42 epoch,  90 batch] loss: 1.43538, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34766,MAE：0.17735
2021-01-09 19:57:18.696044 Training: [43 epoch,  10 batch] loss: 1.35993, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:58:15.953178 Training: [43 epoch,  20 batch] loss: 1.36959, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 19:59:13.389564 Training: [43 epoch,  30 batch] loss: 1.39164, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:00:11.103602 Training: [43 epoch,  40 batch] loss: 1.41051, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:01:08.681398 Training: [43 epoch,  50 batch] loss: 1.36074, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:02:05.078730 Training: [43 epoch,  60 batch] loss: 1.35158, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:03:03.141591 Training: [43 epoch,  70 batch] loss: 1.30331, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:04:00.783244 Training: [43 epoch,  80 batch] loss: 1.38196, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:04:57.362779 Training: [43 epoch,  90 batch] loss: 1.32546, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34828,MAE：0.18615
2021-01-09 20:07:34.374696 Training: [44 epoch,  10 batch] loss: 1.30446, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:08:32.443308 Training: [44 epoch,  20 batch] loss: 1.30207, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:09:34.381824 Training: [44 epoch,  30 batch] loss: 1.28222, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:10:44.803290 Training: [44 epoch,  40 batch] loss: 1.38861, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:11:57.269728 Training: [44 epoch,  50 batch] loss: 1.29139, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:13:10.991538 Training: [44 epoch,  60 batch] loss: 1.28052, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:14:20.556043 Training: [44 epoch,  70 batch] loss: 1.27171, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:15:32.927273 Training: [44 epoch,  80 batch] loss: 1.25499, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:16:46.225467 Training: [44 epoch,  90 batch] loss: 1.27976, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34962,MAE：0.19816
2021-01-09 20:19:51.242173 Training: [45 epoch,  10 batch] loss: 1.33278, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:21:03.720865 Training: [45 epoch,  20 batch] loss: 1.22621, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:22:13.108936 Training: [45 epoch,  30 batch] loss: 1.23482, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:23:22.577253 Training: [45 epoch,  40 batch] loss: 1.22020, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:24:33.432476 Training: [45 epoch,  50 batch] loss: 1.29090, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:25:48.070467 Training: [45 epoch,  60 batch] loss: 1.21000, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:26:55.330790 Training: [45 epoch,  70 batch] loss: 1.21076, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:28:05.804926 Training: [45 epoch,  80 batch] loss: 1.22846, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:29:17.897749 Training: [45 epoch,  90 batch] loss: 1.17338, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.34736,MAE：0.17055
2021-01-09 20:32:23.427085 Training: [46 epoch,  10 batch] loss: 1.21470, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:33:32.829166 Training: [46 epoch,  20 batch] loss: 1.17060, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:34:41.102112 Training: [46 epoch,  30 batch] loss: 1.19304, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:35:48.838949 Training: [46 epoch,  40 batch] loss: 1.24288, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:36:56.530519 Training: [46 epoch,  50 batch] loss: 1.17308, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:38:06.665419 Training: [46 epoch,  60 batch] loss: 1.20356, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:39:15.935208 Training: [46 epoch,  70 batch] loss: 1.14218, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:40:31.042666 Training: [46 epoch,  80 batch] loss: 1.18054, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:41:40.102201 Training: [46 epoch,  90 batch] loss: 1.13774, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35037,MAE：0.20326
2021-01-09 20:44:47.880110 Training: [47 epoch,  10 batch] loss: 1.11162, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:45:54.460500 Training: [47 epoch,  20 batch] loss: 1.11904, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:47:05.334620 Training: [47 epoch,  30 batch] loss: 1.12067, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:48:14.349768 Training: [47 epoch,  40 batch] loss: 1.10743, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:49:22.896874 Training: [47 epoch,  50 batch] loss: 1.14697, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:50:29.868412 Training: [47 epoch,  60 batch] loss: 1.12520, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:51:37.832719 Training: [47 epoch,  70 batch] loss: 1.13231, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:52:46.762641 Training: [47 epoch,  80 batch] loss: 1.16186, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:53:57.341378 Training: [47 epoch,  90 batch] loss: 1.12558, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35391,MAE：0.22171
2021-01-09 20:56:59.108668 Training: [48 epoch,  10 batch] loss: 1.08752, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:58:06.836675 Training: [48 epoch,  20 batch] loss: 1.05094, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 20:59:13.539770 Training: [48 epoch,  30 batch] loss: 1.08561, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:00:23.037801 Training: [48 epoch,  40 batch] loss: 1.14266, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:01:29.784966 Training: [48 epoch,  50 batch] loss: 1.06384, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:02:38.428476 Training: [48 epoch,  60 batch] loss: 1.07046, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:03:51.050044 Training: [48 epoch,  70 batch] loss: 1.12364, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:05:02.476543 Training: [48 epoch,  80 batch] loss: 1.06371, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:06:11.830665 Training: [48 epoch,  90 batch] loss: 1.06590, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35183,MAE：0.21158
2021-01-09 21:09:19.360515 Training: [49 epoch,  10 batch] loss: 1.05781, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:10:28.386391 Training: [49 epoch,  20 batch] loss: 1.07921, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:11:35.486904 Training: [49 epoch,  30 batch] loss: 1.07606, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:12:44.548510 Training: [49 epoch,  40 batch] loss: 1.03539, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:13:57.916098 Training: [49 epoch,  50 batch] loss: 1.03089, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:15:08.300937 Training: [49 epoch,  60 batch] loss: 1.05832, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:16:20.717324 Training: [49 epoch,  70 batch] loss: 1.01436, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:17:33.357124 Training: [49 epoch,  80 batch] loss: 1.00540, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:18:44.943348 Training: [49 epoch,  90 batch] loss: 1.00316, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35097,MAE：0.20684
2021-01-09 21:21:53.120347 Training: [50 epoch,  10 batch] loss: 1.10060, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:23:04.693245 Training: [50 epoch,  20 batch] loss: 1.01054, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:24:15.450014 Training: [50 epoch,  30 batch] loss: 0.97049, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:25:27.839948 Training: [50 epoch,  40 batch] loss: 0.98217, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:26:38.941197 Training: [50 epoch,  50 batch] loss: 0.97592, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:27:49.388158 Training: [50 epoch,  60 batch] loss: 0.97194, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:29:00.054527 Training: [50 epoch,  70 batch] loss: 0.99050, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:30:10.625645 Training: [50 epoch,  80 batch] loss: 0.97019, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:31:23.238313 Training: [50 epoch,  90 batch] loss: 0.97433, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35675,MAE：0.23326
2021-01-09 21:34:35.534600 Training: [51 epoch,  10 batch] loss: 0.94762, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:36:02.605631 Training: [51 epoch,  20 batch] loss: 0.93287, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:38:02.786551 Training: [51 epoch,  30 batch] loss: 0.96588, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:39:51.174335 Training: [51 epoch,  40 batch] loss: 0.93966, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:41:32.900897 Training: [51 epoch,  50 batch] loss: 0.97306, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:43:19.367605 Training: [51 epoch,  60 batch] loss: 0.91917, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:45:11.820365 Training: [51 epoch,  70 batch] loss: 0.99488, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:47:04.302619 Training: [51 epoch,  80 batch] loss: 0.95188, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:48:50.517793 Training: [51 epoch,  90 batch] loss: 0.92108, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.36152,MAE：0.24937
2021-01-09 21:53:19.488069 Training: [52 epoch,  10 batch] loss: 1.00532, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:54:59.102350 Training: [52 epoch,  20 batch] loss: 0.91392, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:56:47.102456 Training: [52 epoch,  30 batch] loss: 0.91769, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 21:58:45.227352 Training: [52 epoch,  40 batch] loss: 0.88664, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:00:34.024295 Training: [52 epoch,  50 batch] loss: 0.88836, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:02:27.916229 Training: [52 epoch,  60 batch] loss: 0.89142, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:04:15.666323 Training: [52 epoch,  70 batch] loss: 0.88731, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:05:59.275765 Training: [52 epoch,  80 batch] loss: 0.90569, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:07:46.856813 Training: [52 epoch,  90 batch] loss: 0.90231, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.41235,MAE：0.35360
2021-01-09 22:12:25.859606 Training: [53 epoch,  10 batch] loss: 0.85965, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:14:11.345850 Training: [53 epoch,  20 batch] loss: 0.87928, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:15:55.845578 Training: [53 epoch,  30 batch] loss: 0.90018, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:17:39.278992 Training: [53 epoch,  40 batch] loss: 0.87274, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:19:20.028301 Training: [53 epoch,  50 batch] loss: 0.87827, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:21:02.742962 Training: [53 epoch,  60 batch] loss: 0.86389, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:22:55.348640 Training: [53 epoch,  70 batch] loss: 0.90943, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:24:40.694156 Training: [53 epoch,  80 batch] loss: 0.84021, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:26:25.128315 Training: [53 epoch,  90 batch] loss: 0.84112, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.40128,MAE：0.33557
2021-01-09 22:31:05.308076 Training: [54 epoch,  10 batch] loss: 0.89814, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:32:56.668857 Training: [54 epoch,  20 batch] loss: 0.82854, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:34:40.803246 Training: [54 epoch,  30 batch] loss: 0.83076, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:36:41.892653 Training: [54 epoch,  40 batch] loss: 0.81300, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:38:27.495060 Training: [54 epoch,  50 batch] loss: 0.86216, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:40:13.264605 Training: [54 epoch,  60 batch] loss: 0.91540, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:41:59.126503 Training: [54 epoch,  70 batch] loss: 0.80705, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:43:51.792082 Training: [54 epoch,  80 batch] loss: 0.79632, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:45:43.608754 Training: [54 epoch,  90 batch] loss: 0.80192, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.39228,MAE：0.31936
2021-01-09 22:50:33.427183 Training: [55 epoch,  10 batch] loss: 0.81255, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:52:14.333664 Training: [55 epoch,  20 batch] loss: 0.83611, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:53:55.042067 Training: [55 epoch,  30 batch] loss: 0.79789, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:55:41.747303 Training: [55 epoch,  40 batch] loss: 0.78934, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:57:32.151748 Training: [55 epoch,  50 batch] loss: 0.76875, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 22:59:19.407918 Training: [55 epoch,  60 batch] loss: 0.78949, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:01:04.602507 Training: [55 epoch,  70 batch] loss: 0.79405, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:02:57.560046 Training: [55 epoch,  80 batch] loss: 0.82100, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:04:54.283310 Training: [55 epoch,  90 batch] loss: 0.83767, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.36785,MAE：0.26723
2021-01-09 23:10:02.076296 Training: [56 epoch,  10 batch] loss: 0.76685, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:11:53.769666 Training: [56 epoch,  20 batch] loss: 0.77078, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:13:44.853448 Training: [56 epoch,  30 batch] loss: 0.77887, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:15:30.400295 Training: [56 epoch,  40 batch] loss: 0.76227, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:17:20.583721 Training: [56 epoch,  50 batch] loss: 0.76506, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:19:16.196712 Training: [56 epoch,  60 batch] loss: 0.75484, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:21:10.803316 Training: [56 epoch,  70 batch] loss: 0.81741, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:23:02.229711 Training: [56 epoch,  80 batch] loss: 0.78737, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:24:47.529214 Training: [56 epoch,  90 batch] loss: 0.78485, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35960,MAE：0.24316
2021-01-09 23:29:25.537361 Training: [57 epoch,  10 batch] loss: 0.75373, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:31:07.594964 Training: [57 epoch,  20 batch] loss: 0.71481, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:32:47.247027 Training: [57 epoch,  30 batch] loss: 0.74429, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:34:30.418609 Training: [57 epoch,  40 batch] loss: 0.73020, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:36:08.353641 Training: [57 epoch,  50 batch] loss: 0.74038, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:37:56.203087 Training: [57 epoch,  60 batch] loss: 0.73114, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:39:40.864703 Training: [57 epoch,  70 batch] loss: 0.81743, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:41:32.066878 Training: [57 epoch,  80 batch] loss: 0.72890, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:43:20.527107 Training: [57 epoch,  90 batch] loss: 0.79789, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.41016,MAE：0.35026
2021-01-09 23:48:17.751023 Training: [58 epoch,  10 batch] loss: 0.71870, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:49:59.615497 Training: [58 epoch,  20 batch] loss: 0.74763, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:51:52.342911 Training: [58 epoch,  30 batch] loss: 0.73217, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:53:43.135901 Training: [58 epoch,  40 batch] loss: 0.69510, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:55:28.382450 Training: [58 epoch,  50 batch] loss: 0.68731, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:57:23.316771 Training: [58 epoch,  60 batch] loss: 0.84538, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-09 23:59:09.398766 Training: [58 epoch,  70 batch] loss: 0.68881, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:01:01.080768 Training: [58 epoch,  80 batch] loss: 0.73567, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:02:55.959272 Training: [58 epoch,  90 batch] loss: 0.68825, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35516,MAE：0.22741
2021-01-10 00:07:45.706486 Training: [59 epoch,  10 batch] loss: 0.73233, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:09:31.312220 Training: [59 epoch,  20 batch] loss: 0.68974, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:11:26.607149 Training: [59 epoch,  30 batch] loss: 0.75221, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:13:15.092039 Training: [59 epoch,  40 batch] loss: 0.70738, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:15:05.953270 Training: [59 epoch,  50 batch] loss: 0.69300, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:16:55.088773 Training: [59 epoch,  60 batch] loss: 0.74082, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:18:39.966808 Training: [59 epoch,  70 batch] loss: 0.74736, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:20:30.657157 Training: [59 epoch,  80 batch] loss: 0.67452, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:22:25.124186 Training: [59 epoch,  90 batch] loss: 0.70095, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.37750,MAE：0.28993
2021-01-10 00:27:04.706679 Training: [60 epoch,  10 batch] loss: 0.70693, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:28:57.232313 Training: [60 epoch,  20 batch] loss: 0.70584, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:30:41.089666 Training: [60 epoch,  30 batch] loss: 0.69442, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:32:28.767881 Training: [60 epoch,  40 batch] loss: 0.71412, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:34:10.373685 Training: [60 epoch,  50 batch] loss: 0.67835, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:36:01.513546 Training: [60 epoch,  60 batch] loss: 0.74969, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:37:50.192126 Training: [60 epoch,  70 batch] loss: 0.72209, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:39:32.640683 Training: [60 epoch,  80 batch] loss: 0.74879, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:41:19.807712 Training: [60 epoch,  90 batch] loss: 0.70336, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.37166,MAE：0.27672
2021-01-10 00:45:52.741555 Training: [61 epoch,  10 batch] loss: 0.70064, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:47:32.237492 Training: [61 epoch,  20 batch] loss: 0.70219, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:49:19.100645 Training: [61 epoch,  30 batch] loss: 0.77679, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:51:03.623635 Training: [61 epoch,  40 batch] loss: 0.70642, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:52:49.982096 Training: [61 epoch,  50 batch] loss: 0.70602, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:54:47.606306 Training: [61 epoch,  60 batch] loss: 0.70143, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:56:46.022484 Training: [61 epoch,  70 batch] loss: 0.74620, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 00:58:29.024282 Training: [61 epoch,  80 batch] loss: 0.71183, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:00:14.770282 Training: [61 epoch,  90 batch] loss: 0.68923, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35990,MAE：0.24442
2021-01-10 01:04:52.128373 Training: [62 epoch,  10 batch] loss: 0.69501, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:06:44.128275 Training: [62 epoch,  20 batch] loss: 0.73179, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:08:42.561835 Training: [62 epoch,  30 batch] loss: 0.73612, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:10:33.582579 Training: [62 epoch,  40 batch] loss: 0.69740, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:12:20.129162 Training: [62 epoch,  50 batch] loss: 0.73067, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:14:11.763451 Training: [62 epoch,  60 batch] loss: 0.72014, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:15:57.482452 Training: [62 epoch,  70 batch] loss: 0.70081, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:17:44.005809 Training: [62 epoch,  80 batch] loss: 0.68404, the best RMSE/MAE: 0.35337 / 0.10628
2021-01-10 01:19:29.588270 Training: [62 epoch,  90 batch] loss: 0.69317, the best RMSE/MAE: 0.35337 / 0.10628
<Test> RMSE：0.35382,MAE：0.22146
The best RMSE/MAE：0.35337/0.10628
