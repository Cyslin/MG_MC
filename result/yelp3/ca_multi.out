-------------------- Hyperparams --------------------
time: 2021-01-07 18:59:32.764843
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-07 19:14:57.427397 Training: [1 epoch,  10 batch] loss: 11.91303, the best RMSE/MAE: inf / inf
2021-01-07 19:15:52.099877 Training: [1 epoch,  20 batch] loss: 11.45126, the best RMSE/MAE: inf / inf
2021-01-07 19:16:51.082123 Training: [1 epoch,  30 batch] loss: 11.24626, the best RMSE/MAE: inf / inf
2021-01-07 19:17:48.563246 Training: [1 epoch,  40 batch] loss: 11.05997, the best RMSE/MAE: inf / inf
2021-01-07 19:18:51.250122 Training: [1 epoch,  50 batch] loss: 10.90494, the best RMSE/MAE: inf / inf
2021-01-07 19:19:55.182660 Training: [1 epoch,  60 batch] loss: 10.79205, the best RMSE/MAE: inf / inf
2021-01-07 19:20:49.468403 Training: [1 epoch,  70 batch] loss: 10.70893, the best RMSE/MAE: inf / inf
2021-01-07 19:21:43.894065 Training: [1 epoch,  80 batch] loss: 10.70096, the best RMSE/MAE: inf / inf
2021-01-07 19:22:38.493867 Training: [1 epoch,  90 batch] loss: 10.64791, the best RMSE/MAE: inf / inf
<Test> RMSE：836502400.00000,MAE：637707712.00000
2021-01-07 19:25:14.461695 Training: [2 epoch,  10 batch] loss: 10.61646, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:26:08.056489 Training: [2 epoch,  20 batch] loss: 10.65193, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:27:01.976494 Training: [2 epoch,  30 batch] loss: 10.55644, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:27:56.223058 Training: [2 epoch,  40 batch] loss: 10.50509, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:28:54.130793 Training: [2 epoch,  50 batch] loss: 10.46843, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:29:57.223998 Training: [2 epoch,  60 batch] loss: 10.49024, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:31:02.982688 Training: [2 epoch,  70 batch] loss: 10.44731, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:32:08.302082 Training: [2 epoch,  80 batch] loss: 10.38494, the best RMSE/MAE: 836502400.00000 / 637707712.00000
2021-01-07 19:33:12.962498 Training: [2 epoch,  90 batch] loss: 10.37405, the best RMSE/MAE: 836502400.00000 / 637707712.00000
<Test> RMSE：978971.62500,MAE：750212.50000
2021-01-07 19:36:29.272636 Training: [3 epoch,  10 batch] loss: 10.35616, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:37:35.409978 Training: [3 epoch,  20 batch] loss: 10.38031, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:38:40.523665 Training: [3 epoch,  30 batch] loss: 10.26847, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:39:44.627166 Training: [3 epoch,  40 batch] loss: 10.22903, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:40:47.833586 Training: [3 epoch,  50 batch] loss: 10.19434, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:41:53.159155 Training: [3 epoch,  60 batch] loss: 10.18911, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:42:57.587941 Training: [3 epoch,  70 batch] loss: 10.19702, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:44:01.832285 Training: [3 epoch,  80 batch] loss: 10.14563, the best RMSE/MAE: 978971.62500 / 750212.50000
2021-01-07 19:45:07.204092 Training: [3 epoch,  90 batch] loss: 10.11586, the best RMSE/MAE: 978971.62500 / 750212.50000
<Test> RMSE：20371.69141,MAE：15882.27539
2021-01-07 19:47:50.383600 Training: [4 epoch,  10 batch] loss: 10.06444, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:48:42.343825 Training: [4 epoch,  20 batch] loss: 10.09125, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:49:34.396658 Training: [4 epoch,  30 batch] loss: 10.03096, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:50:27.250218 Training: [4 epoch,  40 batch] loss: 9.99345, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:51:20.643899 Training: [4 epoch,  50 batch] loss: 9.97491, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:52:14.012370 Training: [4 epoch,  60 batch] loss: 9.98671, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:53:07.488849 Training: [4 epoch,  70 batch] loss: 9.92960, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:54:00.857930 Training: [4 epoch,  80 batch] loss: 9.88425, the best RMSE/MAE: 20371.69141 / 15882.27539
2021-01-07 19:54:53.885510 Training: [4 epoch,  90 batch] loss: 9.86836, the best RMSE/MAE: 20371.69141 / 15882.27539
<Test> RMSE：1255.22900,MAE：964.67688
2021-01-07 19:57:28.306864 Training: [5 epoch,  10 batch] loss: 9.88077, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 19:58:21.913182 Training: [5 epoch,  20 batch] loss: 9.76948, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 19:59:15.561263 Training: [5 epoch,  30 batch] loss: 9.77115, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:00:09.454299 Training: [5 epoch,  40 batch] loss: 9.75139, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:01:03.709684 Training: [5 epoch,  50 batch] loss: 9.72094, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:01:57.552860 Training: [5 epoch,  60 batch] loss: 9.72689, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:02:51.058657 Training: [5 epoch,  70 batch] loss: 9.72473, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:03:44.292525 Training: [5 epoch,  80 batch] loss: 9.62221, the best RMSE/MAE: 1255.22900 / 964.67688
2021-01-07 20:04:37.099241 Training: [5 epoch,  90 batch] loss: 9.60686, the best RMSE/MAE: 1255.22900 / 964.67688
<Test> RMSE：170.94145,MAE：129.58047
2021-01-07 20:07:12.282668 Training: [6 epoch,  10 batch] loss: 9.57540, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:08:05.576748 Training: [6 epoch,  20 batch] loss: 9.54437, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:08:58.721078 Training: [6 epoch,  30 batch] loss: 9.49303, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:09:52.382643 Training: [6 epoch,  40 batch] loss: 9.49241, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:10:46.837068 Training: [6 epoch,  50 batch] loss: 9.45708, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:11:40.713368 Training: [6 epoch,  60 batch] loss: 9.42862, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:12:34.664771 Training: [6 epoch,  70 batch] loss: 9.41069, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:13:28.389969 Training: [6 epoch,  80 batch] loss: 9.40935, the best RMSE/MAE: 170.94145 / 129.58047
2021-01-07 20:14:22.110167 Training: [6 epoch,  90 batch] loss: 9.39849, the best RMSE/MAE: 170.94145 / 129.58047
<Test> RMSE：46.84271,MAE：35.06699
2021-01-07 20:16:55.100374 Training: [7 epoch,  10 batch] loss: 9.29580, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:17:48.393105 Training: [7 epoch,  20 batch] loss: 9.28786, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:18:41.470427 Training: [7 epoch,  30 batch] loss: 9.25977, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:19:34.660435 Training: [7 epoch,  40 batch] loss: 9.20402, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:20:28.209654 Training: [7 epoch,  50 batch] loss: 9.19533, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:21:22.398949 Training: [7 epoch,  60 batch] loss: 9.14906, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:22:16.113391 Training: [7 epoch,  70 batch] loss: 9.20509, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:23:10.206836 Training: [7 epoch,  80 batch] loss: 9.10081, the best RMSE/MAE: 46.84271 / 35.06699
2021-01-07 20:24:04.517795 Training: [7 epoch,  90 batch] loss: 9.04490, the best RMSE/MAE: 46.84271 / 35.06699
<Test> RMSE：17.89646,MAE：13.10720
2021-01-07 20:26:39.424823 Training: [8 epoch,  10 batch] loss: 9.00537, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:27:32.881786 Training: [8 epoch,  20 batch] loss: 8.96611, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:28:26.403306 Training: [8 epoch,  30 batch] loss: 8.92593, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:29:19.457192 Training: [8 epoch,  40 batch] loss: 8.93456, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:30:13.741004 Training: [8 epoch,  50 batch] loss: 8.94728, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:31:08.133172 Training: [8 epoch,  60 batch] loss: 8.88773, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:32:02.486127 Training: [8 epoch,  70 batch] loss: 8.82200, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:32:57.209437 Training: [8 epoch,  80 batch] loss: 8.76988, the best RMSE/MAE: 17.89646 / 13.10720
2021-01-07 20:33:51.128406 Training: [8 epoch,  90 batch] loss: 8.78111, the best RMSE/MAE: 17.89646 / 13.10720
<Test> RMSE：7.63925,MAE：5.56829
2021-01-07 20:36:23.984778 Training: [9 epoch,  10 batch] loss: 8.74448, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:37:17.444497 Training: [9 epoch,  20 batch] loss: 8.68573, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:38:11.049860 Training: [9 epoch,  30 batch] loss: 8.67139, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:39:04.733967 Training: [9 epoch,  40 batch] loss: 8.61639, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:39:58.201400 Training: [9 epoch,  50 batch] loss: 8.56840, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:40:51.723969 Training: [9 epoch,  60 batch] loss: 8.56463, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:41:45.229337 Training: [9 epoch,  70 batch] loss: 8.55290, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:42:38.487504 Training: [9 epoch,  80 batch] loss: 8.47639, the best RMSE/MAE: 7.63925 / 5.56829
2021-01-07 20:43:32.221757 Training: [9 epoch,  90 batch] loss: 8.46184, the best RMSE/MAE: 7.63925 / 5.56829
<Test> RMSE：3.13491,MAE：2.29319
2021-01-07 20:46:08.173724 Training: [10 epoch,  10 batch] loss: 8.37889, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:47:02.120114 Training: [10 epoch,  20 batch] loss: 8.37110, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:47:56.283963 Training: [10 epoch,  30 batch] loss: 8.32174, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:48:49.231855 Training: [10 epoch,  40 batch] loss: 8.28741, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:49:42.245704 Training: [10 epoch,  50 batch] loss: 8.29401, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:50:36.607630 Training: [10 epoch,  60 batch] loss: 8.23850, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:51:30.621209 Training: [10 epoch,  70 batch] loss: 8.23371, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:52:24.961892 Training: [10 epoch,  80 batch] loss: 8.17991, the best RMSE/MAE: 3.13491 / 2.29319
2021-01-07 20:53:19.302399 Training: [10 epoch,  90 batch] loss: 8.14160, the best RMSE/MAE: 3.13491 / 2.29319
<Test> RMSE：2.00945,MAE：1.51081
2021-01-07 20:55:53.751804 Training: [11 epoch,  10 batch] loss: 8.08127, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 20:56:46.365019 Training: [11 epoch,  20 batch] loss: 8.09394, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 20:57:39.257055 Training: [11 epoch,  30 batch] loss: 7.98161, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 20:58:32.268660 Training: [11 epoch,  40 batch] loss: 7.95681, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 20:59:25.784666 Training: [11 epoch,  50 batch] loss: 7.92667, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 21:00:19.085827 Training: [11 epoch,  60 batch] loss: 7.90901, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 21:01:12.277785 Training: [11 epoch,  70 batch] loss: 7.85192, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 21:02:05.326924 Training: [11 epoch,  80 batch] loss: 7.80581, the best RMSE/MAE: 2.00945 / 1.51081
2021-01-07 21:02:58.062855 Training: [11 epoch,  90 batch] loss: 7.81566, the best RMSE/MAE: 2.00945 / 1.51081
<Test> RMSE：1.30354,MAE：1.05694
2021-01-07 21:05:29.414824 Training: [12 epoch,  10 batch] loss: 7.76978, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:06:23.049545 Training: [12 epoch,  20 batch] loss: 7.70782, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:07:17.080088 Training: [12 epoch,  30 batch] loss: 7.62202, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:08:11.026044 Training: [12 epoch,  40 batch] loss: 7.63900, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:09:04.504443 Training: [12 epoch,  50 batch] loss: 7.57819, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:09:58.237492 Training: [12 epoch,  60 batch] loss: 7.53387, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:10:52.474260 Training: [12 epoch,  70 batch] loss: 7.58198, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:11:46.597773 Training: [12 epoch,  80 batch] loss: 7.53645, the best RMSE/MAE: 1.30354 / 1.05694
2021-01-07 21:12:40.770397 Training: [12 epoch,  90 batch] loss: 7.44148, the best RMSE/MAE: 1.30354 / 1.05694
<Test> RMSE：0.84425,MAE：0.69369
2021-01-07 21:15:15.783864 Training: [13 epoch,  10 batch] loss: 7.39194, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:16:08.585900 Training: [13 epoch,  20 batch] loss: 7.33901, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:17:02.259367 Training: [13 epoch,  30 batch] loss: 7.32016, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:17:55.921622 Training: [13 epoch,  40 batch] loss: 7.28280, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:18:49.204787 Training: [13 epoch,  50 batch] loss: 7.25879, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:19:42.561684 Training: [13 epoch,  60 batch] loss: 7.24624, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:20:36.535720 Training: [13 epoch,  70 batch] loss: 7.17692, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:21:31.117738 Training: [13 epoch,  80 batch] loss: 7.15847, the best RMSE/MAE: 0.84425 / 0.69369
2021-01-07 21:22:25.074785 Training: [13 epoch,  90 batch] loss: 7.13545, the best RMSE/MAE: 0.84425 / 0.69369
<Test> RMSE：0.64740,MAE：0.50013
2021-01-07 21:24:42.348737 Training: [14 epoch,  10 batch] loss: 7.04418, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:25:24.172402 Training: [14 epoch,  20 batch] loss: 6.98691, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:26:06.276696 Training: [14 epoch,  30 batch] loss: 6.98146, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:26:48.146334 Training: [14 epoch,  40 batch] loss: 6.92782, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:27:30.301532 Training: [14 epoch,  50 batch] loss: 6.94684, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:28:11.201032 Training: [14 epoch,  60 batch] loss: 6.93322, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:28:52.932051 Training: [14 epoch,  70 batch] loss: 6.81882, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:29:39.616522 Training: [14 epoch,  80 batch] loss: 6.77975, the best RMSE/MAE: 0.64740 / 0.50013
2021-01-07 21:30:34.084465 Training: [14 epoch,  90 batch] loss: 6.74536, the best RMSE/MAE: 0.64740 / 0.50013
<Test> RMSE：0.52526,MAE：0.36546
2021-01-07 21:32:41.621354 Training: [15 epoch,  10 batch] loss: 6.66430, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:33:30.282901 Training: [15 epoch,  20 batch] loss: 6.65974, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:34:17.278844 Training: [15 epoch,  30 batch] loss: 6.60528, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:35:04.283585 Training: [15 epoch,  40 batch] loss: 6.59912, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:35:53.520324 Training: [15 epoch,  50 batch] loss: 6.57449, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:36:43.084007 Training: [15 epoch,  60 batch] loss: 6.50881, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:37:32.917719 Training: [15 epoch,  70 batch] loss: 6.53386, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:38:20.817657 Training: [15 epoch,  80 batch] loss: 6.44734, the best RMSE/MAE: 0.52526 / 0.36546
2021-01-07 21:39:10.653958 Training: [15 epoch,  90 batch] loss: 6.44220, the best RMSE/MAE: 0.52526 / 0.36546
<Test> RMSE：0.49270,MAE：0.31162
2021-01-07 21:41:31.034174 Training: [16 epoch,  10 batch] loss: 6.39552, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:42:18.838215 Training: [16 epoch,  20 batch] loss: 6.31364, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:43:07.420985 Training: [16 epoch,  30 batch] loss: 6.29344, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:43:55.756506 Training: [16 epoch,  40 batch] loss: 6.22760, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:44:43.728846 Training: [16 epoch,  50 batch] loss: 6.20570, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:45:30.954287 Training: [16 epoch,  60 batch] loss: 6.18314, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:46:18.852641 Training: [16 epoch,  70 batch] loss: 6.17152, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:47:09.184745 Training: [16 epoch,  80 batch] loss: 6.11173, the best RMSE/MAE: 0.49270 / 0.31162
2021-01-07 21:47:59.165305 Training: [16 epoch,  90 batch] loss: 6.04663, the best RMSE/MAE: 0.49270 / 0.31162
<Test> RMSE：0.44850,MAE：0.26349
2021-01-07 21:50:21.719514 Training: [17 epoch,  10 batch] loss: 5.99908, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:51:10.885681 Training: [17 epoch,  20 batch] loss: 5.97764, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:51:59.218125 Training: [17 epoch,  30 batch] loss: 5.95833, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:52:48.632620 Training: [17 epoch,  40 batch] loss: 5.94700, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:53:39.920707 Training: [17 epoch,  50 batch] loss: 5.85556, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:54:49.329934 Training: [17 epoch,  60 batch] loss: 5.83048, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:55:57.151283 Training: [17 epoch,  70 batch] loss: 5.83846, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:57:11.882551 Training: [17 epoch,  80 batch] loss: 5.76284, the best RMSE/MAE: 0.44850 / 0.26349
2021-01-07 21:58:25.324331 Training: [17 epoch,  90 batch] loss: 5.73878, the best RMSE/MAE: 0.44850 / 0.26349
<Test> RMSE：0.40866,MAE：0.19988
2021-01-07 22:01:45.854572 Training: [18 epoch,  10 batch] loss: 5.68730, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:02:57.817983 Training: [18 epoch,  20 batch] loss: 5.69498, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:04:09.890686 Training: [18 epoch,  30 batch] loss: 5.59025, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:05:16.253032 Training: [18 epoch,  40 batch] loss: 5.57131, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:06:26.173180 Training: [18 epoch,  50 batch] loss: 5.56972, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:07:25.745058 Training: [18 epoch,  60 batch] loss: 5.49117, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:08:14.298273 Training: [18 epoch,  70 batch] loss: 5.46514, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:09:04.003469 Training: [18 epoch,  80 batch] loss: 5.46549, the best RMSE/MAE: 0.40866 / 0.19988
2021-01-07 22:09:54.093351 Training: [18 epoch,  90 batch] loss: 5.46773, the best RMSE/MAE: 0.40866 / 0.19988
<Test> RMSE：0.39793,MAE：0.16936
2021-01-07 22:12:17.275702 Training: [19 epoch,  10 batch] loss: 5.34085, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:13:07.297947 Training: [19 epoch,  20 batch] loss: 5.35686, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:13:56.888221 Training: [19 epoch,  30 batch] loss: 5.28413, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:14:47.122157 Training: [19 epoch,  40 batch] loss: 5.27052, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:15:36.454608 Training: [19 epoch,  50 batch] loss: 5.24803, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:16:27.157078 Training: [19 epoch,  60 batch] loss: 5.20053, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:17:17.780689 Training: [19 epoch,  70 batch] loss: 5.15434, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:18:07.228170 Training: [19 epoch,  80 batch] loss: 5.18158, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:18:57.357237 Training: [19 epoch,  90 batch] loss: 5.09312, the best RMSE/MAE: 0.39793 / 0.16936
<Test> RMSE：0.40995,MAE：0.17174
2021-01-07 22:21:18.447669 Training: [20 epoch,  10 batch] loss: 5.05326, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:22:08.074759 Training: [20 epoch,  20 batch] loss: 5.04172, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:22:59.211029 Training: [20 epoch,  30 batch] loss: 5.00331, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:24:00.349366 Training: [20 epoch,  40 batch] loss: 4.97075, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:25:01.992302 Training: [20 epoch,  50 batch] loss: 4.90822, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:26:04.111856 Training: [20 epoch,  60 batch] loss: 4.92780, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:27:04.801073 Training: [20 epoch,  70 batch] loss: 4.88919, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:28:06.367989 Training: [20 epoch,  80 batch] loss: 4.83187, the best RMSE/MAE: 0.39793 / 0.16936
2021-01-07 22:29:08.488524 Training: [20 epoch,  90 batch] loss: 4.80209, the best RMSE/MAE: 0.39793 / 0.16936
<Test> RMSE：0.39641,MAE：0.14321
2021-01-07 22:32:05.433868 Training: [21 epoch,  10 batch] loss: 4.74596, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:33:06.130311 Training: [21 epoch,  20 batch] loss: 4.73299, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:34:04.206762 Training: [21 epoch,  30 batch] loss: 4.76028, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:35:04.231990 Training: [21 epoch,  40 batch] loss: 4.67931, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:36:06.064264 Training: [21 epoch,  50 batch] loss: 4.64787, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:37:07.204954 Training: [21 epoch,  60 batch] loss: 4.59409, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:38:07.325682 Training: [21 epoch,  70 batch] loss: 4.59429, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:39:08.916365 Training: [21 epoch,  80 batch] loss: 4.60925, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:40:10.835560 Training: [21 epoch,  90 batch] loss: 4.52700, the best RMSE/MAE: 0.39641 / 0.14321
<Test> RMSE：0.39773,MAE：0.15089
2021-01-07 22:43:06.618432 Training: [22 epoch,  10 batch] loss: 4.48140, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:44:07.176856 Training: [22 epoch,  20 batch] loss: 4.46010, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:45:06.473807 Training: [22 epoch,  30 batch] loss: 4.42699, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:46:08.325637 Training: [22 epoch,  40 batch] loss: 4.43480, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:47:11.135867 Training: [22 epoch,  50 batch] loss: 4.36779, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:48:11.861623 Training: [22 epoch,  60 batch] loss: 4.33693, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:49:14.016019 Training: [22 epoch,  70 batch] loss: 4.31205, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:50:14.407707 Training: [22 epoch,  80 batch] loss: 4.30219, the best RMSE/MAE: 0.39641 / 0.14321
2021-01-07 22:51:14.883045 Training: [22 epoch,  90 batch] loss: 4.30533, the best RMSE/MAE: 0.39641 / 0.14321
<Test> RMSE：0.40249,MAE：0.14223
2021-01-07 22:54:18.166680 Training: [23 epoch,  10 batch] loss: 4.28365, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 22:55:19.021245 Training: [23 epoch,  20 batch] loss: 4.20091, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 22:56:18.988525 Training: [23 epoch,  30 batch] loss: 4.14779, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 22:57:19.708529 Training: [23 epoch,  40 batch] loss: 4.14749, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 22:58:20.127179 Training: [23 epoch,  50 batch] loss: 4.11554, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 22:59:22.108672 Training: [23 epoch,  60 batch] loss: 4.08914, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:00:23.470942 Training: [23 epoch,  70 batch] loss: 4.05839, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:01:24.085099 Training: [23 epoch,  80 batch] loss: 4.03906, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:02:25.665063 Training: [23 epoch,  90 batch] loss: 4.02521, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.41315,MAE：0.16170
2021-01-07 23:05:21.872828 Training: [24 epoch,  10 batch] loss: 3.96201, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:06:21.385539 Training: [24 epoch,  20 batch] loss: 3.98467, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:07:20.182355 Training: [24 epoch,  30 batch] loss: 3.93489, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:08:21.093772 Training: [24 epoch,  40 batch] loss: 3.89840, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:09:22.803563 Training: [24 epoch,  50 batch] loss: 3.89917, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:10:25.091383 Training: [24 epoch,  60 batch] loss: 3.85505, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:11:25.750790 Training: [24 epoch,  70 batch] loss: 3.81471, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:12:27.565431 Training: [24 epoch,  80 batch] loss: 3.79386, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:13:28.337845 Training: [24 epoch,  90 batch] loss: 3.81098, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.40637,MAE：0.15113
2021-01-07 23:16:12.753385 Training: [25 epoch,  10 batch] loss: 3.75370, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:17:07.935803 Training: [25 epoch,  20 batch] loss: 3.71418, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:18:01.581553 Training: [25 epoch,  30 batch] loss: 3.70682, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:18:56.184138 Training: [25 epoch,  40 batch] loss: 3.65747, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:19:50.255159 Training: [25 epoch,  50 batch] loss: 3.63740, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:20:44.444452 Training: [25 epoch,  60 batch] loss: 3.61924, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:21:39.140112 Training: [25 epoch,  70 batch] loss: 3.60426, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:22:34.041781 Training: [25 epoch,  80 batch] loss: 3.59984, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:23:28.522257 Training: [25 epoch,  90 batch] loss: 3.58000, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.40158,MAE：0.14417
2021-01-07 23:26:05.060024 Training: [26 epoch,  10 batch] loss: 3.54438, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:26:59.433261 Training: [26 epoch,  20 batch] loss: 3.48935, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:27:52.366294 Training: [26 epoch,  30 batch] loss: 3.47740, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:28:46.822379 Training: [26 epoch,  40 batch] loss: 3.44895, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:29:41.337717 Training: [26 epoch,  50 batch] loss: 3.42687, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:30:35.657370 Training: [26 epoch,  60 batch] loss: 3.46565, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:31:29.694086 Training: [26 epoch,  70 batch] loss: 3.37911, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:32:23.524023 Training: [26 epoch,  80 batch] loss: 3.38463, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:33:18.042605 Training: [26 epoch,  90 batch] loss: 3.33553, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.41137,MAE：0.15143
2021-01-07 23:35:53.144761 Training: [27 epoch,  10 batch] loss: 3.35510, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:36:48.128389 Training: [27 epoch,  20 batch] loss: 3.29456, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:37:41.689236 Training: [27 epoch,  30 batch] loss: 3.24476, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:38:35.410060 Training: [27 epoch,  40 batch] loss: 3.22561, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:39:29.866168 Training: [27 epoch,  50 batch] loss: 3.22099, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:40:24.477052 Training: [27 epoch,  60 batch] loss: 3.21882, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:41:18.517335 Training: [27 epoch,  70 batch] loss: 3.22064, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:42:13.466128 Training: [27 epoch,  80 batch] loss: 3.16818, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:43:07.500836 Training: [27 epoch,  90 batch] loss: 3.18158, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.41825,MAE：0.15387
2021-01-07 23:45:43.276283 Training: [28 epoch,  10 batch] loss: 3.12755, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:46:37.264814 Training: [28 epoch,  20 batch] loss: 3.10942, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:47:30.909678 Training: [28 epoch,  30 batch] loss: 3.07679, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:48:23.609310 Training: [28 epoch,  40 batch] loss: 3.11059, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:49:16.970005 Training: [28 epoch,  50 batch] loss: 3.02796, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:50:11.592794 Training: [28 epoch,  60 batch] loss: 3.04723, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:51:06.736932 Training: [28 epoch,  70 batch] loss: 3.01035, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:52:01.969131 Training: [28 epoch,  80 batch] loss: 3.00987, the best RMSE/MAE: 0.40249 / 0.14223
2021-01-07 23:52:57.050452 Training: [28 epoch,  90 batch] loss: 2.98021, the best RMSE/MAE: 0.40249 / 0.14223
<Test> RMSE：0.40776,MAE：0.13184
2021-01-07 23:55:33.436266 Training: [29 epoch,  10 batch] loss: 2.94742, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-07 23:56:30.229953 Training: [29 epoch,  20 batch] loss: 2.92244, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-07 23:57:27.207957 Training: [29 epoch,  30 batch] loss: 2.94169, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-07 23:58:24.082060 Training: [29 epoch,  40 batch] loss: 2.89501, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-07 23:59:19.541261 Training: [29 epoch,  50 batch] loss: 2.89394, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:00:13.990141 Training: [29 epoch,  60 batch] loss: 2.84576, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:01:08.292901 Training: [29 epoch,  70 batch] loss: 2.82742, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:02:02.562097 Training: [29 epoch,  80 batch] loss: 2.81695, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:02:57.569882 Training: [29 epoch,  90 batch] loss: 2.79785, the best RMSE/MAE: 0.40776 / 0.13184
<Test> RMSE：0.41327,MAE：0.13833
2021-01-08 00:05:42.680022 Training: [30 epoch,  10 batch] loss: 2.75345, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:06:37.309329 Training: [30 epoch,  20 batch] loss: 2.77768, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:07:31.285151 Training: [30 epoch,  30 batch] loss: 2.79639, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:08:26.258150 Training: [30 epoch,  40 batch] loss: 2.73663, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:09:20.622006 Training: [30 epoch,  50 batch] loss: 2.71386, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:10:15.032830 Training: [30 epoch,  60 batch] loss: 2.68529, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:11:10.077190 Training: [30 epoch,  70 batch] loss: 2.67008, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:12:05.102189 Training: [30 epoch,  80 batch] loss: 2.66757, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:12:59.730169 Training: [30 epoch,  90 batch] loss: 2.64953, the best RMSE/MAE: 0.40776 / 0.13184
<Test> RMSE：0.41041,MAE：0.13505
2021-01-08 00:15:35.531786 Training: [31 epoch,  10 batch] loss: 2.66678, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:16:29.489497 Training: [31 epoch,  20 batch] loss: 2.61103, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:17:22.967726 Training: [31 epoch,  30 batch] loss: 2.57304, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:18:16.870485 Training: [31 epoch,  40 batch] loss: 2.58728, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:19:11.420371 Training: [31 epoch,  50 batch] loss: 2.55678, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:20:04.984379 Training: [31 epoch,  60 batch] loss: 2.53818, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:20:58.039831 Training: [31 epoch,  70 batch] loss: 2.51182, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:21:51.626262 Training: [31 epoch,  80 batch] loss: 2.55779, the best RMSE/MAE: 0.40776 / 0.13184
2021-01-08 00:22:43.577035 Training: [31 epoch,  90 batch] loss: 2.49313, the best RMSE/MAE: 0.40776 / 0.13184
<Test> RMSE：0.40035,MAE：0.11252
2021-01-08 00:24:45.411932 Training: [32 epoch,  10 batch] loss: 2.47849, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:25:27.023643 Training: [32 epoch,  20 batch] loss: 2.49988, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:26:07.929276 Training: [32 epoch,  30 batch] loss: 2.44758, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:26:48.990213 Training: [32 epoch,  40 batch] loss: 2.41543, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:27:31.126428 Training: [32 epoch,  50 batch] loss: 2.45302, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:28:13.612786 Training: [32 epoch,  60 batch] loss: 2.41279, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:28:56.208029 Training: [32 epoch,  70 batch] loss: 2.39858, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:29:39.180875 Training: [32 epoch,  80 batch] loss: 2.36158, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:30:21.227483 Training: [32 epoch,  90 batch] loss: 2.33161, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.40890,MAE：0.13224
2021-01-08 00:32:24.331731 Training: [33 epoch,  10 batch] loss: 2.34221, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:33:06.098713 Training: [33 epoch,  20 batch] loss: 2.31500, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:33:46.781228 Training: [33 epoch,  30 batch] loss: 2.32187, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:34:27.693328 Training: [33 epoch,  40 batch] loss: 2.28517, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:35:09.504386 Training: [33 epoch,  50 batch] loss: 2.30141, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:35:51.267902 Training: [33 epoch,  60 batch] loss: 2.30896, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:36:33.549847 Training: [33 epoch,  70 batch] loss: 2.24927, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:37:16.109187 Training: [33 epoch,  80 batch] loss: 2.29431, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:37:58.558431 Training: [33 epoch,  90 batch] loss: 2.23848, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.40994,MAE：0.13290
2021-01-08 00:40:02.511324 Training: [34 epoch,  10 batch] loss: 2.23601, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:40:44.600256 Training: [34 epoch,  20 batch] loss: 2.18480, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:41:25.505440 Training: [34 epoch,  30 batch] loss: 2.18825, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:42:06.321700 Training: [34 epoch,  40 batch] loss: 2.15819, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:42:48.476020 Training: [34 epoch,  50 batch] loss: 2.15875, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:43:30.164230 Training: [34 epoch,  60 batch] loss: 2.17503, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:44:11.707155 Training: [34 epoch,  70 batch] loss: 2.13008, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:44:53.869230 Training: [34 epoch,  80 batch] loss: 2.16955, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:45:36.444163 Training: [34 epoch,  90 batch] loss: 2.09500, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.42046,MAE：0.16311
2021-01-08 00:47:39.470709 Training: [35 epoch,  10 batch] loss: 2.09098, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:48:20.907835 Training: [35 epoch,  20 batch] loss: 2.08784, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:49:01.839486 Training: [35 epoch,  30 batch] loss: 2.07185, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:49:42.331820 Training: [35 epoch,  40 batch] loss: 2.07469, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:50:24.631815 Training: [35 epoch,  50 batch] loss: 2.03341, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:51:07.022696 Training: [35 epoch,  60 batch] loss: 2.07172, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:51:49.284692 Training: [35 epoch,  70 batch] loss: 2.02660, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:52:31.665199 Training: [35 epoch,  80 batch] loss: 2.00284, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:53:14.048513 Training: [35 epoch,  90 batch] loss: 1.99729, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.41709,MAE：0.15336
2021-01-08 00:55:17.588536 Training: [36 epoch,  10 batch] loss: 1.98257, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:55:59.476564 Training: [36 epoch,  20 batch] loss: 2.01259, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:56:40.544946 Training: [36 epoch,  30 batch] loss: 1.96474, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:57:20.871935 Training: [36 epoch,  40 batch] loss: 1.97026, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:58:03.177443 Training: [36 epoch,  50 batch] loss: 1.93078, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:58:46.011708 Training: [36 epoch,  60 batch] loss: 1.91819, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 00:59:28.217273 Training: [36 epoch,  70 batch] loss: 1.92531, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:00:09.765456 Training: [36 epoch,  80 batch] loss: 1.92773, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:00:50.843052 Training: [36 epoch,  90 batch] loss: 1.88172, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.41331,MAE：0.14316
2021-01-08 01:02:55.784439 Training: [37 epoch,  10 batch] loss: 1.88845, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:03:37.318018 Training: [37 epoch,  20 batch] loss: 1.89666, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:04:18.577095 Training: [37 epoch,  30 batch] loss: 1.84805, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:04:58.982630 Training: [37 epoch,  40 batch] loss: 1.83015, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:05:40.869547 Training: [37 epoch,  50 batch] loss: 1.84072, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:06:23.208688 Training: [37 epoch,  60 batch] loss: 1.84647, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:07:05.469940 Training: [37 epoch,  70 batch] loss: 1.81912, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:07:47.283052 Training: [37 epoch,  80 batch] loss: 1.80338, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:08:29.186320 Training: [37 epoch,  90 batch] loss: 1.83140, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.40902,MAE：0.12967
2021-01-08 01:10:36.828361 Training: [38 epoch,  10 batch] loss: 1.76397, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:11:23.253175 Training: [38 epoch,  20 batch] loss: 1.78480, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:12:09.181857 Training: [38 epoch,  30 batch] loss: 1.75419, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:12:54.808586 Training: [38 epoch,  40 batch] loss: 1.80958, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:13:41.753894 Training: [38 epoch,  50 batch] loss: 1.77818, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:14:28.588444 Training: [38 epoch,  60 batch] loss: 1.72950, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:15:15.520611 Training: [38 epoch,  70 batch] loss: 1.76293, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:16:02.265411 Training: [38 epoch,  80 batch] loss: 1.70990, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:16:49.183350 Training: [38 epoch,  90 batch] loss: 1.71063, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.40891,MAE：0.12961
2021-01-08 01:18:59.819759 Training: [39 epoch,  10 batch] loss: 1.70457, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:19:45.690606 Training: [39 epoch,  20 batch] loss: 1.69986, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:20:31.369359 Training: [39 epoch,  30 batch] loss: 1.68584, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:21:18.508623 Training: [39 epoch,  40 batch] loss: 1.69174, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:22:05.678408 Training: [39 epoch,  50 batch] loss: 1.66261, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:22:52.937936 Training: [39 epoch,  60 batch] loss: 1.63761, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:23:40.394677 Training: [39 epoch,  70 batch] loss: 1.69371, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:24:27.295301 Training: [39 epoch,  80 batch] loss: 1.61273, the best RMSE/MAE: 0.40035 / 0.11252
2021-01-08 01:25:13.997040 Training: [39 epoch,  90 batch] loss: 1.60104, the best RMSE/MAE: 0.40035 / 0.11252
<Test> RMSE：0.40152,MAE：0.10383
2021-01-08 01:27:23.087677 Training: [40 epoch,  10 batch] loss: 1.62906, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:28:08.112522 Training: [40 epoch,  20 batch] loss: 1.61048, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:28:54.783915 Training: [40 epoch,  30 batch] loss: 1.57928, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:29:42.615582 Training: [40 epoch,  40 batch] loss: 1.59482, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:30:29.369247 Training: [40 epoch,  50 batch] loss: 1.58544, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:31:16.081839 Training: [40 epoch,  60 batch] loss: 1.58378, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:32:02.903755 Training: [40 epoch,  70 batch] loss: 1.61137, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:32:49.868003 Training: [40 epoch,  80 batch] loss: 1.55580, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:33:36.786001 Training: [40 epoch,  90 batch] loss: 1.51643, the best RMSE/MAE: 0.40152 / 0.10383
<Test> RMSE：0.40281,MAE：0.10837
2021-01-08 01:35:44.656617 Training: [41 epoch,  10 batch] loss: 1.52432, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:36:30.956799 Training: [41 epoch,  20 batch] loss: 1.51936, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:37:17.243915 Training: [41 epoch,  30 batch] loss: 1.53765, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:38:03.628042 Training: [41 epoch,  40 batch] loss: 1.53123, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:38:49.861210 Training: [41 epoch,  50 batch] loss: 1.50247, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:39:36.285666 Training: [41 epoch,  60 batch] loss: 1.50095, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:40:22.674798 Training: [41 epoch,  70 batch] loss: 1.48424, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:41:09.272190 Training: [41 epoch,  80 batch] loss: 1.49545, the best RMSE/MAE: 0.40152 / 0.10383
2021-01-08 01:41:55.777487 Training: [41 epoch,  90 batch] loss: 1.47070, the best RMSE/MAE: 0.40152 / 0.10383
<Test> RMSE：0.39319,MAE：0.10072
2021-01-08 01:44:06.932295 Training: [42 epoch,  10 batch] loss: 1.46227, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:44:56.812044 Training: [42 epoch,  20 batch] loss: 1.46935, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:45:43.344151 Training: [42 epoch,  30 batch] loss: 1.48269, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:46:29.505535 Training: [42 epoch,  40 batch] loss: 1.44609, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:47:15.689486 Training: [42 epoch,  50 batch] loss: 1.41232, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:48:01.863835 Training: [42 epoch,  60 batch] loss: 1.45938, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:48:48.366854 Training: [42 epoch,  70 batch] loss: 1.40860, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:49:34.961456 Training: [42 epoch,  80 batch] loss: 1.40052, the best RMSE/MAE: 0.39319 / 0.10072
2021-01-08 01:50:20.808223 Training: [42 epoch,  90 batch] loss: 1.39353, the best RMSE/MAE: 0.39319 / 0.10072
<Test> RMSE：0.39652,MAE：0.08529
2021-01-08 01:52:29.688360 Training: [43 epoch,  10 batch] loss: 1.38909, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:53:15.904053 Training: [43 epoch,  20 batch] loss: 1.39089, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:54:02.251409 Training: [43 epoch,  30 batch] loss: 1.37349, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:54:48.705352 Training: [43 epoch,  40 batch] loss: 1.35524, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:55:35.064474 Training: [43 epoch,  50 batch] loss: 1.35472, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:56:21.610925 Training: [43 epoch,  60 batch] loss: 1.40720, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:57:07.988696 Training: [43 epoch,  70 batch] loss: 1.37271, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:57:53.821616 Training: [43 epoch,  80 batch] loss: 1.33736, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 01:58:39.450051 Training: [43 epoch,  90 batch] loss: 1.31232, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39360,MAE：0.09847
2021-01-08 02:00:57.499332 Training: [44 epoch,  10 batch] loss: 1.34342, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:01:43.994306 Training: [44 epoch,  20 batch] loss: 1.31412, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:02:30.500287 Training: [44 epoch,  30 batch] loss: 1.33822, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:03:17.049476 Training: [44 epoch,  40 batch] loss: 1.29147, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:04:03.630876 Training: [44 epoch,  50 batch] loss: 1.28994, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:04:50.229948 Training: [44 epoch,  60 batch] loss: 1.36663, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:05:36.028076 Training: [44 epoch,  70 batch] loss: 1.29295, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:06:21.529237 Training: [44 epoch,  80 batch] loss: 1.25388, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:07:08.372724 Training: [44 epoch,  90 batch] loss: 1.27159, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39126,MAE：0.11206
2021-01-08 02:09:18.830385 Training: [45 epoch,  10 batch] loss: 1.28608, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:10:03.503702 Training: [45 epoch,  20 batch] loss: 1.25299, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:10:45.436360 Training: [45 epoch,  30 batch] loss: 1.26636, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:11:27.971936 Training: [45 epoch,  40 batch] loss: 1.28756, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:12:10.745604 Training: [45 epoch,  50 batch] loss: 1.22038, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:12:53.513805 Training: [45 epoch,  60 batch] loss: 1.24481, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:13:34.768368 Training: [45 epoch,  70 batch] loss: 1.21953, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:14:16.377920 Training: [45 epoch,  80 batch] loss: 1.20765, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:14:58.837672 Training: [45 epoch,  90 batch] loss: 1.22434, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39541,MAE：0.08994
2021-01-08 02:17:11.035121 Training: [46 epoch,  10 batch] loss: 1.22332, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:17:56.877476 Training: [46 epoch,  20 batch] loss: 1.18597, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:18:42.850867 Training: [46 epoch,  30 batch] loss: 1.20095, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:19:28.896829 Training: [46 epoch,  40 batch] loss: 1.19727, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:20:14.941653 Training: [46 epoch,  50 batch] loss: 1.18180, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:21:00.929415 Training: [46 epoch,  60 batch] loss: 1.19939, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:21:47.093401 Training: [46 epoch,  70 batch] loss: 1.18600, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:22:33.645779 Training: [46 epoch,  80 batch] loss: 1.15790, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:23:20.114164 Training: [46 epoch,  90 batch] loss: 1.17058, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39425,MAE：0.09570
2021-01-08 02:25:30.181431 Training: [47 epoch,  10 batch] loss: 1.15976, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:26:16.404433 Training: [47 epoch,  20 batch] loss: 1.16300, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:27:03.105844 Training: [47 epoch,  30 batch] loss: 1.14495, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:27:49.695185 Training: [47 epoch,  40 batch] loss: 1.13724, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:28:35.465150 Training: [47 epoch,  50 batch] loss: 1.15249, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:29:18.966043 Training: [47 epoch,  60 batch] loss: 1.13878, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:30:01.392177 Training: [47 epoch,  70 batch] loss: 1.10509, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:30:44.555735 Training: [47 epoch,  80 batch] loss: 1.11842, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:31:27.449720 Training: [47 epoch,  90 batch] loss: 1.10119, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39165,MAE：0.10908
2021-01-08 02:33:30.333925 Training: [48 epoch,  10 batch] loss: 1.10444, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:34:12.030766 Training: [48 epoch,  20 batch] loss: 1.12618, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:34:55.943264 Training: [48 epoch,  30 batch] loss: 1.09024, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:35:39.512493 Training: [48 epoch,  40 batch] loss: 1.09588, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:36:20.644543 Training: [48 epoch,  50 batch] loss: 1.08938, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:37:02.329065 Training: [48 epoch,  60 batch] loss: 1.09408, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:37:45.090027 Training: [48 epoch,  70 batch] loss: 1.07014, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:38:27.460670 Training: [48 epoch,  80 batch] loss: 1.07451, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:39:09.745506 Training: [48 epoch,  90 batch] loss: 1.04883, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39085,MAE：0.11352
2021-01-08 02:41:12.229726 Training: [49 epoch,  10 batch] loss: 1.03393, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:41:54.497281 Training: [49 epoch,  20 batch] loss: 1.05786, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:42:36.593604 Training: [49 epoch,  30 batch] loss: 1.02913, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:43:18.461292 Training: [49 epoch,  40 batch] loss: 1.04007, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:43:59.453732 Training: [49 epoch,  50 batch] loss: 1.05159, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:44:41.002056 Training: [49 epoch,  60 batch] loss: 1.03661, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:45:23.573243 Training: [49 epoch,  70 batch] loss: 1.04809, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:46:05.820275 Training: [49 epoch,  80 batch] loss: 1.05984, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:46:47.921829 Training: [49 epoch,  90 batch] loss: 1.03299, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39123,MAE：0.11241
2021-01-08 02:48:50.002668 Training: [50 epoch,  10 batch] loss: 0.99424, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:49:31.649374 Training: [50 epoch,  20 batch] loss: 0.98046, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:50:13.524032 Training: [50 epoch,  30 batch] loss: 1.01836, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:51:02.862888 Training: [50 epoch,  40 batch] loss: 0.99065, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:51:44.605563 Training: [50 epoch,  50 batch] loss: 1.02118, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:52:26.771694 Training: [50 epoch,  60 batch] loss: 1.00079, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:53:09.125668 Training: [50 epoch,  70 batch] loss: 0.98784, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:53:50.825103 Training: [50 epoch,  80 batch] loss: 0.96817, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:54:32.459371 Training: [50 epoch,  90 batch] loss: 0.99551, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39281,MAE：0.10094
2021-01-08 02:56:34.689826 Training: [51 epoch,  10 batch] loss: 1.02033, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:57:16.746617 Training: [51 epoch,  20 batch] loss: 0.99018, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:57:58.817299 Training: [51 epoch,  30 batch] loss: 1.00729, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:58:40.853650 Training: [51 epoch,  40 batch] loss: 0.92945, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 02:59:22.080715 Training: [51 epoch,  50 batch] loss: 0.94099, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:00:04.229369 Training: [51 epoch,  60 batch] loss: 0.97841, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:00:46.468567 Training: [51 epoch,  70 batch] loss: 0.93348, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:01:28.698000 Training: [51 epoch,  80 batch] loss: 0.93224, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:02:11.375029 Training: [51 epoch,  90 batch] loss: 0.92526, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38836,MAE：0.14002
2021-01-08 03:04:14.681338 Training: [52 epoch,  10 batch] loss: 0.91014, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:04:56.741481 Training: [52 epoch,  20 batch] loss: 0.93863, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:05:38.959738 Training: [52 epoch,  30 batch] loss: 0.89698, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:06:20.373321 Training: [52 epoch,  40 batch] loss: 0.93017, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:07:01.509479 Training: [52 epoch,  50 batch] loss: 0.90998, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:07:44.440428 Training: [52 epoch,  60 batch] loss: 0.96798, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:08:26.864177 Training: [52 epoch,  70 batch] loss: 0.91527, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:09:09.440423 Training: [52 epoch,  80 batch] loss: 0.91642, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:09:52.537505 Training: [52 epoch,  90 batch] loss: 0.89663, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38810,MAE：0.14768
2021-01-08 03:11:55.845601 Training: [53 epoch,  10 batch] loss: 0.87640, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:12:38.003127 Training: [53 epoch,  20 batch] loss: 0.88067, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:13:20.117583 Training: [53 epoch,  30 batch] loss: 0.86040, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:14:01.191691 Training: [53 epoch,  40 batch] loss: 0.88215, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:14:42.280326 Training: [53 epoch,  50 batch] loss: 0.86865, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:15:24.671758 Training: [53 epoch,  60 batch] loss: 0.87136, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:16:06.781476 Training: [53 epoch,  70 batch] loss: 0.87966, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:16:48.676442 Training: [53 epoch,  80 batch] loss: 0.89953, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:17:30.363761 Training: [53 epoch,  90 batch] loss: 0.89850, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38955,MAE：0.12311
2021-01-08 03:19:32.118157 Training: [54 epoch,  10 batch] loss: 0.86123, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:20:14.252268 Training: [54 epoch,  20 batch] loss: 0.86758, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:20:56.492833 Training: [54 epoch,  30 batch] loss: 0.87288, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:21:37.297818 Training: [54 epoch,  40 batch] loss: 0.82762, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:22:18.519222 Training: [54 epoch,  50 batch] loss: 0.84831, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:23:00.820086 Training: [54 epoch,  60 batch] loss: 0.82082, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:23:43.260267 Training: [54 epoch,  70 batch] loss: 0.84352, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:24:25.213568 Training: [54 epoch,  80 batch] loss: 0.84033, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:25:07.282245 Training: [54 epoch,  90 batch] loss: 0.84384, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38859,MAE：0.13678
2021-01-08 03:27:14.632499 Training: [55 epoch,  10 batch] loss: 0.82867, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:27:56.365533 Training: [55 epoch,  20 batch] loss: 0.84260, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:28:38.177648 Training: [55 epoch,  30 batch] loss: 0.80932, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:29:18.744969 Training: [55 epoch,  40 batch] loss: 0.82387, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:30:00.244301 Training: [55 epoch,  50 batch] loss: 0.79089, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:30:42.733596 Training: [55 epoch,  60 batch] loss: 0.78009, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:31:24.781050 Training: [55 epoch,  70 batch] loss: 0.81467, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:32:06.577997 Training: [55 epoch,  80 batch] loss: 0.83648, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:32:48.132988 Training: [55 epoch,  90 batch] loss: 0.80505, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38818,MAE：0.16129
2021-01-08 03:34:49.775496 Training: [56 epoch,  10 batch] loss: 0.78504, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:35:31.415764 Training: [56 epoch,  20 batch] loss: 0.78428, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:36:13.304635 Training: [56 epoch,  30 batch] loss: 0.78973, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:36:54.444032 Training: [56 epoch,  40 batch] loss: 0.77462, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:37:36.236917 Training: [56 epoch,  50 batch] loss: 0.77372, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:38:18.936664 Training: [56 epoch,  60 batch] loss: 0.78711, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:39:01.236472 Training: [56 epoch,  70 batch] loss: 0.83348, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:39:43.386144 Training: [56 epoch,  80 batch] loss: 0.79116, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:40:25.900336 Training: [56 epoch,  90 batch] loss: 0.74699, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38806,MAE：0.15737
2021-01-08 03:42:29.392168 Training: [57 epoch,  10 batch] loss: 0.76444, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:43:11.597499 Training: [57 epoch,  20 batch] loss: 0.76840, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:43:53.014354 Training: [57 epoch,  30 batch] loss: 0.76203, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:44:34.124650 Training: [57 epoch,  40 batch] loss: 0.76268, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:45:16.281249 Training: [57 epoch,  50 batch] loss: 0.74718, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:45:58.850761 Training: [57 epoch,  60 batch] loss: 0.73791, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:46:41.321564 Training: [57 epoch,  70 batch] loss: 0.75213, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:47:23.448060 Training: [57 epoch,  80 batch] loss: 0.72608, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:48:05.504131 Training: [57 epoch,  90 batch] loss: 0.76671, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38901,MAE：0.17748
2021-01-08 03:50:07.600716 Training: [58 epoch,  10 batch] loss: 0.72957, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:50:49.819766 Training: [58 epoch,  20 batch] loss: 0.74395, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:51:31.471800 Training: [58 epoch,  30 batch] loss: 0.72097, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:52:12.156546 Training: [58 epoch,  40 batch] loss: 0.73977, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:52:54.855746 Training: [58 epoch,  50 batch] loss: 0.73309, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:53:37.411038 Training: [58 epoch,  60 batch] loss: 0.74328, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:54:19.668594 Training: [58 epoch,  70 batch] loss: 0.70091, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:55:01.711756 Training: [58 epoch,  80 batch] loss: 0.74671, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:55:43.821289 Training: [58 epoch,  90 batch] loss: 0.73131, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38830,MAE：0.16524
2021-01-08 03:57:46.307958 Training: [59 epoch,  10 batch] loss: 0.71567, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:58:28.438004 Training: [59 epoch,  20 batch] loss: 0.76358, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:59:11.969746 Training: [59 epoch,  30 batch] loss: 0.71331, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 03:59:57.117274 Training: [59 epoch,  40 batch] loss: 0.71389, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:00:44.048019 Training: [59 epoch,  50 batch] loss: 0.73239, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:01:30.718179 Training: [59 epoch,  60 batch] loss: 0.74119, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:02:17.462154 Training: [59 epoch,  70 batch] loss: 0.69243, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:03:04.156749 Training: [59 epoch,  80 batch] loss: 0.70007, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:03:50.960444 Training: [59 epoch,  90 batch] loss: 0.69680, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38870,MAE：0.17245
2021-01-08 04:06:00.155279 Training: [60 epoch,  10 batch] loss: 0.71950, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:06:41.332950 Training: [60 epoch,  20 batch] loss: 0.72149, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:07:21.921593 Training: [60 epoch,  30 batch] loss: 0.76381, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:08:03.478064 Training: [60 epoch,  40 batch] loss: 0.70834, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:08:44.973756 Training: [60 epoch,  50 batch] loss: 0.69912, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:09:26.934725 Training: [60 epoch,  60 batch] loss: 0.71582, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:10:09.018203 Training: [60 epoch,  70 batch] loss: 0.72040, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:10:51.148211 Training: [60 epoch,  80 batch] loss: 0.68926, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:11:33.354343 Training: [60 epoch,  90 batch] loss: 0.70040, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38902,MAE：0.17718
2021-01-08 04:13:37.064507 Training: [61 epoch,  10 batch] loss: 0.69500, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:14:18.136001 Training: [61 epoch,  20 batch] loss: 0.74117, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:14:58.912367 Training: [61 epoch,  30 batch] loss: 0.71959, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:15:41.732119 Training: [61 epoch,  40 batch] loss: 0.72417, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:16:24.137351 Training: [61 epoch,  50 batch] loss: 0.69880, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:17:06.481706 Training: [61 epoch,  60 batch] loss: 0.69374, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:17:48.642728 Training: [61 epoch,  70 batch] loss: 0.70018, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:18:31.084753 Training: [61 epoch,  80 batch] loss: 0.69328, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:19:14.585549 Training: [61 epoch,  90 batch] loss: 0.72683, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38864,MAE：0.17164
2021-01-08 04:21:20.084717 Training: [62 epoch,  10 batch] loss: 0.71893, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:22:00.899998 Training: [62 epoch,  20 batch] loss: 0.69591, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:22:41.781139 Training: [62 epoch,  30 batch] loss: 0.69506, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:23:23.893368 Training: [62 epoch,  40 batch] loss: 0.71239, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:24:05.664081 Training: [62 epoch,  50 batch] loss: 0.75104, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:24:47.431108 Training: [62 epoch,  60 batch] loss: 0.69923, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:25:29.097098 Training: [62 epoch,  70 batch] loss: 0.70301, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:26:10.894707 Training: [62 epoch,  80 batch] loss: 0.69109, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:26:53.167555 Training: [62 epoch,  90 batch] loss: 0.69935, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38866,MAE：0.17331
2021-01-08 04:28:59.498142 Training: [63 epoch,  10 batch] loss: 0.74409, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:29:45.136972 Training: [63 epoch,  20 batch] loss: 0.71129, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:30:26.338210 Training: [63 epoch,  30 batch] loss: 0.70860, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:31:08.389179 Training: [63 epoch,  40 batch] loss: 0.67488, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:31:50.356527 Training: [63 epoch,  50 batch] loss: 0.68932, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:32:32.397967 Training: [63 epoch,  60 batch] loss: 0.68474, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:33:14.534118 Training: [63 epoch,  70 batch] loss: 0.72025, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:33:56.462523 Training: [63 epoch,  80 batch] loss: 0.74234, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:34:38.406430 Training: [63 epoch,  90 batch] loss: 0.70036, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38948,MAE：0.18237
2021-01-08 04:36:41.066439 Training: [64 epoch,  10 batch] loss: 0.73087, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:37:22.124058 Training: [64 epoch,  20 batch] loss: 0.69916, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:38:03.296460 Training: [64 epoch,  30 batch] loss: 0.69336, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:38:45.382847 Training: [64 epoch,  40 batch] loss: 0.70490, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:39:27.540397 Training: [64 epoch,  50 batch] loss: 0.71195, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:40:12.368377 Training: [64 epoch,  60 batch] loss: 0.69972, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:40:58.949385 Training: [64 epoch,  70 batch] loss: 0.70578, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:41:45.445731 Training: [64 epoch,  80 batch] loss: 0.68406, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:42:31.915064 Training: [64 epoch,  90 batch] loss: 0.73145, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38862,MAE：0.17277
2021-01-08 04:44:36.451307 Training: [65 epoch,  10 batch] loss: 0.70202, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:45:17.473460 Training: [65 epoch,  20 batch] loss: 0.71308, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:45:59.266109 Training: [65 epoch,  30 batch] loss: 0.68872, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:46:41.560528 Training: [65 epoch,  40 batch] loss: 0.70705, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:47:23.461501 Training: [65 epoch,  50 batch] loss: 0.69696, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:48:05.276476 Training: [65 epoch,  60 batch] loss: 0.69145, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:48:47.317916 Training: [65 epoch,  70 batch] loss: 0.73727, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:49:29.234739 Training: [65 epoch,  80 batch] loss: 0.69248, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:50:11.201197 Training: [65 epoch,  90 batch] loss: 0.71864, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38902,MAE：0.17815
2021-01-08 04:52:13.050587 Training: [66 epoch,  10 batch] loss: 0.71521, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:52:53.596947 Training: [66 epoch,  20 batch] loss: 0.69054, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:53:35.117634 Training: [66 epoch,  30 batch] loss: 0.69292, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:54:17.183923 Training: [66 epoch,  40 batch] loss: 0.69950, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:54:59.310567 Training: [66 epoch,  50 batch] loss: 0.72196, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:55:41.606203 Training: [66 epoch,  60 batch] loss: 0.70631, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:56:23.798708 Training: [66 epoch,  70 batch] loss: 0.71229, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:57:05.908530 Training: [66 epoch,  80 batch] loss: 0.69530, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 04:57:48.150452 Training: [66 epoch,  90 batch] loss: 0.69489, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39199,MAE：0.20222
2021-01-08 04:59:50.641407 Training: [67 epoch,  10 batch] loss: 0.71278, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:00:31.137590 Training: [67 epoch,  20 batch] loss: 0.69453, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:01:12.518218 Training: [67 epoch,  30 batch] loss: 0.68249, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:01:54.983610 Training: [67 epoch,  40 batch] loss: 0.68451, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:02:36.552748 Training: [67 epoch,  50 batch] loss: 0.72876, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:03:18.024618 Training: [67 epoch,  60 batch] loss: 0.71912, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:03:59.654057 Training: [67 epoch,  70 batch] loss: 0.68995, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:04:42.006396 Training: [67 epoch,  80 batch] loss: 0.70362, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:05:24.396301 Training: [67 epoch,  90 batch] loss: 0.70089, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38911,MAE：0.17959
2021-01-08 05:07:30.472206 Training: [68 epoch,  10 batch] loss: 0.70452, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:08:15.216628 Training: [68 epoch,  20 batch] loss: 0.68767, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:09:01.091254 Training: [68 epoch,  30 batch] loss: 0.71329, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:09:47.033615 Training: [68 epoch,  40 batch] loss: 0.69842, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:10:32.987841 Training: [68 epoch,  50 batch] loss: 0.69872, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:11:19.122091 Training: [68 epoch,  60 batch] loss: 0.67833, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:12:05.251151 Training: [68 epoch,  70 batch] loss: 0.69592, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:12:51.335498 Training: [68 epoch,  80 batch] loss: 0.69447, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:13:37.463775 Training: [68 epoch,  90 batch] loss: 0.73737, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38866,MAE：0.17436
2021-01-08 05:15:45.920408 Training: [69 epoch,  10 batch] loss: 0.72551, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:16:28.033527 Training: [69 epoch,  20 batch] loss: 0.68427, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:17:10.036371 Training: [69 epoch,  30 batch] loss: 0.73395, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:17:51.925847 Training: [69 epoch,  40 batch] loss: 0.70520, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:18:33.922197 Training: [69 epoch,  50 batch] loss: 0.68173, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:19:16.165941 Training: [69 epoch,  60 batch] loss: 0.69449, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:19:58.465135 Training: [69 epoch,  70 batch] loss: 0.72138, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:20:40.278170 Training: [69 epoch,  80 batch] loss: 0.68665, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:21:21.961386 Training: [69 epoch,  90 batch] loss: 0.68175, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38950,MAE：0.18408
2021-01-08 05:23:26.284822 Training: [70 epoch,  10 batch] loss: 0.69051, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:24:12.125298 Training: [70 epoch,  20 batch] loss: 0.73939, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:24:58.024802 Training: [70 epoch,  30 batch] loss: 0.70466, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:25:44.157780 Training: [70 epoch,  40 batch] loss: 0.71568, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:26:30.305521 Training: [70 epoch,  50 batch] loss: 0.69575, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:27:16.545873 Training: [70 epoch,  60 batch] loss: 0.69831, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:28:02.826387 Training: [70 epoch,  70 batch] loss: 0.69904, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:28:49.093330 Training: [70 epoch,  80 batch] loss: 0.70008, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:29:32.759694 Training: [70 epoch,  90 batch] loss: 0.67618, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38803,MAE：0.16241
2021-01-08 05:31:38.428950 Training: [71 epoch,  10 batch] loss: 0.68256, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:32:20.468262 Training: [71 epoch,  20 batch] loss: 0.68916, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:33:02.675999 Training: [71 epoch,  30 batch] loss: 0.70192, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:33:44.606294 Training: [71 epoch,  40 batch] loss: 0.72847, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:34:26.809630 Training: [71 epoch,  50 batch] loss: 0.69425, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:35:09.243274 Training: [71 epoch,  60 batch] loss: 0.68888, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:35:52.617494 Training: [71 epoch,  70 batch] loss: 0.69128, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:36:33.933850 Training: [71 epoch,  80 batch] loss: 0.70659, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:37:16.299907 Training: [71 epoch,  90 batch] loss: 0.69433, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38872,MAE：0.17546
2021-01-08 05:39:15.359085 Training: [72 epoch,  10 batch] loss: 0.70063, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:39:56.693263 Training: [72 epoch,  20 batch] loss: 0.69402, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:40:38.657545 Training: [72 epoch,  30 batch] loss: 0.69097, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:41:20.525881 Training: [72 epoch,  40 batch] loss: 0.70887, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:42:02.442524 Training: [72 epoch,  50 batch] loss: 0.67089, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:42:44.698408 Training: [72 epoch,  60 batch] loss: 0.72447, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:43:27.125317 Training: [72 epoch,  70 batch] loss: 0.70285, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:44:09.472080 Training: [72 epoch,  80 batch] loss: 0.70978, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:44:51.904841 Training: [72 epoch,  90 batch] loss: 0.67920, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.38790,MAE：0.15752
2021-01-08 05:46:51.680287 Training: [73 epoch,  10 batch] loss: 0.68974, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:47:32.895700 Training: [73 epoch,  20 batch] loss: 0.69055, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:48:14.361847 Training: [73 epoch,  30 batch] loss: 0.72003, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:48:56.413381 Training: [73 epoch,  40 batch] loss: 0.70224, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:49:38.340423 Training: [73 epoch,  50 batch] loss: 0.68647, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:50:20.368445 Training: [73 epoch,  60 batch] loss: 0.70417, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:51:02.173426 Training: [73 epoch,  70 batch] loss: 0.69318, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:51:43.977562 Training: [73 epoch,  80 batch] loss: 0.71598, the best RMSE/MAE: 0.39652 / 0.08529
2021-01-08 05:52:26.144812 Training: [73 epoch,  90 batch] loss: 0.71912, the best RMSE/MAE: 0.39652 / 0.08529
<Test> RMSE：0.39041,MAE：0.19246
The best RMSE/MAE：0.39652/0.08529
