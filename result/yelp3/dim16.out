-------------------- Hyperparams --------------------
time: 2021-01-08 09:45:03.334204
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 16
use_cuda: True
2021-01-08 10:01:01.872363 Training: [1 epoch,  10 batch] loss: 6.63339, the best RMSE/MAE: inf / inf
2021-01-08 10:01:29.601082 Training: [1 epoch,  20 batch] loss: 6.36388, the best RMSE/MAE: inf / inf
2021-01-08 10:01:57.452027 Training: [1 epoch,  30 batch] loss: 6.19314, the best RMSE/MAE: inf / inf
2021-01-08 10:02:25.339199 Training: [1 epoch,  40 batch] loss: 6.12427, the best RMSE/MAE: inf / inf
2021-01-08 10:02:53.434378 Training: [1 epoch,  50 batch] loss: 6.01384, the best RMSE/MAE: inf / inf
2021-01-08 10:03:21.133367 Training: [1 epoch,  60 batch] loss: 5.88375, the best RMSE/MAE: inf / inf
2021-01-08 10:03:48.707318 Training: [1 epoch,  70 batch] loss: 5.78524, the best RMSE/MAE: inf / inf
2021-01-08 10:04:16.431387 Training: [1 epoch,  80 batch] loss: 5.72139, the best RMSE/MAE: inf / inf
2021-01-08 10:04:44.106632 Training: [1 epoch,  90 batch] loss: 5.67028, the best RMSE/MAE: inf / inf
<Test> RMSE：637715904.00000,MAE：464132768.00000
2021-01-08 10:06:09.182910 Training: [2 epoch,  10 batch] loss: 5.59847, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:06:36.809720 Training: [2 epoch,  20 batch] loss: 5.54766, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:07:04.465008 Training: [2 epoch,  30 batch] loss: 5.53298, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:07:34.048422 Training: [2 epoch,  40 batch] loss: 5.49651, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:08:03.979757 Training: [2 epoch,  50 batch] loss: 5.53427, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:08:32.715929 Training: [2 epoch,  60 batch] loss: 5.43253, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:09:00.407076 Training: [2 epoch,  70 batch] loss: 5.41418, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:09:37.734347 Training: [2 epoch,  80 batch] loss: 5.43680, the best RMSE/MAE: 637715904.00000 / 464132768.00000
2021-01-08 10:10:19.676974 Training: [2 epoch,  90 batch] loss: 5.46536, the best RMSE/MAE: 637715904.00000 / 464132768.00000
<Test> RMSE：611610.43750,MAE：457975.43750
2021-01-08 10:12:22.547343 Training: [3 epoch,  10 batch] loss: 5.35236, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:13:04.135463 Training: [3 epoch,  20 batch] loss: 5.37480, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:13:46.052271 Training: [3 epoch,  30 batch] loss: 5.34628, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:14:28.056097 Training: [3 epoch,  40 batch] loss: 5.31247, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:15:18.006905 Training: [3 epoch,  50 batch] loss: 5.27577, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:16:11.699012 Training: [3 epoch,  60 batch] loss: 5.32483, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:17:10.098097 Training: [3 epoch,  70 batch] loss: 5.23568, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:18:05.918110 Training: [3 epoch,  80 batch] loss: 5.20658, the best RMSE/MAE: 611610.43750 / 457975.43750
2021-01-08 10:19:01.364550 Training: [3 epoch,  90 batch] loss: 5.22983, the best RMSE/MAE: 611610.43750 / 457975.43750
<Test> RMSE：12874.69336,MAE：9735.15820
2021-01-08 10:21:37.295803 Training: [4 epoch,  10 batch] loss: 5.18285, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:22:30.512087 Training: [4 epoch,  20 batch] loss: 5.18287, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:23:24.585288 Training: [4 epoch,  30 batch] loss: 5.17317, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:24:15.179080 Training: [4 epoch,  40 batch] loss: 5.17889, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:24:57.435622 Training: [4 epoch,  50 batch] loss: 5.14233, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:25:38.646928 Training: [4 epoch,  60 batch] loss: 5.09729, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:26:20.058286 Training: [4 epoch,  70 batch] loss: 5.11310, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:27:02.636690 Training: [4 epoch,  80 batch] loss: 5.08997, the best RMSE/MAE: 12874.69336 / 9735.15820
2021-01-08 10:27:45.072120 Training: [4 epoch,  90 batch] loss: 5.10218, the best RMSE/MAE: 12874.69336 / 9735.15820
<Test> RMSE：1066.70251,MAE：762.26672
2021-01-08 10:29:53.344858 Training: [5 epoch,  10 batch] loss: 5.07214, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:30:35.106434 Training: [5 epoch,  20 batch] loss: 5.04434, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:31:17.408037 Training: [5 epoch,  30 batch] loss: 5.01694, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:31:59.684167 Training: [5 epoch,  40 batch] loss: 4.98963, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:32:42.140089 Training: [5 epoch,  50 batch] loss: 4.99731, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:33:23.072409 Training: [5 epoch,  60 batch] loss: 4.95931, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:34:05.048229 Training: [5 epoch,  70 batch] loss: 4.97338, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:34:47.816716 Training: [5 epoch,  80 batch] loss: 5.02438, the best RMSE/MAE: 1066.70251 / 762.26672
2021-01-08 10:35:29.924792 Training: [5 epoch,  90 batch] loss: 4.93060, the best RMSE/MAE: 1066.70251 / 762.26672
<Test> RMSE：155.40889,MAE：108.38531
2021-01-08 10:37:32.193433 Training: [6 epoch,  10 batch] loss: 4.97051, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:38:26.675398 Training: [6 epoch,  20 batch] loss: 4.92167, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:39:22.670304 Training: [6 epoch,  30 batch] loss: 4.86500, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:40:23.550903 Training: [6 epoch,  40 batch] loss: 4.87104, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:41:21.371109 Training: [6 epoch,  50 batch] loss: 4.83641, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:42:17.808609 Training: [6 epoch,  60 batch] loss: 4.87376, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:43:18.556490 Training: [6 epoch,  70 batch] loss: 4.84915, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:44:12.297287 Training: [6 epoch,  80 batch] loss: 4.82560, the best RMSE/MAE: 155.40889 / 108.38531
2021-01-08 10:45:07.879352 Training: [6 epoch,  90 batch] loss: 4.80167, the best RMSE/MAE: 155.40889 / 108.38531
<Test> RMSE：48.91842,MAE：33.76779
2021-01-08 10:47:23.355835 Training: [7 epoch,  10 batch] loss: 4.77138, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:48:05.139295 Training: [7 epoch,  20 batch] loss: 4.75298, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:48:47.068558 Training: [7 epoch,  30 batch] loss: 4.79207, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:49:29.085400 Training: [7 epoch,  40 batch] loss: 4.75806, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:50:10.228881 Training: [7 epoch,  50 batch] loss: 4.75318, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:50:51.451889 Training: [7 epoch,  60 batch] loss: 4.71920, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:51:34.018287 Training: [7 epoch,  70 batch] loss: 4.69565, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:52:16.175178 Training: [7 epoch,  80 batch] loss: 4.68024, the best RMSE/MAE: 48.91842 / 33.76779
2021-01-08 10:52:58.861341 Training: [7 epoch,  90 batch] loss: 4.64457, the best RMSE/MAE: 48.91842 / 33.76779
<Test> RMSE：17.05324,MAE：11.88892
2021-01-08 10:55:18.613804 Training: [8 epoch,  10 batch] loss: 4.60852, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 10:56:11.255380 Training: [8 epoch,  20 batch] loss: 4.64292, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 10:57:03.298887 Training: [8 epoch,  30 batch] loss: 4.60616, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 10:57:55.641866 Training: [8 epoch,  40 batch] loss: 4.59796, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 10:58:50.524220 Training: [8 epoch,  50 batch] loss: 4.57259, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 10:59:46.500762 Training: [8 epoch,  60 batch] loss: 4.61305, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 11:00:36.712759 Training: [8 epoch,  70 batch] loss: 4.55188, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 11:01:30.952619 Training: [8 epoch,  80 batch] loss: 4.52724, the best RMSE/MAE: 17.05324 / 11.88892
2021-01-08 11:02:26.717007 Training: [8 epoch,  90 batch] loss: 4.51373, the best RMSE/MAE: 17.05324 / 11.88892
<Test> RMSE：7.23150,MAE：5.06528
2021-01-08 11:05:01.346797 Training: [9 epoch,  10 batch] loss: 4.50824, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:05:53.476410 Training: [9 epoch,  20 batch] loss: 4.47191, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:06:50.650366 Training: [9 epoch,  30 batch] loss: 4.52979, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:07:46.054980 Training: [9 epoch,  40 batch] loss: 4.44221, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:08:41.474674 Training: [9 epoch,  50 batch] loss: 4.40200, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:09:32.911239 Training: [9 epoch,  60 batch] loss: 4.39945, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:10:27.726345 Training: [9 epoch,  70 batch] loss: 4.42106, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:11:17.431515 Training: [9 epoch,  80 batch] loss: 4.36519, the best RMSE/MAE: 7.23150 / 5.06528
2021-01-08 11:12:03.628901 Training: [9 epoch,  90 batch] loss: 4.35568, the best RMSE/MAE: 7.23150 / 5.06528
<Test> RMSE：3.58429,MAE：2.47826
2021-01-08 11:14:18.307173 Training: [10 epoch,  10 batch] loss: 4.40595, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:15:03.912396 Training: [10 epoch,  20 batch] loss: 4.33415, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:15:49.609035 Training: [10 epoch,  30 batch] loss: 4.28477, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:16:34.199664 Training: [10 epoch,  40 batch] loss: 4.28417, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:17:19.340616 Training: [10 epoch,  50 batch] loss: 4.26340, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:18:05.323582 Training: [10 epoch,  60 batch] loss: 4.28157, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:18:51.276331 Training: [10 epoch,  70 batch] loss: 4.24388, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:19:37.248397 Training: [10 epoch,  80 batch] loss: 4.21193, the best RMSE/MAE: 3.58429 / 2.47826
2021-01-08 11:20:23.137304 Training: [10 epoch,  90 batch] loss: 4.17906, the best RMSE/MAE: 3.58429 / 2.47826
<Test> RMSE：1.99098,MAE：1.37654
2021-01-08 11:22:32.300493 Training: [11 epoch,  10 batch] loss: 4.16798, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:23:18.198474 Training: [11 epoch,  20 batch] loss: 4.18132, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:24:03.268596 Training: [11 epoch,  30 batch] loss: 4.21134, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:24:48.296781 Training: [11 epoch,  40 batch] loss: 4.10638, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:25:34.809034 Training: [11 epoch,  50 batch] loss: 4.09555, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:26:31.219387 Training: [11 epoch,  60 batch] loss: 4.07436, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:27:26.864674 Training: [11 epoch,  70 batch] loss: 4.06113, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:28:23.572984 Training: [11 epoch,  80 batch] loss: 4.04516, the best RMSE/MAE: 1.99098 / 1.37654
2021-01-08 11:29:19.745970 Training: [11 epoch,  90 batch] loss: 4.08047, the best RMSE/MAE: 1.99098 / 1.37654
<Test> RMSE：1.31224,MAE：0.89179
2021-01-08 11:32:02.342158 Training: [12 epoch,  10 batch] loss: 4.02020, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:32:58.620962 Training: [12 epoch,  20 batch] loss: 4.00972, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:33:54.408421 Training: [12 epoch,  30 batch] loss: 3.96604, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:34:50.117095 Training: [12 epoch,  40 batch] loss: 3.93915, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:35:46.513884 Training: [12 epoch,  50 batch] loss: 3.92688, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:36:43.133842 Training: [12 epoch,  60 batch] loss: 3.92467, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:37:39.609276 Training: [12 epoch,  70 batch] loss: 3.92270, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:38:36.793395 Training: [12 epoch,  80 batch] loss: 3.89657, the best RMSE/MAE: 1.31224 / 0.89179
2021-01-08 11:39:33.605816 Training: [12 epoch,  90 batch] loss: 3.86301, the best RMSE/MAE: 1.31224 / 0.89179
<Test> RMSE：0.80279,MAE：0.52278
2021-01-08 11:42:14.585031 Training: [13 epoch,  10 batch] loss: 3.83596, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:43:09.760389 Training: [13 epoch,  20 batch] loss: 3.83942, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:44:05.027876 Training: [13 epoch,  30 batch] loss: 3.83977, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:45:01.210339 Training: [13 epoch,  40 batch] loss: 3.75696, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:45:58.357978 Training: [13 epoch,  50 batch] loss: 3.77180, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:46:52.757663 Training: [13 epoch,  60 batch] loss: 3.73950, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:47:46.409135 Training: [13 epoch,  70 batch] loss: 3.72668, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:48:39.900432 Training: [13 epoch,  80 batch] loss: 3.73175, the best RMSE/MAE: 0.80279 / 0.52278
2021-01-08 11:49:33.455969 Training: [13 epoch,  90 batch] loss: 3.72218, the best RMSE/MAE: 0.80279 / 0.52278
<Test> RMSE：0.47203,MAE：0.23882
2021-01-08 11:52:08.920881 Training: [14 epoch,  10 batch] loss: 3.64709, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:53:01.123397 Training: [14 epoch,  20 batch] loss: 3.66903, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:53:53.336006 Training: [14 epoch,  30 batch] loss: 3.64542, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:54:46.125096 Training: [14 epoch,  40 batch] loss: 3.61077, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:55:40.308884 Training: [14 epoch,  50 batch] loss: 3.56745, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:56:34.229742 Training: [14 epoch,  60 batch] loss: 3.57286, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:57:28.422202 Training: [14 epoch,  70 batch] loss: 3.57053, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:58:22.622497 Training: [14 epoch,  80 batch] loss: 3.55315, the best RMSE/MAE: 0.47203 / 0.23882
2021-01-08 11:59:16.842976 Training: [14 epoch,  90 batch] loss: 3.51138, the best RMSE/MAE: 0.47203 / 0.23882
<Test> RMSE：0.45776,MAE：0.22587
2021-01-08 12:01:52.702172 Training: [15 epoch,  10 batch] loss: 3.52369, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:02:45.546094 Training: [15 epoch,  20 batch] loss: 3.46421, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:03:38.459828 Training: [15 epoch,  30 batch] loss: 3.47355, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:04:31.344249 Training: [15 epoch,  40 batch] loss: 3.44159, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:05:25.065239 Training: [15 epoch,  50 batch] loss: 3.42577, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:06:19.049606 Training: [15 epoch,  60 batch] loss: 3.40040, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:07:13.161717 Training: [15 epoch,  70 batch] loss: 3.41745, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:08:07.429109 Training: [15 epoch,  80 batch] loss: 3.36512, the best RMSE/MAE: 0.45776 / 0.22587
2021-01-08 12:09:01.457761 Training: [15 epoch,  90 batch] loss: 3.32952, the best RMSE/MAE: 0.45776 / 0.22587
<Test> RMSE：0.41349,MAE：0.16856
2021-01-08 12:11:36.999949 Training: [16 epoch,  10 batch] loss: 3.30790, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:12:29.192484 Training: [16 epoch,  20 batch] loss: 3.36192, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:13:21.080376 Training: [16 epoch,  30 batch] loss: 3.28416, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:14:13.217009 Training: [16 epoch,  40 batch] loss: 3.28875, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:15:06.256277 Training: [16 epoch,  50 batch] loss: 3.25299, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:15:59.926018 Training: [16 epoch,  60 batch] loss: 3.23480, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:16:53.657267 Training: [16 epoch,  70 batch] loss: 3.19857, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:17:47.445039 Training: [16 epoch,  80 batch] loss: 3.18670, the best RMSE/MAE: 0.41349 / 0.16856
2021-01-08 12:18:41.281236 Training: [16 epoch,  90 batch] loss: 3.18195, the best RMSE/MAE: 0.41349 / 0.16856
<Test> RMSE：0.40184,MAE：0.14642
2021-01-08 12:21:16.789537 Training: [17 epoch,  10 batch] loss: 3.23092, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:22:09.551967 Training: [17 epoch,  20 batch] loss: 3.11940, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:23:02.172637 Training: [17 epoch,  30 batch] loss: 3.11843, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:23:55.406102 Training: [17 epoch,  40 batch] loss: 3.09732, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:24:47.796879 Training: [17 epoch,  50 batch] loss: 3.07637, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:25:40.401652 Training: [17 epoch,  60 batch] loss: 3.07223, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:26:33.701922 Training: [17 epoch,  70 batch] loss: 3.01571, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:27:27.491985 Training: [17 epoch,  80 batch] loss: 3.02571, the best RMSE/MAE: 0.40184 / 0.14642
2021-01-08 12:28:21.509362 Training: [17 epoch,  90 batch] loss: 3.03165, the best RMSE/MAE: 0.40184 / 0.14642
<Test> RMSE：0.39159,MAE：0.12502
2021-01-08 12:30:56.468623 Training: [18 epoch,  10 batch] loss: 2.97734, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:31:48.993296 Training: [18 epoch,  20 batch] loss: 2.96700, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:32:41.513840 Training: [18 epoch,  30 batch] loss: 2.94762, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:33:34.028099 Training: [18 epoch,  40 batch] loss: 2.91257, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:34:27.192506 Training: [18 epoch,  50 batch] loss: 2.92162, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:35:20.612219 Training: [18 epoch,  60 batch] loss: 2.88810, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:36:13.749104 Training: [18 epoch,  70 batch] loss: 2.85928, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:37:07.066215 Training: [18 epoch,  80 batch] loss: 2.89677, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:38:00.173469 Training: [18 epoch,  90 batch] loss: 2.85673, the best RMSE/MAE: 0.39159 / 0.12502
<Test> RMSE：0.39838,MAE：0.13654
2021-01-08 12:40:36.517845 Training: [19 epoch,  10 batch] loss: 2.80541, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:41:30.251741 Training: [19 epoch,  20 batch] loss: 2.80753, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:42:24.699298 Training: [19 epoch,  30 batch] loss: 2.79883, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:43:17.990448 Training: [19 epoch,  40 batch] loss: 2.81148, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:44:11.493676 Training: [19 epoch,  50 batch] loss: 2.74207, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:45:05.240934 Training: [19 epoch,  60 batch] loss: 2.72901, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:45:59.289345 Training: [19 epoch,  70 batch] loss: 2.73165, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:46:53.475883 Training: [19 epoch,  80 batch] loss: 2.70792, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:47:47.616620 Training: [19 epoch,  90 batch] loss: 2.72220, the best RMSE/MAE: 0.39159 / 0.12502
<Test> RMSE：0.39828,MAE：0.13329
2021-01-08 12:50:22.578631 Training: [20 epoch,  10 batch] loss: 2.67328, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:51:14.981446 Training: [20 epoch,  20 batch] loss: 2.68771, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:52:08.173475 Training: [20 epoch,  30 batch] loss: 2.62348, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:53:01.073709 Training: [20 epoch,  40 batch] loss: 2.61390, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:53:54.260071 Training: [20 epoch,  50 batch] loss: 2.60664, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:54:47.773618 Training: [20 epoch,  60 batch] loss: 2.56849, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:55:41.448261 Training: [20 epoch,  70 batch] loss: 2.56747, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:56:35.182111 Training: [20 epoch,  80 batch] loss: 2.57094, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 12:57:29.244118 Training: [20 epoch,  90 batch] loss: 2.56929, the best RMSE/MAE: 0.39159 / 0.12502
<Test> RMSE：0.40454,MAE：0.14493
2021-01-08 13:00:04.743999 Training: [21 epoch,  10 batch] loss: 2.53185, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:00:57.610067 Training: [21 epoch,  20 batch] loss: 2.50477, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:01:50.596978 Training: [21 epoch,  30 batch] loss: 2.48906, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:02:43.831267 Training: [21 epoch,  40 batch] loss: 2.47396, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:03:37.202081 Training: [21 epoch,  50 batch] loss: 2.45855, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:04:31.020199 Training: [21 epoch,  60 batch] loss: 2.42793, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:05:25.037684 Training: [21 epoch,  70 batch] loss: 2.46758, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:06:18.764700 Training: [21 epoch,  80 batch] loss: 2.41839, the best RMSE/MAE: 0.39159 / 0.12502
2021-01-08 13:07:12.304581 Training: [21 epoch,  90 batch] loss: 2.37718, the best RMSE/MAE: 0.39159 / 0.12502
<Test> RMSE：0.38854,MAE：0.11621
2021-01-08 13:09:52.443797 Training: [22 epoch,  10 batch] loss: 2.41313, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:10:46.554105 Training: [22 epoch,  20 batch] loss: 2.36844, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:11:42.872700 Training: [22 epoch,  30 batch] loss: 2.32942, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:12:38.083361 Training: [22 epoch,  40 batch] loss: 2.36025, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:13:31.128690 Training: [22 epoch,  50 batch] loss: 2.30068, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:14:24.524318 Training: [22 epoch,  60 batch] loss: 2.30922, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:15:18.380720 Training: [22 epoch,  70 batch] loss: 2.31733, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:16:12.136802 Training: [22 epoch,  80 batch] loss: 2.26565, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:17:05.809528 Training: [22 epoch,  90 batch] loss: 2.28245, the best RMSE/MAE: 0.38854 / 0.11621
<Test> RMSE：0.39835,MAE：0.12277
2021-01-08 13:19:48.004558 Training: [23 epoch,  10 batch] loss: 2.26823, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:20:40.841824 Training: [23 epoch,  20 batch] loss: 2.21151, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:21:33.977322 Training: [23 epoch,  30 batch] loss: 2.20414, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:22:26.500469 Training: [23 epoch,  40 batch] loss: 2.19393, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:23:19.360762 Training: [23 epoch,  50 batch] loss: 2.17522, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:24:12.925162 Training: [23 epoch,  60 batch] loss: 2.17244, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:25:06.600042 Training: [23 epoch,  70 batch] loss: 2.16518, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:26:00.038993 Training: [23 epoch,  80 batch] loss: 2.20646, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:26:53.501210 Training: [23 epoch,  90 batch] loss: 2.15737, the best RMSE/MAE: 0.38854 / 0.11621
<Test> RMSE：0.40394,MAE：0.14206
2021-01-08 13:29:34.809135 Training: [24 epoch,  10 batch] loss: 2.10932, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:30:27.789970 Training: [24 epoch,  20 batch] loss: 2.11149, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:31:20.693195 Training: [24 epoch,  30 batch] loss: 2.08191, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:32:13.985016 Training: [24 epoch,  40 batch] loss: 2.10056, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:33:06.770421 Training: [24 epoch,  50 batch] loss: 2.06747, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:34:00.140473 Training: [24 epoch,  60 batch] loss: 2.07222, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:34:53.972365 Training: [24 epoch,  70 batch] loss: 2.04726, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:35:47.822153 Training: [24 epoch,  80 batch] loss: 2.03389, the best RMSE/MAE: 0.38854 / 0.11621
2021-01-08 13:36:41.745388 Training: [24 epoch,  90 batch] loss: 2.01090, the best RMSE/MAE: 0.38854 / 0.11621
<Test> RMSE：0.39842,MAE：0.11605
2021-01-08 13:39:20.206446 Training: [25 epoch,  10 batch] loss: 2.01864, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:40:14.206975 Training: [25 epoch,  20 batch] loss: 1.96186, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:41:08.280532 Training: [25 epoch,  30 batch] loss: 1.95489, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:42:01.294781 Training: [25 epoch,  40 batch] loss: 1.91711, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:42:54.918165 Training: [25 epoch,  50 batch] loss: 1.95255, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:43:48.713658 Training: [25 epoch,  60 batch] loss: 1.94754, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:44:42.214746 Training: [25 epoch,  70 batch] loss: 1.91385, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:45:35.984946 Training: [25 epoch,  80 batch] loss: 1.91777, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:46:29.709275 Training: [25 epoch,  90 batch] loss: 1.94728, the best RMSE/MAE: 0.39842 / 0.11605
<Test> RMSE：0.40701,MAE：0.14304
2021-01-08 13:49:11.117133 Training: [26 epoch,  10 batch] loss: 1.87780, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:50:02.450943 Training: [26 epoch,  20 batch] loss: 1.89873, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:50:55.004841 Training: [26 epoch,  30 batch] loss: 1.83357, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:51:46.958317 Training: [26 epoch,  40 batch] loss: 1.84908, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:52:39.796036 Training: [26 epoch,  50 batch] loss: 1.82126, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:53:35.839665 Training: [26 epoch,  60 batch] loss: 1.84862, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:54:33.182848 Training: [26 epoch,  70 batch] loss: 1.80958, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:55:28.868064 Training: [26 epoch,  80 batch] loss: 1.85101, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 13:56:25.660811 Training: [26 epoch,  90 batch] loss: 1.80256, the best RMSE/MAE: 0.39842 / 0.11605
<Test> RMSE：0.40013,MAE：0.12463
2021-01-08 13:59:08.457896 Training: [27 epoch,  10 batch] loss: 1.78172, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:00:06.295862 Training: [27 epoch,  20 batch] loss: 1.74125, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:01:04.540615 Training: [27 epoch,  30 batch] loss: 1.77040, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:01:57.624550 Training: [27 epoch,  40 batch] loss: 1.76352, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:02:50.824797 Training: [27 epoch,  50 batch] loss: 1.73598, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:03:44.120894 Training: [27 epoch,  60 batch] loss: 1.75009, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:04:37.913915 Training: [27 epoch,  70 batch] loss: 1.73647, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:05:31.402474 Training: [27 epoch,  80 batch] loss: 1.69172, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:06:25.373407 Training: [27 epoch,  90 batch] loss: 1.68748, the best RMSE/MAE: 0.39842 / 0.11605
<Test> RMSE：0.40517,MAE：0.13215
2021-01-08 14:09:00.143176 Training: [28 epoch,  10 batch] loss: 1.65330, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:09:51.940834 Training: [28 epoch,  20 batch] loss: 1.66889, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:10:43.689445 Training: [28 epoch,  30 batch] loss: 1.67020, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:11:35.763398 Training: [28 epoch,  40 batch] loss: 1.65400, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:12:28.022204 Training: [28 epoch,  50 batch] loss: 1.65395, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:13:21.690053 Training: [28 epoch,  60 batch] loss: 1.61008, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:14:24.565966 Training: [28 epoch,  70 batch] loss: 1.63307, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:15:27.461159 Training: [28 epoch,  80 batch] loss: 1.65732, the best RMSE/MAE: 0.39842 / 0.11605
2021-01-08 14:16:30.484902 Training: [28 epoch,  90 batch] loss: 1.59327, the best RMSE/MAE: 0.39842 / 0.11605
<Test> RMSE：0.39350,MAE：0.10561
2021-01-08 14:19:46.345255 Training: [29 epoch,  10 batch] loss: 1.58632, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:20:48.066892 Training: [29 epoch,  20 batch] loss: 1.55854, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:21:48.346157 Training: [29 epoch,  30 batch] loss: 1.58082, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:22:49.385488 Training: [29 epoch,  40 batch] loss: 1.52953, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:23:51.423808 Training: [29 epoch,  50 batch] loss: 1.60358, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:24:52.741943 Training: [29 epoch,  60 batch] loss: 1.52234, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:25:56.131986 Training: [29 epoch,  70 batch] loss: 1.53648, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:26:59.108651 Training: [29 epoch,  80 batch] loss: 1.53187, the best RMSE/MAE: 0.39350 / 0.10561
2021-01-08 14:28:02.417240 Training: [29 epoch,  90 batch] loss: 1.50746, the best RMSE/MAE: 0.39350 / 0.10561
<Test> RMSE：0.39787,MAE：0.10305
2021-01-08 14:31:17.284327 Training: [30 epoch,  10 batch] loss: 1.49418, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:32:19.816660 Training: [30 epoch,  20 batch] loss: 1.49795, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:33:19.743187 Training: [30 epoch,  30 batch] loss: 1.49448, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:34:20.129330 Training: [30 epoch,  40 batch] loss: 1.51605, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:35:22.228757 Training: [30 epoch,  50 batch] loss: 1.45053, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:36:23.753729 Training: [30 epoch,  60 batch] loss: 1.44349, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:37:27.202718 Training: [30 epoch,  70 batch] loss: 1.45803, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:38:30.450079 Training: [30 epoch,  80 batch] loss: 1.43016, the best RMSE/MAE: 0.39787 / 0.10305
2021-01-08 14:39:33.710120 Training: [30 epoch,  90 batch] loss: 1.43472, the best RMSE/MAE: 0.39787 / 0.10305
<Test> RMSE：0.39723,MAE：0.09768
2021-01-08 14:42:47.133381 Training: [31 epoch,  10 batch] loss: 1.43176, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:43:49.969392 Training: [31 epoch,  20 batch] loss: 1.41186, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:44:50.620757 Training: [31 epoch,  30 batch] loss: 1.40778, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:45:50.165086 Training: [31 epoch,  40 batch] loss: 1.38838, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:46:52.417823 Training: [31 epoch,  50 batch] loss: 1.36199, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:47:53.672028 Training: [31 epoch,  60 batch] loss: 1.35651, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:48:57.022784 Training: [31 epoch,  70 batch] loss: 1.35533, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:50:00.517768 Training: [31 epoch,  80 batch] loss: 1.37065, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:51:03.844394 Training: [31 epoch,  90 batch] loss: 1.42811, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.41738,MAE：0.15693
2021-01-08 14:54:18.607871 Training: [32 epoch,  10 batch] loss: 1.36731, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:55:21.405275 Training: [32 epoch,  20 batch] loss: 1.38421, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:56:24.546683 Training: [32 epoch,  30 batch] loss: 1.34250, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:57:30.063558 Training: [32 epoch,  40 batch] loss: 1.31776, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:58:36.782644 Training: [32 epoch,  50 batch] loss: 1.28401, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 14:59:42.791742 Training: [32 epoch,  60 batch] loss: 1.30078, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:00:50.263012 Training: [32 epoch,  70 batch] loss: 1.28760, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:01:57.563392 Training: [32 epoch,  80 batch] loss: 1.27618, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:03:04.778093 Training: [32 epoch,  90 batch] loss: 1.30236, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.41802,MAE：0.15935
2021-01-08 15:06:21.697876 Training: [33 epoch,  10 batch] loss: 1.30938, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:07:24.525418 Training: [33 epoch,  20 batch] loss: 1.26812, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:08:24.608704 Training: [33 epoch,  30 batch] loss: 1.26449, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:09:24.758624 Training: [33 epoch,  40 batch] loss: 1.27321, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:10:26.133702 Training: [33 epoch,  50 batch] loss: 1.23301, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:11:28.715949 Training: [33 epoch,  60 batch] loss: 1.21645, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:12:32.217362 Training: [33 epoch,  70 batch] loss: 1.24511, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:13:35.605924 Training: [33 epoch,  80 batch] loss: 1.21173, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:14:38.516874 Training: [33 epoch,  90 batch] loss: 1.21689, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40259,MAE：0.11200
2021-01-08 15:17:51.874453 Training: [34 epoch,  10 batch] loss: 1.22336, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:18:54.501539 Training: [34 epoch,  20 batch] loss: 1.19400, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:19:54.684436 Training: [34 epoch,  30 batch] loss: 1.19449, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:20:54.855007 Training: [34 epoch,  40 batch] loss: 1.21168, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:21:56.229866 Training: [34 epoch,  50 batch] loss: 1.16313, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:22:59.123526 Training: [34 epoch,  60 batch] loss: 1.15527, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:24:02.966380 Training: [34 epoch,  70 batch] loss: 1.18177, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:25:06.941603 Training: [34 epoch,  80 batch] loss: 1.14504, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:26:10.654366 Training: [34 epoch,  90 batch] loss: 1.18549, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.39949,MAE：0.10053
2021-01-08 15:29:25.702290 Training: [35 epoch,  10 batch] loss: 1.18709, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:30:28.569960 Training: [35 epoch,  20 batch] loss: 1.13379, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:31:29.611373 Training: [35 epoch,  30 batch] loss: 1.14542, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:32:30.333410 Training: [35 epoch,  40 batch] loss: 1.10501, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:33:33.289622 Training: [35 epoch,  50 batch] loss: 1.13706, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:34:41.904880 Training: [35 epoch,  60 batch] loss: 1.09374, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:36:05.795972 Training: [35 epoch,  70 batch] loss: 1.14089, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:37:30.214161 Training: [35 epoch,  80 batch] loss: 1.10622, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:38:54.706215 Training: [35 epoch,  90 batch] loss: 1.07998, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40319,MAE：0.11177
2021-01-08 15:43:12.078484 Training: [36 epoch,  10 batch] loss: 1.13191, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:44:35.593374 Training: [36 epoch,  20 batch] loss: 1.08333, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:45:58.243503 Training: [36 epoch,  30 batch] loss: 1.08627, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:47:20.717501 Training: [36 epoch,  40 batch] loss: 1.11196, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:48:42.713802 Training: [36 epoch,  50 batch] loss: 1.05638, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:49:59.677802 Training: [36 epoch,  60 batch] loss: 1.04750, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:51:16.725717 Training: [36 epoch,  70 batch] loss: 1.05591, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:52:33.404838 Training: [36 epoch,  80 batch] loss: 1.02809, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:53:50.751079 Training: [36 epoch,  90 batch] loss: 1.04369, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40841,MAE：0.12842
2021-01-08 15:57:17.876684 Training: [37 epoch,  10 batch] loss: 1.01849, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:58:20.093001 Training: [37 epoch,  20 batch] loss: 1.02810, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 15:59:21.859543 Training: [37 epoch,  30 batch] loss: 1.04632, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:00:24.882598 Training: [37 epoch,  40 batch] loss: 1.00661, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:01:26.456558 Training: [37 epoch,  50 batch] loss: 1.00208, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:02:31.710417 Training: [37 epoch,  60 batch] loss: 1.02741, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:03:36.309697 Training: [37 epoch,  70 batch] loss: 1.02105, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:04:42.853428 Training: [37 epoch,  80 batch] loss: 1.01197, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:05:48.006044 Training: [37 epoch,  90 batch] loss: 1.01040, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40900,MAE：0.13109
2021-01-08 16:09:03.750397 Training: [38 epoch,  10 batch] loss: 0.97165, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:10:05.731071 Training: [38 epoch,  20 batch] loss: 1.00715, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:11:07.400120 Training: [38 epoch,  30 batch] loss: 0.98207, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:12:07.604022 Training: [38 epoch,  40 batch] loss: 0.97206, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:13:08.538457 Training: [38 epoch,  50 batch] loss: 0.98211, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:14:11.815036 Training: [38 epoch,  60 batch] loss: 0.98571, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:15:15.289142 Training: [38 epoch,  70 batch] loss: 0.95291, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:16:21.415768 Training: [38 epoch,  80 batch] loss: 0.95071, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:17:27.773987 Training: [38 epoch,  90 batch] loss: 0.92911, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40697,MAE：0.12478
2021-01-08 16:20:50.568600 Training: [39 epoch,  10 batch] loss: 0.92952, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:21:54.589487 Training: [39 epoch,  20 batch] loss: 0.92518, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:22:54.884057 Training: [39 epoch,  30 batch] loss: 0.95653, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:23:55.010048 Training: [39 epoch,  40 batch] loss: 0.91312, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:24:56.197332 Training: [39 epoch,  50 batch] loss: 0.92751, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:25:59.157043 Training: [39 epoch,  60 batch] loss: 0.93622, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:27:02.154504 Training: [39 epoch,  70 batch] loss: 0.90368, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:28:05.224406 Training: [39 epoch,  80 batch] loss: 0.92806, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:29:18.829374 Training: [39 epoch,  90 batch] loss: 0.88921, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40147,MAE：0.10415
2021-01-08 16:33:13.038583 Training: [40 epoch,  10 batch] loss: 0.88989, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:34:28.370779 Training: [40 epoch,  20 batch] loss: 0.90523, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:35:41.958563 Training: [40 epoch,  30 batch] loss: 0.88080, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:36:55.936339 Training: [40 epoch,  40 batch] loss: 0.89073, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:38:11.673790 Training: [40 epoch,  50 batch] loss: 0.85601, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:39:28.950833 Training: [40 epoch,  60 batch] loss: 0.86245, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:40:45.777583 Training: [40 epoch,  70 batch] loss: 0.85811, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:42:01.445848 Training: [40 epoch,  80 batch] loss: 0.90254, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:43:18.037637 Training: [40 epoch,  90 batch] loss: 0.89726, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40265,MAE：0.10866
2021-01-08 16:47:13.214627 Training: [41 epoch,  10 batch] loss: 0.86344, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:48:28.936661 Training: [41 epoch,  20 batch] loss: 0.87419, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:49:42.597249 Training: [41 epoch,  30 batch] loss: 0.83990, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:50:56.406179 Training: [41 epoch,  40 batch] loss: 0.80979, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:52:11.538592 Training: [41 epoch,  50 batch] loss: 0.84340, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:53:27.857870 Training: [41 epoch,  60 batch] loss: 0.82685, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:54:43.612616 Training: [41 epoch,  70 batch] loss: 0.83281, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:55:58.674220 Training: [41 epoch,  80 batch] loss: 0.87215, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 16:57:15.224070 Training: [41 epoch,  90 batch] loss: 0.80023, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.40204,MAE：0.10617
2021-01-08 17:01:11.168805 Training: [42 epoch,  10 batch] loss: 0.80698, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:02:26.969796 Training: [42 epoch,  20 batch] loss: 0.82739, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:03:41.666584 Training: [42 epoch,  30 batch] loss: 0.83742, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:04:57.298255 Training: [42 epoch,  40 batch] loss: 0.78694, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:06:14.464433 Training: [42 epoch,  50 batch] loss: 0.78780, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:07:31.388288 Training: [42 epoch,  60 batch] loss: 0.79775, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:08:46.908010 Training: [42 epoch,  70 batch] loss: 0.79487, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:10:01.421803 Training: [42 epoch,  80 batch] loss: 0.80122, the best RMSE/MAE: 0.39723 / 0.09768
2021-01-08 17:11:17.289808 Training: [42 epoch,  90 batch] loss: 0.77818, the best RMSE/MAE: 0.39723 / 0.09768
<Test> RMSE：0.39532,MAE：0.08682
2021-01-08 17:15:11.958246 Training: [43 epoch,  10 batch] loss: 0.77290, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:16:26.692404 Training: [43 epoch,  20 batch] loss: 0.79235, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:17:39.799654 Training: [43 epoch,  30 batch] loss: 0.77081, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:18:53.816040 Training: [43 epoch,  40 batch] loss: 0.78727, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:20:10.194902 Training: [43 epoch,  50 batch] loss: 0.74639, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:21:28.249428 Training: [43 epoch,  60 batch] loss: 0.82619, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:22:44.615603 Training: [43 epoch,  70 batch] loss: 0.75505, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:23:59.405213 Training: [43 epoch,  80 batch] loss: 0.73036, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:25:15.838447 Training: [43 epoch,  90 batch] loss: 0.73392, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.40037,MAE：0.09896
2021-01-08 17:29:10.221813 Training: [44 epoch,  10 batch] loss: 0.75182, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:30:26.486399 Training: [44 epoch,  20 batch] loss: 0.72266, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:31:41.284140 Training: [44 epoch,  30 batch] loss: 0.73754, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:32:57.382447 Training: [44 epoch,  40 batch] loss: 0.72442, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:34:14.057921 Training: [44 epoch,  50 batch] loss: 0.75468, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:35:31.433683 Training: [44 epoch,  60 batch] loss: 0.73004, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:36:48.636704 Training: [44 epoch,  70 batch] loss: 0.76122, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:38:04.659597 Training: [44 epoch,  80 batch] loss: 0.74125, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:39:22.795435 Training: [44 epoch,  90 batch] loss: 0.72558, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39310,MAE：0.09725
2021-01-08 17:43:14.180787 Training: [45 epoch,  10 batch] loss: 0.70539, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:44:28.181173 Training: [45 epoch,  20 batch] loss: 0.70555, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:45:40.873360 Training: [45 epoch,  30 batch] loss: 0.71122, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:46:54.557424 Training: [45 epoch,  40 batch] loss: 0.70913, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:48:09.848997 Training: [45 epoch,  50 batch] loss: 0.73263, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:49:26.267696 Training: [45 epoch,  60 batch] loss: 0.68535, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:50:41.509797 Training: [45 epoch,  70 batch] loss: 0.73163, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:51:56.083033 Training: [45 epoch,  80 batch] loss: 0.68890, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:53:12.437634 Training: [45 epoch,  90 batch] loss: 0.68621, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39319,MAE：0.09776
2021-01-08 17:57:06.356390 Training: [46 epoch,  10 batch] loss: 0.67323, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:58:20.968751 Training: [46 epoch,  20 batch] loss: 0.66480, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 17:59:34.194857 Training: [46 epoch,  30 batch] loss: 0.67310, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:00:47.925295 Training: [46 epoch,  40 batch] loss: 0.66501, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:02:02.790356 Training: [46 epoch,  50 batch] loss: 0.72859, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:03:18.570613 Training: [46 epoch,  60 batch] loss: 0.69468, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:04:34.086375 Training: [46 epoch,  70 batch] loss: 0.65558, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:05:49.248151 Training: [46 epoch,  80 batch] loss: 0.68447, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:07:06.133764 Training: [46 epoch,  90 batch] loss: 0.65510, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38849,MAE：0.13503
2021-01-08 18:10:57.693872 Training: [47 epoch,  10 batch] loss: 0.65562, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:12:13.228568 Training: [47 epoch,  20 batch] loss: 0.64349, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:13:26.457401 Training: [47 epoch,  30 batch] loss: 0.64203, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:14:41.172991 Training: [47 epoch,  40 batch] loss: 0.69200, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:15:56.625598 Training: [47 epoch,  50 batch] loss: 0.64310, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:17:13.541306 Training: [47 epoch,  60 batch] loss: 0.64970, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:18:29.657671 Training: [47 epoch,  70 batch] loss: 0.63979, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:19:44.750709 Training: [47 epoch,  80 batch] loss: 0.61952, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:21:01.034271 Training: [47 epoch,  90 batch] loss: 0.67834, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38813,MAE：0.13921
2021-01-08 18:24:54.965158 Training: [48 epoch,  10 batch] loss: 0.67216, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:26:09.658398 Training: [48 epoch,  20 batch] loss: 0.60952, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:27:22.706106 Training: [48 epoch,  30 batch] loss: 0.64967, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:28:37.385850 Training: [48 epoch,  40 batch] loss: 0.62333, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:29:52.992214 Training: [48 epoch,  50 batch] loss: 0.60569, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:31:09.584076 Training: [48 epoch,  60 batch] loss: 0.62146, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:32:25.501824 Training: [48 epoch,  70 batch] loss: 0.63534, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:33:41.053439 Training: [48 epoch,  80 batch] loss: 0.60222, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:34:57.824880 Training: [48 epoch,  90 batch] loss: 0.59766, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39030,MAE：0.19152
2021-01-08 18:38:51.923953 Training: [49 epoch,  10 batch] loss: 0.60114, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:40:07.533568 Training: [49 epoch,  20 batch] loss: 0.61002, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:41:21.768502 Training: [49 epoch,  30 batch] loss: 0.61118, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:42:38.019206 Training: [49 epoch,  40 batch] loss: 0.58920, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:43:53.724789 Training: [49 epoch,  50 batch] loss: 0.60023, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:45:10.257134 Training: [49 epoch,  60 batch] loss: 0.62365, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:46:25.434473 Training: [49 epoch,  70 batch] loss: 0.63349, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:47:39.619340 Training: [49 epoch,  80 batch] loss: 0.59202, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:48:55.728177 Training: [49 epoch,  90 batch] loss: 0.57903, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39046,MAE：0.19184
2021-01-08 18:52:51.714453 Training: [50 epoch,  10 batch] loss: 0.59186, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:54:05.845494 Training: [50 epoch,  20 batch] loss: 0.58929, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:55:19.673209 Training: [50 epoch,  30 batch] loss: 0.57954, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:56:35.635760 Training: [50 epoch,  40 batch] loss: 0.62795, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:57:51.796516 Training: [50 epoch,  50 batch] loss: 0.55838, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 18:59:08.558158 Training: [50 epoch,  60 batch] loss: 0.57515, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:00:24.618490 Training: [50 epoch,  70 batch] loss: 0.58859, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:01:40.149401 Training: [50 epoch,  80 batch] loss: 0.56335, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:02:56.352686 Training: [50 epoch,  90 batch] loss: 0.54882, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39172,MAE：0.20053
2021-01-08 19:06:51.136884 Training: [51 epoch,  10 batch] loss: 0.57979, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:08:04.787673 Training: [51 epoch,  20 batch] loss: 0.58883, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:09:18.081282 Training: [51 epoch,  30 batch] loss: 0.56592, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:10:32.832711 Training: [51 epoch,  40 batch] loss: 0.55935, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:11:48.170665 Training: [51 epoch,  50 batch] loss: 0.55197, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:13:03.792722 Training: [51 epoch,  60 batch] loss: 0.56239, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:14:18.872102 Training: [51 epoch,  70 batch] loss: 0.54203, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:15:33.838402 Training: [51 epoch,  80 batch] loss: 0.53047, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:16:49.873571 Training: [51 epoch,  90 batch] loss: 0.54779, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38793,MAE：0.15132
2021-01-08 19:20:44.260186 Training: [52 epoch,  10 batch] loss: 0.52503, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:21:58.157822 Training: [52 epoch,  20 batch] loss: 0.54581, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:23:11.510132 Training: [52 epoch,  30 batch] loss: 0.51520, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:24:26.537377 Training: [52 epoch,  40 batch] loss: 0.58258, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:25:41.589968 Training: [52 epoch,  50 batch] loss: 0.53952, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:26:58.210549 Training: [52 epoch,  60 batch] loss: 0.56170, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:28:13.893874 Training: [52 epoch,  70 batch] loss: 0.53732, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:29:28.859158 Training: [52 epoch,  80 batch] loss: 0.52631, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:30:45.445217 Training: [52 epoch,  90 batch] loss: 0.51408, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38794,MAE：0.15585
2021-01-08 19:34:39.077688 Training: [53 epoch,  10 batch] loss: 0.50438, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:35:53.128834 Training: [53 epoch,  20 batch] loss: 0.54728, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:37:06.319664 Training: [53 epoch,  30 batch] loss: 0.49743, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:38:20.700789 Training: [53 epoch,  40 batch] loss: 0.53102, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:39:35.571230 Training: [53 epoch,  50 batch] loss: 0.50486, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:40:52.449592 Training: [53 epoch,  60 batch] loss: 0.55708, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:42:07.745920 Training: [53 epoch,  70 batch] loss: 0.49839, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:43:22.126135 Training: [53 epoch,  80 batch] loss: 0.55410, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:44:38.793097 Training: [53 epoch,  90 batch] loss: 0.49800, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39008,MAE：0.11829
2021-01-08 19:48:33.125687 Training: [54 epoch,  10 batch] loss: 0.49997, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:49:48.033633 Training: [54 epoch,  20 batch] loss: 0.51564, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:51:01.181723 Training: [54 epoch,  30 batch] loss: 0.49192, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:52:15.685758 Training: [54 epoch,  40 batch] loss: 0.52061, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:53:30.910505 Training: [54 epoch,  50 batch] loss: 0.51536, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:54:47.794489 Training: [54 epoch,  60 batch] loss: 0.50050, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:56:02.741057 Training: [54 epoch,  70 batch] loss: 0.47573, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:57:17.453819 Training: [54 epoch,  80 batch] loss: 0.47243, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 19:58:34.257754 Training: [54 epoch,  90 batch] loss: 0.49094, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38869,MAE：0.13236
2021-01-08 20:02:27.840784 Training: [55 epoch,  10 batch] loss: 0.52163, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:03:43.008577 Training: [55 epoch,  20 batch] loss: 0.46890, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:04:56.136871 Training: [55 epoch,  30 batch] loss: 0.48971, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:06:10.271591 Training: [55 epoch,  40 batch] loss: 0.49136, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:07:25.578885 Training: [55 epoch,  50 batch] loss: 0.45756, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:08:42.808299 Training: [55 epoch,  60 batch] loss: 0.47451, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:09:59.081740 Training: [55 epoch,  70 batch] loss: 0.50043, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:11:14.452201 Training: [55 epoch,  80 batch] loss: 0.46810, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:12:31.050132 Training: [55 epoch,  90 batch] loss: 0.48142, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38794,MAE：0.15835
2021-01-08 20:16:24.015241 Training: [56 epoch,  10 batch] loss: 0.47947, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:17:38.545622 Training: [56 epoch,  20 batch] loss: 0.47869, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:18:51.148633 Training: [56 epoch,  30 batch] loss: 0.45304, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:20:05.440438 Training: [56 epoch,  40 batch] loss: 0.45305, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:21:20.370423 Training: [56 epoch,  50 batch] loss: 0.43934, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:22:37.037333 Training: [56 epoch,  60 batch] loss: 0.50828, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:23:52.900998 Training: [56 epoch,  70 batch] loss: 0.47098, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:25:07.636220 Training: [56 epoch,  80 batch] loss: 0.45809, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:26:24.361755 Training: [56 epoch,  90 batch] loss: 0.44967, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38984,MAE：0.11965
2021-01-08 20:30:16.489033 Training: [57 epoch,  10 batch] loss: 0.44672, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:31:31.226457 Training: [57 epoch,  20 batch] loss: 0.44594, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:32:45.164430 Training: [57 epoch,  30 batch] loss: 0.45242, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:33:59.857681 Training: [57 epoch,  40 batch] loss: 0.47712, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:35:14.685865 Training: [57 epoch,  50 batch] loss: 0.47791, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:36:31.410598 Training: [57 epoch,  60 batch] loss: 0.44797, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:37:47.309389 Training: [57 epoch,  70 batch] loss: 0.43549, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:39:02.337205 Training: [57 epoch,  80 batch] loss: 0.47476, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:40:18.622702 Training: [57 epoch,  90 batch] loss: 0.43014, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38942,MAE：0.12355
2021-01-08 20:44:12.192939 Training: [58 epoch,  10 batch] loss: 0.47815, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:45:26.750173 Training: [58 epoch,  20 batch] loss: 0.43955, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:46:39.304850 Training: [58 epoch,  30 batch] loss: 0.46964, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:47:53.497653 Training: [58 epoch,  40 batch] loss: 0.45515, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:49:09.128746 Training: [58 epoch,  50 batch] loss: 0.42261, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:50:26.081125 Training: [58 epoch,  60 batch] loss: 0.43529, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:51:41.998463 Training: [58 epoch,  70 batch] loss: 0.42864, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:52:57.181590 Training: [58 epoch,  80 batch] loss: 0.43293, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:54:12.962436 Training: [58 epoch,  90 batch] loss: 0.42713, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38797,MAE：0.15723
2021-01-08 20:58:05.843692 Training: [59 epoch,  10 batch] loss: 0.43311, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 20:59:20.686992 Training: [59 epoch,  20 batch] loss: 0.44427, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:00:34.425979 Training: [59 epoch,  30 batch] loss: 0.43240, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:01:49.044271 Training: [59 epoch,  40 batch] loss: 0.42877, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:03:03.716975 Training: [59 epoch,  50 batch] loss: 0.42646, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:04:19.721714 Training: [59 epoch,  60 batch] loss: 0.41926, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:05:35.377791 Training: [59 epoch,  70 batch] loss: 0.41417, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:06:50.490420 Training: [59 epoch,  80 batch] loss: 0.42681, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:08:06.289228 Training: [59 epoch,  90 batch] loss: 0.45176, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38857,MAE：0.13416
2021-01-08 21:11:58.473913 Training: [60 epoch,  10 batch] loss: 0.42055, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:13:14.137251 Training: [60 epoch,  20 batch] loss: 0.39987, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:14:27.504518 Training: [60 epoch,  30 batch] loss: 0.40706, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:15:42.557972 Training: [60 epoch,  40 batch] loss: 0.40291, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:16:57.866216 Training: [60 epoch,  50 batch] loss: 0.43004, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:18:14.714157 Training: [60 epoch,  60 batch] loss: 0.42105, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:19:30.489826 Training: [60 epoch,  70 batch] loss: 0.44607, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:20:45.440934 Training: [60 epoch,  80 batch] loss: 0.43291, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:22:01.694094 Training: [60 epoch,  90 batch] loss: 0.46940, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38818,MAE：0.16424
2021-01-08 21:25:52.205257 Training: [61 epoch,  10 batch] loss: 0.42206, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:27:07.323905 Training: [61 epoch,  20 batch] loss: 0.39762, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:28:20.982897 Training: [61 epoch,  30 batch] loss: 0.45941, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:29:36.572607 Training: [61 epoch,  40 batch] loss: 0.41942, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:30:52.297737 Training: [61 epoch,  50 batch] loss: 0.44603, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:32:09.166318 Training: [61 epoch,  60 batch] loss: 0.44060, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:33:25.286454 Training: [61 epoch,  70 batch] loss: 0.43857, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:34:40.314609 Training: [61 epoch,  80 batch] loss: 0.41451, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:35:56.574183 Training: [61 epoch,  90 batch] loss: 0.39699, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38835,MAE：0.16878
2021-01-08 21:39:52.173565 Training: [62 epoch,  10 batch] loss: 0.39465, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:41:05.801895 Training: [62 epoch,  20 batch] loss: 0.42814, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:42:18.849912 Training: [62 epoch,  30 batch] loss: 0.43161, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:43:34.109972 Training: [62 epoch,  40 batch] loss: 0.42072, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:44:49.069519 Training: [62 epoch,  50 batch] loss: 0.40743, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:46:04.890180 Training: [62 epoch,  60 batch] loss: 0.40626, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:47:19.952674 Training: [62 epoch,  70 batch] loss: 0.40736, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:48:34.498159 Training: [62 epoch,  80 batch] loss: 0.47191, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:49:50.644856 Training: [62 epoch,  90 batch] loss: 0.43668, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38823,MAE：0.14082
2021-01-08 21:53:44.798043 Training: [63 epoch,  10 batch] loss: 0.43366, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:54:59.178524 Training: [63 epoch,  20 batch] loss: 0.40177, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:56:12.630792 Training: [63 epoch,  30 batch] loss: 0.41019, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:57:27.805239 Training: [63 epoch,  40 batch] loss: 0.40991, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:58:42.892701 Training: [63 epoch,  50 batch] loss: 0.40382, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 21:59:58.509847 Training: [63 epoch,  60 batch] loss: 0.41185, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:01:13.455055 Training: [63 epoch,  70 batch] loss: 0.44395, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:02:28.235493 Training: [63 epoch,  80 batch] loss: 0.42564, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:03:44.251932 Training: [63 epoch,  90 batch] loss: 0.43081, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38815,MAE：0.14122
2021-01-08 22:07:37.589912 Training: [64 epoch,  10 batch] loss: 0.42139, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:08:52.297180 Training: [64 epoch,  20 batch] loss: 0.40453, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:10:05.457593 Training: [64 epoch,  30 batch] loss: 0.43060, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:11:20.361815 Training: [64 epoch,  40 batch] loss: 0.45127, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:12:34.652285 Training: [64 epoch,  50 batch] loss: 0.40911, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:13:50.157928 Training: [64 epoch,  60 batch] loss: 0.40039, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:15:05.476797 Training: [64 epoch,  70 batch] loss: 0.38682, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:16:20.442984 Training: [64 epoch,  80 batch] loss: 0.40939, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:17:36.291327 Training: [64 epoch,  90 batch] loss: 0.42822, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.39829,MAE：0.23169
2021-01-08 22:21:28.913882 Training: [65 epoch,  10 batch] loss: 0.46202, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:22:44.090717 Training: [65 epoch,  20 batch] loss: 0.40192, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:23:57.203166 Training: [65 epoch,  30 batch] loss: 0.40028, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:25:12.016507 Training: [65 epoch,  40 batch] loss: 0.39611, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:26:26.932177 Training: [65 epoch,  50 batch] loss: 0.40705, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:27:43.454829 Training: [65 epoch,  60 batch] loss: 0.41599, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:28:59.806022 Training: [65 epoch,  70 batch] loss: 0.43224, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:30:14.957823 Training: [65 epoch,  80 batch] loss: 0.41853, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:31:30.865630 Training: [65 epoch,  90 batch] loss: 0.41984, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38938,MAE：0.18236
2021-01-08 22:35:23.742249 Training: [66 epoch,  10 batch] loss: 0.38818, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:36:38.793509 Training: [66 epoch,  20 batch] loss: 0.41675, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:37:52.339712 Training: [66 epoch,  30 batch] loss: 0.40677, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:39:07.745659 Training: [66 epoch,  40 batch] loss: 0.41577, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:40:22.059740 Training: [66 epoch,  50 batch] loss: 0.42736, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:41:37.932198 Training: [66 epoch,  60 batch] loss: 0.40615, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:42:54.269926 Training: [66 epoch,  70 batch] loss: 0.43273, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:44:09.301712 Training: [66 epoch,  80 batch] loss: 0.43445, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:45:25.021661 Training: [66 epoch,  90 batch] loss: 0.42317, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38797,MAE：0.15967
2021-01-08 22:49:19.993678 Training: [67 epoch,  10 batch] loss: 0.41125, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:50:35.210632 Training: [67 epoch,  20 batch] loss: 0.41344, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:51:49.391407 Training: [67 epoch,  30 batch] loss: 0.42241, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:53:05.726867 Training: [67 epoch,  40 batch] loss: 0.42633, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:54:21.441623 Training: [67 epoch,  50 batch] loss: 0.41305, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:55:38.444480 Training: [67 epoch,  60 batch] loss: 0.43215, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:56:55.554776 Training: [67 epoch,  70 batch] loss: 0.45807, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:58:11.169132 Training: [67 epoch,  80 batch] loss: 0.39623, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 22:59:27.773054 Training: [67 epoch,  90 batch] loss: 0.40031, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38857,MAE：0.17313
2021-01-08 23:03:22.325626 Training: [68 epoch,  10 batch] loss: 0.44408, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:04:36.998268 Training: [68 epoch,  20 batch] loss: 0.39634, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:05:50.517593 Training: [68 epoch,  30 batch] loss: 0.42216, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:07:06.096931 Training: [68 epoch,  40 batch] loss: 0.40676, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:08:20.924825 Training: [68 epoch,  50 batch] loss: 0.40756, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:09:36.693301 Training: [68 epoch,  60 batch] loss: 0.39838, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:10:52.422076 Training: [68 epoch,  70 batch] loss: 0.38694, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:12:07.067037 Training: [68 epoch,  80 batch] loss: 0.46079, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:13:22.702382 Training: [68 epoch,  90 batch] loss: 0.41205, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38794,MAE：0.14788
2021-01-08 23:17:18.059790 Training: [69 epoch,  10 batch] loss: 0.39385, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:18:33.286323 Training: [69 epoch,  20 batch] loss: 0.41408, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:19:43.884839 Training: [69 epoch,  30 batch] loss: 0.42852, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:20:39.928156 Training: [69 epoch,  40 batch] loss: 0.39653, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:21:33.290674 Training: [69 epoch,  50 batch] loss: 0.46287, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:22:26.910313 Training: [69 epoch,  60 batch] loss: 0.40687, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:23:20.154173 Training: [69 epoch,  70 batch] loss: 0.39831, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:24:13.761692 Training: [69 epoch,  80 batch] loss: 0.40991, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:25:07.869724 Training: [69 epoch,  90 batch] loss: 0.43911, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38901,MAE：0.12867
2021-01-08 23:27:44.310956 Training: [70 epoch,  10 batch] loss: 0.45869, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:28:38.245366 Training: [70 epoch,  20 batch] loss: 0.39922, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:29:32.577594 Training: [70 epoch,  30 batch] loss: 0.39527, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:30:26.372576 Training: [70 epoch,  40 batch] loss: 0.41756, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:31:19.637174 Training: [70 epoch,  50 batch] loss: 0.42796, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:32:13.758754 Training: [70 epoch,  60 batch] loss: 0.38267, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:33:06.930223 Training: [70 epoch,  70 batch] loss: 0.39822, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:34:00.050571 Training: [70 epoch,  80 batch] loss: 0.44190, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:34:54.021242 Training: [70 epoch,  90 batch] loss: 0.39955, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38944,MAE：0.12368
2021-01-08 23:37:29.744194 Training: [71 epoch,  10 batch] loss: 0.43780, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:38:23.325411 Training: [71 epoch,  20 batch] loss: 0.40390, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:39:17.505980 Training: [71 epoch,  30 batch] loss: 0.40961, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:40:11.352127 Training: [71 epoch,  40 batch] loss: 0.41310, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:41:05.074185 Training: [71 epoch,  50 batch] loss: 0.40713, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:41:59.034497 Training: [71 epoch,  60 batch] loss: 0.39527, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:42:52.792931 Training: [71 epoch,  70 batch] loss: 0.41646, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:43:46.083829 Training: [71 epoch,  80 batch] loss: 0.42100, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:44:40.042236 Training: [71 epoch,  90 batch] loss: 0.42112, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38970,MAE：0.12121
2021-01-08 23:47:16.182592 Training: [72 epoch,  10 batch] loss: 0.39508, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:48:10.305334 Training: [72 epoch,  20 batch] loss: 0.40273, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:49:04.486069 Training: [72 epoch,  30 batch] loss: 0.40951, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:49:58.228131 Training: [72 epoch,  40 batch] loss: 0.40411, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:50:51.939141 Training: [72 epoch,  50 batch] loss: 0.39769, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:51:45.521743 Training: [72 epoch,  60 batch] loss: 0.38179, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:52:39.328648 Training: [72 epoch,  70 batch] loss: 0.43544, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:53:32.646484 Training: [72 epoch,  80 batch] loss: 0.43186, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:54:26.318399 Training: [72 epoch,  90 batch] loss: 0.40975, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38792,MAE：0.15018
2021-01-08 23:57:01.647655 Training: [73 epoch,  10 batch] loss: 0.37943, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:57:55.274350 Training: [73 epoch,  20 batch] loss: 0.43377, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:58:48.950804 Training: [73 epoch,  30 batch] loss: 0.39658, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-08 23:59:42.085027 Training: [73 epoch,  40 batch] loss: 0.46536, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-09 00:00:35.589157 Training: [73 epoch,  50 batch] loss: 0.41364, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-09 00:01:29.494198 Training: [73 epoch,  60 batch] loss: 0.39310, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-09 00:02:23.013648 Training: [73 epoch,  70 batch] loss: 0.41622, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-09 00:03:16.390944 Training: [73 epoch,  80 batch] loss: 0.43045, the best RMSE/MAE: 0.39532 / 0.08682
2021-01-09 00:04:09.950601 Training: [73 epoch,  90 batch] loss: 0.41035, the best RMSE/MAE: 0.39532 / 0.08682
<Test> RMSE：0.38808,MAE：0.14255
The best RMSE/MAE：0.39532/0.08682
