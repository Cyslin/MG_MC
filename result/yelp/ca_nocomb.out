-------------------- Hyperparams --------------------
time: 2021-01-02 19:32:21.403975
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-02 19:42:15.710599 Training: [1 epoch,  10 batch] loss: 7.93484, the best RMSE/MAE: inf / inf
2021-01-02 19:42:31.646746 Training: [1 epoch,  20 batch] loss: 7.68274, the best RMSE/MAE: inf / inf
2021-01-02 19:42:48.020058 Training: [1 epoch,  30 batch] loss: 7.58868, the best RMSE/MAE: inf / inf
2021-01-02 19:43:03.590895 Training: [1 epoch,  40 batch] loss: 7.42244, the best RMSE/MAE: inf / inf
2021-01-02 19:43:19.916493 Training: [1 epoch,  50 batch] loss: 7.38375, the best RMSE/MAE: inf / inf
2021-01-02 19:43:36.102607 Training: [1 epoch,  60 batch] loss: 7.36830, the best RMSE/MAE: inf / inf
2021-01-02 19:43:52.560563 Training: [1 epoch,  70 batch] loss: 7.26711, the best RMSE/MAE: inf / inf
2021-01-02 19:44:09.069394 Training: [1 epoch,  80 batch] loss: 7.25061, the best RMSE/MAE: inf / inf
2021-01-02 19:44:25.819133 Training: [1 epoch,  90 batch] loss: 7.23890, the best RMSE/MAE: inf / inf
<Test> RMSE：258174912.00000,MAE：204762416.00000
2021-01-02 19:45:07.658498 Training: [2 epoch,  10 batch] loss: 7.19433, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:45:22.123471 Training: [2 epoch,  20 batch] loss: 7.17458, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:45:36.795003 Training: [2 epoch,  30 batch] loss: 7.11088, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:45:51.383174 Training: [2 epoch,  40 batch] loss: 7.13353, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:46:07.163512 Training: [2 epoch,  50 batch] loss: 7.06327, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:46:23.464054 Training: [2 epoch,  60 batch] loss: 7.06225, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:46:39.857526 Training: [2 epoch,  70 batch] loss: 7.09074, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:46:56.295722 Training: [2 epoch,  80 batch] loss: 7.03309, the best RMSE/MAE: 258174912.00000 / 204762416.00000
2021-01-02 19:47:12.456603 Training: [2 epoch,  90 batch] loss: 6.98881, the best RMSE/MAE: 258174912.00000 / 204762416.00000
<Test> RMSE：454276.31250,MAE：367643.68750
2021-01-02 19:47:56.953083 Training: [3 epoch,  10 batch] loss: 7.04273, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:48:12.915911 Training: [3 epoch,  20 batch] loss: 6.97005, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:48:29.299355 Training: [3 epoch,  30 batch] loss: 6.92100, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:48:44.721457 Training: [3 epoch,  40 batch] loss: 6.89577, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:49:01.071569 Training: [3 epoch,  50 batch] loss: 6.92173, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:49:18.069866 Training: [3 epoch,  60 batch] loss: 6.92532, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:49:34.273372 Training: [3 epoch,  70 batch] loss: 6.85563, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:49:50.206365 Training: [3 epoch,  80 batch] loss: 6.84389, the best RMSE/MAE: 454276.31250 / 367643.68750
2021-01-02 19:50:07.122428 Training: [3 epoch,  90 batch] loss: 6.87762, the best RMSE/MAE: 454276.31250 / 367643.68750
<Test> RMSE：12317.83691,MAE：10129.02734
2021-01-02 19:50:48.122449 Training: [4 epoch,  10 batch] loss: 6.86604, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:51:04.470355 Training: [4 epoch,  20 batch] loss: 6.80789, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:51:21.170392 Training: [4 epoch,  30 batch] loss: 6.79668, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:51:38.054661 Training: [4 epoch,  40 batch] loss: 6.78015, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:51:54.838584 Training: [4 epoch,  50 batch] loss: 6.76246, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:52:10.797534 Training: [4 epoch,  60 batch] loss: 6.73848, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:52:27.562337 Training: [4 epoch,  70 batch] loss: 6.72093, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:52:45.073420 Training: [4 epoch,  80 batch] loss: 6.69467, the best RMSE/MAE: 12317.83691 / 10129.02734
2021-01-02 19:53:01.288127 Training: [4 epoch,  90 batch] loss: 6.72122, the best RMSE/MAE: 12317.83691 / 10129.02734
<Test> RMSE：1149.72266,MAE：959.36237
2021-01-02 19:53:46.363556 Training: [5 epoch,  10 batch] loss: 6.67303, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:54:09.410098 Training: [5 epoch,  20 batch] loss: 6.66039, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:54:31.017426 Training: [5 epoch,  30 batch] loss: 6.64626, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:54:53.973899 Training: [5 epoch,  40 batch] loss: 6.63354, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:55:17.518494 Training: [5 epoch,  50 batch] loss: 6.66977, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:55:43.169932 Training: [5 epoch,  60 batch] loss: 6.58567, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:56:04.845936 Training: [5 epoch,  70 batch] loss: 6.60250, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:56:27.911480 Training: [5 epoch,  80 batch] loss: 6.58550, the best RMSE/MAE: 1149.72266 / 959.36237
2021-01-02 19:56:51.847869 Training: [5 epoch,  90 batch] loss: 6.54175, the best RMSE/MAE: 1149.72266 / 959.36237
<Test> RMSE：235.68416,MAE：189.60007
2021-01-02 19:57:54.623112 Training: [6 epoch,  10 batch] loss: 6.51700, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:58:17.510691 Training: [6 epoch,  20 batch] loss: 6.49304, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:58:39.119293 Training: [6 epoch,  30 batch] loss: 6.48425, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:59:00.371324 Training: [6 epoch,  40 batch] loss: 6.53761, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:59:16.148810 Training: [6 epoch,  50 batch] loss: 6.46754, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:59:32.623127 Training: [6 epoch,  60 batch] loss: 6.42457, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 19:59:47.782747 Training: [6 epoch,  70 batch] loss: 6.42992, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 20:00:07.503605 Training: [6 epoch,  80 batch] loss: 6.41924, the best RMSE/MAE: 235.68416 / 189.60007
2021-01-02 20:00:30.464322 Training: [6 epoch,  90 batch] loss: 6.41552, the best RMSE/MAE: 235.68416 / 189.60007
<Test> RMSE：65.62557,MAE：54.02304
2021-01-02 20:01:31.287620 Training: [7 epoch,  10 batch] loss: 6.35064, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:01:56.334421 Training: [7 epoch,  20 batch] loss: 6.33781, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:02:20.045148 Training: [7 epoch,  30 batch] loss: 6.32593, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:02:42.436751 Training: [7 epoch,  40 batch] loss: 6.32275, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:03:08.566279 Training: [7 epoch,  50 batch] loss: 6.28778, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:03:32.124900 Training: [7 epoch,  60 batch] loss: 6.28615, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:03:55.589019 Training: [7 epoch,  70 batch] loss: 6.24048, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:04:18.109937 Training: [7 epoch,  80 batch] loss: 6.25512, the best RMSE/MAE: 65.62557 / 54.02304
2021-01-02 20:04:42.073834 Training: [7 epoch,  90 batch] loss: 6.21257, the best RMSE/MAE: 65.62557 / 54.02304
<Test> RMSE：31.22496,MAE：24.93462
2021-01-02 20:05:45.084353 Training: [8 epoch,  10 batch] loss: 6.18092, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:06:06.240042 Training: [8 epoch,  20 batch] loss: 6.15352, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:06:30.823788 Training: [8 epoch,  30 batch] loss: 6.17455, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:06:54.526251 Training: [8 epoch,  40 batch] loss: 6.15112, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:07:17.257441 Training: [8 epoch,  50 batch] loss: 6.14233, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:07:41.041098 Training: [8 epoch,  60 batch] loss: 6.08687, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:08:03.412702 Training: [8 epoch,  70 batch] loss: 6.10563, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:08:27.278081 Training: [8 epoch,  80 batch] loss: 6.03954, the best RMSE/MAE: 31.22496 / 24.93462
2021-01-02 20:08:49.747288 Training: [8 epoch,  90 batch] loss: 6.03008, the best RMSE/MAE: 31.22496 / 24.93462
<Test> RMSE：15.80085,MAE：13.07226
2021-01-02 20:09:53.206714 Training: [9 epoch,  10 batch] loss: 5.99664, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:10:15.010140 Training: [9 epoch,  20 batch] loss: 5.96448, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:10:37.299703 Training: [9 epoch,  30 batch] loss: 5.95221, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:11:01.016857 Training: [9 epoch,  40 batch] loss: 5.94305, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:11:25.457387 Training: [9 epoch,  50 batch] loss: 5.94897, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:11:51.047538 Training: [9 epoch,  60 batch] loss: 5.89789, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:12:14.757978 Training: [9 epoch,  70 batch] loss: 5.87003, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:12:38.898937 Training: [9 epoch,  80 batch] loss: 5.94862, the best RMSE/MAE: 15.80085 / 13.07226
2021-01-02 20:13:03.365516 Training: [9 epoch,  90 batch] loss: 5.81821, the best RMSE/MAE: 15.80085 / 13.07226
<Test> RMSE：7.72629,MAE：6.46591
2021-01-02 20:14:06.872323 Training: [10 epoch,  10 batch] loss: 5.79289, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:14:32.168296 Training: [10 epoch,  20 batch] loss: 5.79670, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:14:59.623056 Training: [10 epoch,  30 batch] loss: 5.79393, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:15:26.316674 Training: [10 epoch,  40 batch] loss: 5.73217, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:15:50.965261 Training: [10 epoch,  50 batch] loss: 5.70471, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:16:16.012371 Training: [10 epoch,  60 batch] loss: 5.70403, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:16:41.782131 Training: [10 epoch,  70 batch] loss: 5.67615, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:17:04.354120 Training: [10 epoch,  80 batch] loss: 5.69702, the best RMSE/MAE: 7.72629 / 6.46591
2021-01-02 20:17:23.134026 Training: [10 epoch,  90 batch] loss: 5.62326, the best RMSE/MAE: 7.72629 / 6.46591
<Test> RMSE：4.90620,MAE：4.11554
2021-01-02 20:18:23.735586 Training: [11 epoch,  10 batch] loss: 5.63988, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:18:43.884871 Training: [11 epoch,  20 batch] loss: 5.56748, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:19:03.905444 Training: [11 epoch,  30 batch] loss: 5.57832, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:19:28.178982 Training: [11 epoch,  40 batch] loss: 5.51770, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:19:50.010697 Training: [11 epoch,  50 batch] loss: 5.52572, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:20:14.012917 Training: [11 epoch,  60 batch] loss: 5.51029, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:20:37.162690 Training: [11 epoch,  70 batch] loss: 5.44283, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:21:02.032114 Training: [11 epoch,  80 batch] loss: 5.44012, the best RMSE/MAE: 4.90620 / 4.11554
2021-01-02 20:21:25.950249 Training: [11 epoch,  90 batch] loss: 5.43288, the best RMSE/MAE: 4.90620 / 4.11554
<Test> RMSE：3.15764,MAE：2.67374
2021-01-02 20:22:28.831213 Training: [12 epoch,  10 batch] loss: 5.42743, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:22:46.308053 Training: [12 epoch,  20 batch] loss: 5.34491, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:23:11.678126 Training: [12 epoch,  30 batch] loss: 5.31796, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:23:35.954289 Training: [12 epoch,  40 batch] loss: 5.31409, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:23:58.306344 Training: [12 epoch,  50 batch] loss: 5.36140, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:24:22.877466 Training: [12 epoch,  60 batch] loss: 5.28260, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:24:47.865238 Training: [12 epoch,  70 batch] loss: 5.24087, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:25:15.255705 Training: [12 epoch,  80 batch] loss: 5.21052, the best RMSE/MAE: 3.15764 / 2.67374
2021-01-02 20:25:41.195866 Training: [12 epoch,  90 batch] loss: 5.19427, the best RMSE/MAE: 3.15764 / 2.67374
<Test> RMSE：1.99276,MAE：1.68699
2021-01-02 20:26:42.859257 Training: [13 epoch,  10 batch] loss: 5.16523, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:27:02.761129 Training: [13 epoch,  20 batch] loss: 5.16708, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:27:25.540851 Training: [13 epoch,  30 batch] loss: 5.14273, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:27:50.149628 Training: [13 epoch,  40 batch] loss: 5.08267, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:28:07.529570 Training: [13 epoch,  50 batch] loss: 5.06935, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:28:30.763259 Training: [13 epoch,  60 batch] loss: 5.06014, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:28:57.050196 Training: [13 epoch,  70 batch] loss: 5.02326, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:29:22.245207 Training: [13 epoch,  80 batch] loss: 5.00257, the best RMSE/MAE: 1.99276 / 1.68699
2021-01-02 20:29:47.363576 Training: [13 epoch,  90 batch] loss: 4.96812, the best RMSE/MAE: 1.99276 / 1.68699
<Test> RMSE：1.57210,MAE：1.29377
2021-01-02 20:30:51.703226 Training: [14 epoch,  10 batch] loss: 4.93167, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:31:20.543864 Training: [14 epoch,  20 batch] loss: 4.91097, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:31:46.356749 Training: [14 epoch,  30 batch] loss: 4.91222, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:32:12.435009 Training: [14 epoch,  40 batch] loss: 4.87410, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:32:31.332690 Training: [14 epoch,  50 batch] loss: 4.83813, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:32:55.361039 Training: [14 epoch,  60 batch] loss: 4.84278, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:33:18.059806 Training: [14 epoch,  70 batch] loss: 4.78917, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:33:38.626963 Training: [14 epoch,  80 batch] loss: 4.78736, the best RMSE/MAE: 1.57210 / 1.29377
2021-01-02 20:34:04.046102 Training: [14 epoch,  90 batch] loss: 4.78043, the best RMSE/MAE: 1.57210 / 1.29377
<Test> RMSE：1.06176,MAE：0.88727
2021-01-02 20:35:07.193165 Training: [15 epoch,  10 batch] loss: 4.71310, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:35:30.377252 Training: [15 epoch,  20 batch] loss: 4.69697, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:35:50.601346 Training: [15 epoch,  30 batch] loss: 4.66912, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:36:16.586811 Training: [15 epoch,  40 batch] loss: 4.63661, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:36:41.606994 Training: [15 epoch,  50 batch] loss: 4.61620, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:37:08.856510 Training: [15 epoch,  60 batch] loss: 4.63876, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:37:36.033521 Training: [15 epoch,  70 batch] loss: 4.57872, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:37:55.662211 Training: [15 epoch,  80 batch] loss: 4.55605, the best RMSE/MAE: 1.06176 / 0.88727
2021-01-02 20:38:20.264281 Training: [15 epoch,  90 batch] loss: 4.51650, the best RMSE/MAE: 1.06176 / 0.88727
<Test> RMSE：0.91908,MAE：0.72202
2021-01-02 20:39:16.134000 Training: [16 epoch,  10 batch] loss: 4.47512, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:39:40.539347 Training: [16 epoch,  20 batch] loss: 4.51411, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:40:07.150971 Training: [16 epoch,  30 batch] loss: 4.43372, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:40:32.738061 Training: [16 epoch,  40 batch] loss: 4.39134, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:40:56.949777 Training: [16 epoch,  50 batch] loss: 4.38482, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:41:17.018703 Training: [16 epoch,  60 batch] loss: 4.38670, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:41:37.003706 Training: [16 epoch,  70 batch] loss: 4.34401, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:42:00.204147 Training: [16 epoch,  80 batch] loss: 4.33910, the best RMSE/MAE: 0.91908 / 0.72202
2021-01-02 20:42:23.810147 Training: [16 epoch,  90 batch] loss: 4.33275, the best RMSE/MAE: 0.91908 / 0.72202
<Test> RMSE：0.72674,MAE：0.59593
2021-01-02 20:43:39.983644 Training: [17 epoch,  10 batch] loss: 4.25767, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:44:17.796187 Training: [17 epoch,  20 batch] loss: 4.27996, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:44:52.395358 Training: [17 epoch,  30 batch] loss: 4.22809, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:45:35.601649 Training: [17 epoch,  40 batch] loss: 4.18016, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:46:17.905263 Training: [17 epoch,  50 batch] loss: 4.17607, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:46:59.449452 Training: [17 epoch,  60 batch] loss: 4.14678, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:47:41.210785 Training: [17 epoch,  70 batch] loss: 4.12434, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:48:22.227358 Training: [17 epoch,  80 batch] loss: 4.13576, the best RMSE/MAE: 0.72674 / 0.59593
2021-01-02 20:49:03.391674 Training: [17 epoch,  90 batch] loss: 4.06568, the best RMSE/MAE: 0.72674 / 0.59593
<Test> RMSE：0.54748,MAE：0.41829
2021-01-02 20:50:51.617594 Training: [18 epoch,  10 batch] loss: 4.07609, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:51:30.338645 Training: [18 epoch,  20 batch] loss: 3.99824, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:52:10.449256 Training: [18 epoch,  30 batch] loss: 3.97673, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:52:51.420147 Training: [18 epoch,  40 batch] loss: 3.96221, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:53:31.762267 Training: [18 epoch,  50 batch] loss: 3.97912, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:54:13.994096 Training: [18 epoch,  60 batch] loss: 3.94006, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:54:53.130802 Training: [18 epoch,  70 batch] loss: 3.95422, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:55:34.099458 Training: [18 epoch,  80 batch] loss: 3.88613, the best RMSE/MAE: 0.54748 / 0.41829
2021-01-02 20:56:13.931873 Training: [18 epoch,  90 batch] loss: 3.85565, the best RMSE/MAE: 0.54748 / 0.41829
<Test> RMSE：0.53273,MAE：0.40128
2021-01-02 20:58:02.665336 Training: [19 epoch,  10 batch] loss: 3.81322, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 20:58:41.964268 Training: [19 epoch,  20 batch] loss: 3.79827, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 20:59:22.140089 Training: [19 epoch,  30 batch] loss: 3.81119, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:00:04.176391 Training: [19 epoch,  40 batch] loss: 3.75629, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:00:39.195476 Training: [19 epoch,  50 batch] loss: 3.75865, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:01:01.026241 Training: [19 epoch,  60 batch] loss: 3.74490, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:01:24.418238 Training: [19 epoch,  70 batch] loss: 3.66508, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:01:44.332789 Training: [19 epoch,  80 batch] loss: 3.67332, the best RMSE/MAE: 0.53273 / 0.40128
2021-01-02 21:02:06.235780 Training: [19 epoch,  90 batch] loss: 3.66027, the best RMSE/MAE: 0.53273 / 0.40128
<Test> RMSE：0.46414,MAE：0.34004
2021-01-02 21:03:08.837890 Training: [20 epoch,  10 batch] loss: 3.61289, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:03:35.597972 Training: [20 epoch,  20 batch] loss: 3.59050, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:03:58.634140 Training: [20 epoch,  30 batch] loss: 3.59321, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:04:22.250645 Training: [20 epoch,  40 batch] loss: 3.58027, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:04:47.474114 Training: [20 epoch,  50 batch] loss: 3.57197, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:05:11.301673 Training: [20 epoch,  60 batch] loss: 3.49796, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:05:35.881293 Training: [20 epoch,  70 batch] loss: 3.49250, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:05:58.100563 Training: [20 epoch,  80 batch] loss: 3.44372, the best RMSE/MAE: 0.46414 / 0.34004
2021-01-02 21:06:22.326917 Training: [20 epoch,  90 batch] loss: 3.46344, the best RMSE/MAE: 0.46414 / 0.34004
<Test> RMSE：0.43518,MAE：0.28395
2021-01-02 21:07:23.989480 Training: [21 epoch,  10 batch] loss: 3.40882, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:07:48.287675 Training: [21 epoch,  20 batch] loss: 3.40898, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:08:10.620425 Training: [21 epoch,  30 batch] loss: 3.37775, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:08:31.077948 Training: [21 epoch,  40 batch] loss: 3.36446, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:08:58.327213 Training: [21 epoch,  50 batch] loss: 3.33467, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:09:18.765478 Training: [21 epoch,  60 batch] loss: 3.31451, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:09:43.829703 Training: [21 epoch,  70 batch] loss: 3.29689, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:10:05.859742 Training: [21 epoch,  80 batch] loss: 3.27969, the best RMSE/MAE: 0.43518 / 0.28395
2021-01-02 21:10:30.710202 Training: [21 epoch,  90 batch] loss: 3.28888, the best RMSE/MAE: 0.43518 / 0.28395
<Test> RMSE：0.42370,MAE：0.26187
2021-01-02 21:11:33.057169 Training: [22 epoch,  10 batch] loss: 3.22961, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:11:56.867964 Training: [22 epoch,  20 batch] loss: 3.17804, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:12:18.994868 Training: [22 epoch,  30 batch] loss: 3.16637, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:12:43.891529 Training: [22 epoch,  40 batch] loss: 3.20496, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:13:06.896156 Training: [22 epoch,  50 batch] loss: 3.17105, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:13:30.983848 Training: [22 epoch,  60 batch] loss: 3.13311, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:13:50.812059 Training: [22 epoch,  70 batch] loss: 3.10076, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:14:12.209832 Training: [22 epoch,  80 batch] loss: 3.09127, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:14:38.523407 Training: [22 epoch,  90 batch] loss: 3.11111, the best RMSE/MAE: 0.42370 / 0.26187
<Test> RMSE：0.42374,MAE：0.25557
2021-01-02 21:15:41.638732 Training: [23 epoch,  10 batch] loss: 3.03374, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:16:08.763803 Training: [23 epoch,  20 batch] loss: 3.03738, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:16:32.944147 Training: [23 epoch,  30 batch] loss: 3.02082, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:16:56.570433 Training: [23 epoch,  40 batch] loss: 2.98398, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:17:21.872257 Training: [23 epoch,  50 batch] loss: 2.98066, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:17:46.829910 Training: [23 epoch,  60 batch] loss: 2.92749, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:18:14.271612 Training: [23 epoch,  70 batch] loss: 2.95595, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:18:38.061389 Training: [23 epoch,  80 batch] loss: 2.93181, the best RMSE/MAE: 0.42370 / 0.26187
2021-01-02 21:19:02.450820 Training: [23 epoch,  90 batch] loss: 2.89195, the best RMSE/MAE: 0.42370 / 0.26187
<Test> RMSE：0.39656,MAE：0.19514
2021-01-02 21:19:59.820205 Training: [24 epoch,  10 batch] loss: 2.86514, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:20:23.113855 Training: [24 epoch,  20 batch] loss: 2.89353, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:20:44.867087 Training: [24 epoch,  30 batch] loss: 2.82331, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:21:10.755539 Training: [24 epoch,  40 batch] loss: 2.79817, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:21:34.808026 Training: [24 epoch,  50 batch] loss: 2.80280, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:21:59.760425 Training: [24 epoch,  60 batch] loss: 2.79797, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:22:24.900884 Training: [24 epoch,  70 batch] loss: 2.76884, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:22:50.211202 Training: [24 epoch,  80 batch] loss: 2.73484, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:23:13.417052 Training: [24 epoch,  90 batch] loss: 2.73239, the best RMSE/MAE: 0.39656 / 0.19514
<Test> RMSE：0.39877,MAE：0.21025
2021-01-02 21:24:14.778225 Training: [25 epoch,  10 batch] loss: 2.69180, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:24:38.057408 Training: [25 epoch,  20 batch] loss: 2.71424, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:24:56.723912 Training: [25 epoch,  30 batch] loss: 2.65731, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:25:16.602048 Training: [25 epoch,  40 batch] loss: 2.67284, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:25:39.924782 Training: [25 epoch,  50 batch] loss: 2.63483, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:26:05.886217 Training: [25 epoch,  60 batch] loss: 2.60761, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:26:30.191811 Training: [25 epoch,  70 batch] loss: 2.61001, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:26:51.982221 Training: [25 epoch,  80 batch] loss: 2.61287, the best RMSE/MAE: 0.39656 / 0.19514
2021-01-02 21:27:16.145114 Training: [25 epoch,  90 batch] loss: 2.56652, the best RMSE/MAE: 0.39656 / 0.19514
<Test> RMSE：0.38908,MAE：0.18714
2021-01-02 21:28:13.788299 Training: [26 epoch,  10 batch] loss: 2.52787, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:28:36.564520 Training: [26 epoch,  20 batch] loss: 2.53188, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:29:03.171478 Training: [26 epoch,  30 batch] loss: 2.51050, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:29:25.874788 Training: [26 epoch,  40 batch] loss: 2.50170, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:29:49.888504 Training: [26 epoch,  50 batch] loss: 2.51666, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:30:13.081130 Training: [26 epoch,  60 batch] loss: 2.45829, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:30:33.231529 Training: [26 epoch,  70 batch] loss: 2.46396, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:30:57.199017 Training: [26 epoch,  80 batch] loss: 2.43981, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:31:18.540006 Training: [26 epoch,  90 batch] loss: 2.41394, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39304,MAE：0.17741
2021-01-02 21:32:23.592509 Training: [27 epoch,  10 batch] loss: 2.39119, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:32:51.114596 Training: [27 epoch,  20 batch] loss: 2.37568, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:33:13.815204 Training: [27 epoch,  30 batch] loss: 2.38503, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:33:34.082824 Training: [27 epoch,  40 batch] loss: 2.34609, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:33:57.790432 Training: [27 epoch,  50 batch] loss: 2.35470, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:34:20.809053 Training: [27 epoch,  60 batch] loss: 2.35693, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:34:42.888385 Training: [27 epoch,  70 batch] loss: 2.31982, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:35:06.826716 Training: [27 epoch,  80 batch] loss: 2.31763, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:35:32.718646 Training: [27 epoch,  90 batch] loss: 2.30065, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39141,MAE：0.17550
2021-01-02 21:36:43.261715 Training: [28 epoch,  10 batch] loss: 2.26474, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:37:06.643313 Training: [28 epoch,  20 batch] loss: 2.29049, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:37:27.200795 Training: [28 epoch,  30 batch] loss: 2.26172, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:37:50.605660 Training: [28 epoch,  40 batch] loss: 2.22865, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:38:12.679219 Training: [28 epoch,  50 batch] loss: 2.19621, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:38:34.833979 Training: [28 epoch,  60 batch] loss: 2.19979, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:38:55.700858 Training: [28 epoch,  70 batch] loss: 2.19826, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:39:18.331113 Training: [28 epoch,  80 batch] loss: 2.16626, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:39:46.114213 Training: [28 epoch,  90 batch] loss: 2.14133, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39308,MAE：0.18717
2021-01-02 21:40:49.644226 Training: [29 epoch,  10 batch] loss: 2.15541, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:41:12.436186 Training: [29 epoch,  20 batch] loss: 2.13250, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:41:31.406941 Training: [29 epoch,  30 batch] loss: 2.15232, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:41:54.757714 Training: [29 epoch,  40 batch] loss: 2.10085, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:42:18.510255 Training: [29 epoch,  50 batch] loss: 2.07362, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:42:45.254036 Training: [29 epoch,  60 batch] loss: 2.04435, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:43:10.210231 Training: [29 epoch,  70 batch] loss: 2.08751, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:43:33.803955 Training: [29 epoch,  80 batch] loss: 2.03920, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:43:58.920540 Training: [29 epoch,  90 batch] loss: 2.04802, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39165,MAE：0.16990
2021-01-02 21:45:02.579032 Training: [30 epoch,  10 batch] loss: 2.02976, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:45:24.314296 Training: [30 epoch,  20 batch] loss: 2.01705, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:45:47.727057 Training: [30 epoch,  30 batch] loss: 1.98303, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:46:11.874833 Training: [30 epoch,  40 batch] loss: 1.99020, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:46:37.033919 Training: [30 epoch,  50 batch] loss: 1.94913, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:46:55.768672 Training: [30 epoch,  60 batch] loss: 1.97805, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:47:17.734493 Training: [30 epoch,  70 batch] loss: 1.94647, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:47:42.775471 Training: [30 epoch,  80 batch] loss: 1.93580, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:48:05.625431 Training: [30 epoch,  90 batch] loss: 1.92945, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39241,MAE：0.18054
2021-01-02 21:49:09.861797 Training: [31 epoch,  10 batch] loss: 1.88434, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:49:32.423958 Training: [31 epoch,  20 batch] loss: 1.89783, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:49:56.702669 Training: [31 epoch,  30 batch] loss: 1.87803, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:50:20.408602 Training: [31 epoch,  40 batch] loss: 1.92092, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:50:45.179850 Training: [31 epoch,  50 batch] loss: 1.87999, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:51:07.540381 Training: [31 epoch,  60 batch] loss: 1.85988, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:51:36.753650 Training: [31 epoch,  70 batch] loss: 1.83542, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:52:11.073821 Training: [31 epoch,  80 batch] loss: 1.82983, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:52:42.535728 Training: [31 epoch,  90 batch] loss: 1.81181, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39963,MAE：0.22105
2021-01-02 21:54:10.873259 Training: [32 epoch,  10 batch] loss: 1.79190, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:54:41.267264 Training: [32 epoch,  20 batch] loss: 1.80543, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:55:13.121502 Training: [32 epoch,  30 batch] loss: 1.77351, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:55:44.996298 Training: [32 epoch,  40 batch] loss: 1.75768, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:56:15.947932 Training: [32 epoch,  50 batch] loss: 1.78218, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:56:49.547530 Training: [32 epoch,  60 batch] loss: 1.73974, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:57:21.281696 Training: [32 epoch,  70 batch] loss: 1.75048, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:57:52.112477 Training: [32 epoch,  80 batch] loss: 1.73918, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 21:58:22.880430 Training: [32 epoch,  90 batch] loss: 1.72999, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39611,MAE：0.20437
2021-01-02 21:59:50.969951 Training: [33 epoch,  10 batch] loss: 1.68522, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:00:25.359566 Training: [33 epoch,  20 batch] loss: 1.68489, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:00:59.921689 Training: [33 epoch,  30 batch] loss: 1.69621, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:01:31.114034 Training: [33 epoch,  40 batch] loss: 1.68245, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:02:01.834407 Training: [33 epoch,  50 batch] loss: 1.71398, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:02:32.763432 Training: [33 epoch,  60 batch] loss: 1.63551, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:03:05.001102 Training: [33 epoch,  70 batch] loss: 1.66440, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:03:36.141167 Training: [33 epoch,  80 batch] loss: 1.61032, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:04:06.486258 Training: [33 epoch,  90 batch] loss: 1.63230, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.39453,MAE：0.20502
2021-01-02 22:05:09.973171 Training: [34 epoch,  10 batch] loss: 1.60555, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:05:33.752483 Training: [34 epoch,  20 batch] loss: 1.63506, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:05:56.335603 Training: [34 epoch,  30 batch] loss: 1.57706, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:06:19.494817 Training: [34 epoch,  40 batch] loss: 1.61865, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:06:42.062422 Training: [34 epoch,  50 batch] loss: 1.60653, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:07:05.479518 Training: [34 epoch,  60 batch] loss: 1.55457, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:07:27.114789 Training: [34 epoch,  70 batch] loss: 1.55002, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:07:52.282110 Training: [34 epoch,  80 batch] loss: 1.55345, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:08:15.615295 Training: [34 epoch,  90 batch] loss: 1.52657, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.43700,MAE：0.31545
2021-01-02 22:09:15.122904 Training: [35 epoch,  10 batch] loss: 1.52047, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:09:41.085733 Training: [35 epoch,  20 batch] loss: 1.53983, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:10:05.070656 Training: [35 epoch,  30 batch] loss: 1.51541, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:10:30.053844 Training: [35 epoch,  40 batch] loss: 1.50348, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:10:52.594796 Training: [35 epoch,  50 batch] loss: 1.47920, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:11:17.935913 Training: [35 epoch,  60 batch] loss: 1.49173, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:11:42.529032 Training: [35 epoch,  70 batch] loss: 1.47288, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:12:08.795292 Training: [35 epoch,  80 batch] loss: 1.47571, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:12:32.764629 Training: [35 epoch,  90 batch] loss: 1.47507, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.40495,MAE：0.24557
2021-01-02 22:13:39.342849 Training: [36 epoch,  10 batch] loss: 1.43825, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:14:00.853343 Training: [36 epoch,  20 batch] loss: 1.47179, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:14:23.212897 Training: [36 epoch,  30 batch] loss: 1.46951, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:14:45.778596 Training: [36 epoch,  40 batch] loss: 1.40569, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:15:12.337131 Training: [36 epoch,  50 batch] loss: 1.42864, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:15:34.123768 Training: [36 epoch,  60 batch] loss: 1.39793, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:16:01.174136 Training: [36 epoch,  70 batch] loss: 1.39794, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:16:24.516799 Training: [36 epoch,  80 batch] loss: 1.39628, the best RMSE/MAE: 0.38908 / 0.18714
2021-01-02 22:16:52.524970 Training: [36 epoch,  90 batch] loss: 1.38181, the best RMSE/MAE: 0.38908 / 0.18714
<Test> RMSE：0.41302,MAE：0.26799
The best RMSE/MAE：0.38908/0.18714
