-------------------- Hyperparams --------------------
time: 2021-01-02 22:08:35.226024
Dataset: yelp
N: 30000
weight decay: 0.0005
dropout rate: 0.5
learning rate: 0.001
dimension of embedding: 32
use_cuda: True
2021-01-02 22:23:24.114560 Training: [1 epoch,  10 batch] loss: 15.32890, the best RMSE/MAE: inf / inf
2021-01-02 22:24:29.043215 Training: [1 epoch,  20 batch] loss: 14.95000, the best RMSE/MAE: inf / inf
2021-01-02 22:25:34.177671 Training: [1 epoch,  30 batch] loss: 14.67014, the best RMSE/MAE: inf / inf
2021-01-02 22:26:34.478951 Training: [1 epoch,  40 batch] loss: 14.45473, the best RMSE/MAE: inf / inf
2021-01-02 22:27:39.337192 Training: [1 epoch,  50 batch] loss: 14.30674, the best RMSE/MAE: inf / inf
2021-01-02 22:28:43.544572 Training: [1 epoch,  60 batch] loss: 14.21137, the best RMSE/MAE: inf / inf
2021-01-02 22:29:47.321029 Training: [1 epoch,  70 batch] loss: 14.16593, the best RMSE/MAE: inf / inf
2021-01-02 22:30:52.479102 Training: [1 epoch,  80 batch] loss: 14.06642, the best RMSE/MAE: inf / inf
2021-01-02 22:31:52.029101 Training: [1 epoch,  90 batch] loss: 14.01830, the best RMSE/MAE: inf / inf
<Test> RMSE：195264528.00000,MAE：152585728.00000
2021-01-02 22:34:45.025404 Training: [2 epoch,  10 batch] loss: 13.95029, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:35:48.680800 Training: [2 epoch,  20 batch] loss: 13.95392, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:36:50.453333 Training: [2 epoch,  30 batch] loss: 13.92033, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:37:49.607791 Training: [2 epoch,  40 batch] loss: 13.86074, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:38:53.523675 Training: [2 epoch,  50 batch] loss: 13.87335, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:39:58.458778 Training: [2 epoch,  60 batch] loss: 13.82266, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:41:03.901371 Training: [2 epoch,  70 batch] loss: 13.79595, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:42:08.270729 Training: [2 epoch,  80 batch] loss: 13.73237, the best RMSE/MAE: 195264528.00000 / 152585728.00000
2021-01-02 22:43:09.001743 Training: [2 epoch,  90 batch] loss: 13.67098, the best RMSE/MAE: 195264528.00000 / 152585728.00000
<Test> RMSE：333575.81250,MAE：261339.78125
2021-01-02 22:46:06.274056 Training: [3 epoch,  10 batch] loss: 13.68781, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:47:09.149142 Training: [3 epoch,  20 batch] loss: 13.62367, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:48:08.331538 Training: [3 epoch,  30 batch] loss: 13.61772, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:49:10.743471 Training: [3 epoch,  40 batch] loss: 13.55979, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:50:15.962597 Training: [3 epoch,  50 batch] loss: 13.58486, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:51:20.352957 Training: [3 epoch,  60 batch] loss: 13.48264, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:52:24.399866 Training: [3 epoch,  70 batch] loss: 13.49956, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:53:26.996387 Training: [3 epoch,  80 batch] loss: 13.45071, the best RMSE/MAE: 333575.81250 / 261339.78125
2021-01-02 22:54:29.836753 Training: [3 epoch,  90 batch] loss: 13.44298, the best RMSE/MAE: 333575.81250 / 261339.78125
<Test> RMSE：8891.58203,MAE：6756.75244
2021-01-02 22:57:26.364381 Training: [4 epoch,  10 batch] loss: 13.35861, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 22:58:26.261852 Training: [4 epoch,  20 batch] loss: 13.31487, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 22:59:26.195505 Training: [4 epoch,  30 batch] loss: 13.31127, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:00:32.925811 Training: [4 epoch,  40 batch] loss: 13.32060, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:01:36.145535 Training: [4 epoch,  50 batch] loss: 13.28129, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:02:41.627030 Training: [4 epoch,  60 batch] loss: 13.22101, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:03:46.795292 Training: [4 epoch,  70 batch] loss: 13.20323, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:04:46.651339 Training: [4 epoch,  80 batch] loss: 13.15523, the best RMSE/MAE: 8891.58203 / 6756.75244
2021-01-02 23:05:50.604166 Training: [4 epoch,  90 batch] loss: 13.10793, the best RMSE/MAE: 8891.58203 / 6756.75244
<Test> RMSE：657.20416,MAE：495.08618
2021-01-02 23:08:52.824282 Training: [5 epoch,  10 batch] loss: 13.10364, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:09:54.176635 Training: [5 epoch,  20 batch] loss: 13.03673, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:10:57.689458 Training: [5 epoch,  30 batch] loss: 12.97255, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:12:01.112849 Training: [5 epoch,  40 batch] loss: 12.96673, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:13:06.611673 Training: [5 epoch,  50 batch] loss: 12.92821, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:14:10.694031 Training: [5 epoch,  60 batch] loss: 12.95445, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:15:12.340187 Training: [5 epoch,  70 batch] loss: 12.84645, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:16:12.452631 Training: [5 epoch,  80 batch] loss: 12.81547, the best RMSE/MAE: 657.20416 / 495.08618
2021-01-02 23:17:16.726090 Training: [5 epoch,  90 batch] loss: 12.82752, the best RMSE/MAE: 657.20416 / 495.08618
<Test> RMSE：136.21587,MAE：99.65807
2021-01-02 23:20:10.517190 Training: [6 epoch,  10 batch] loss: 12.74008, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:21:12.199645 Training: [6 epoch,  20 batch] loss: 12.69978, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:22:18.944260 Training: [6 epoch,  30 batch] loss: 12.71142, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:23:23.304523 Training: [6 epoch,  40 batch] loss: 12.63922, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:24:27.128262 Training: [6 epoch,  50 batch] loss: 12.60816, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:25:30.609133 Training: [6 epoch,  60 batch] loss: 12.56043, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:26:32.346785 Training: [6 epoch,  70 batch] loss: 12.51552, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:27:36.855672 Training: [6 epoch,  80 batch] loss: 12.54440, the best RMSE/MAE: 136.21587 / 99.65807
2021-01-02 23:28:40.851328 Training: [6 epoch,  90 batch] loss: 12.46856, the best RMSE/MAE: 136.21587 / 99.65807
<Test> RMSE：39.73857,MAE：28.93191
2021-01-02 23:31:37.526883 Training: [7 epoch,  10 batch] loss: 12.43277, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:32:37.437977 Training: [7 epoch,  20 batch] loss: 12.35666, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:33:40.775176 Training: [7 epoch,  30 batch] loss: 12.32388, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:34:44.227741 Training: [7 epoch,  40 batch] loss: 12.31341, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:35:47.408422 Training: [7 epoch,  50 batch] loss: 12.24404, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:36:50.313668 Training: [7 epoch,  60 batch] loss: 12.22023, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:37:52.389335 Training: [7 epoch,  70 batch] loss: 12.20752, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:38:58.168868 Training: [7 epoch,  80 batch] loss: 12.13768, the best RMSE/MAE: 39.73857 / 28.93191
2021-01-02 23:40:03.045023 Training: [7 epoch,  90 batch] loss: 12.07220, the best RMSE/MAE: 39.73857 / 28.93191
<Test> RMSE：18.97226,MAE：13.83567
2021-01-02 23:42:52.127152 Training: [8 epoch,  10 batch] loss: 12.07843, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:43:54.757161 Training: [8 epoch,  20 batch] loss: 11.97665, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:44:55.465992 Training: [8 epoch,  30 batch] loss: 11.94340, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:45:59.212467 Training: [8 epoch,  40 batch] loss: 11.91894, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:47:03.476044 Training: [8 epoch,  50 batch] loss: 11.86055, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:48:05.593545 Training: [8 epoch,  60 batch] loss: 11.84506, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:49:07.105065 Training: [8 epoch,  70 batch] loss: 11.76437, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:50:08.697496 Training: [8 epoch,  80 batch] loss: 11.78460, the best RMSE/MAE: 18.97226 / 13.83567
2021-01-02 23:51:14.508409 Training: [8 epoch,  90 batch] loss: 11.73296, the best RMSE/MAE: 18.97226 / 13.83567
<Test> RMSE：7.95021,MAE：6.17295
2021-01-02 23:54:05.255078 Training: [9 epoch,  10 batch] loss: 11.64678, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-02 23:55:10.217517 Training: [9 epoch,  20 batch] loss: 11.58707, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-02 23:56:14.668098 Training: [9 epoch,  30 batch] loss: 11.60378, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-02 23:57:20.133395 Training: [9 epoch,  40 batch] loss: 11.51571, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-02 23:58:23.268289 Training: [9 epoch,  50 batch] loss: 11.45747, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-02 23:59:25.475777 Training: [9 epoch,  60 batch] loss: 11.42624, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-03 00:00:33.770560 Training: [9 epoch,  70 batch] loss: 11.37725, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-03 00:01:39.985791 Training: [9 epoch,  80 batch] loss: 11.34088, the best RMSE/MAE: 7.95021 / 6.17295
2021-01-03 00:02:41.427931 Training: [9 epoch,  90 batch] loss: 11.32476, the best RMSE/MAE: 7.95021 / 6.17295
<Test> RMSE：4.08179,MAE：3.18086
2021-01-03 00:05:27.274576 Training: [10 epoch,  10 batch] loss: 11.21186, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:06:15.755895 Training: [10 epoch,  20 batch] loss: 11.19924, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:07:03.712187 Training: [10 epoch,  30 batch] loss: 11.14603, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:07:52.187451 Training: [10 epoch,  40 batch] loss: 11.09037, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:08:39.400535 Training: [10 epoch,  50 batch] loss: 11.06121, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:09:27.305038 Training: [10 epoch,  60 batch] loss: 11.01884, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:10:15.945886 Training: [10 epoch,  70 batch] loss: 10.95534, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:11:04.648315 Training: [10 epoch,  80 batch] loss: 10.92794, the best RMSE/MAE: 4.08179 / 3.18086
2021-01-03 00:11:53.504990 Training: [10 epoch,  90 batch] loss: 10.89780, the best RMSE/MAE: 4.08179 / 3.18086
<Test> RMSE：2.42317,MAE：1.86542
2021-01-03 00:14:12.012447 Training: [11 epoch,  10 batch] loss: 10.80182, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:14:59.971185 Training: [11 epoch,  20 batch] loss: 10.72927, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:15:48.320464 Training: [11 epoch,  30 batch] loss: 10.68691, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:16:35.803449 Training: [11 epoch,  40 batch] loss: 10.67189, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:17:23.676525 Training: [11 epoch,  50 batch] loss: 10.61544, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:18:12.424168 Training: [11 epoch,  60 batch] loss: 10.56901, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:19:01.147939 Training: [11 epoch,  70 batch] loss: 10.55275, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:19:49.872518 Training: [11 epoch,  80 batch] loss: 10.49343, the best RMSE/MAE: 2.42317 / 1.86542
2021-01-03 00:20:38.607623 Training: [11 epoch,  90 batch] loss: 10.45022, the best RMSE/MAE: 2.42317 / 1.86542
<Test> RMSE：1.60344,MAE：1.24584
2021-01-03 00:22:57.455038 Training: [12 epoch,  10 batch] loss: 10.37766, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:23:45.235957 Training: [12 epoch,  20 batch] loss: 10.29637, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:24:33.228368 Training: [12 epoch,  30 batch] loss: 10.29778, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:25:20.398504 Training: [12 epoch,  40 batch] loss: 10.21211, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:26:07.632684 Training: [12 epoch,  50 batch] loss: 10.19043, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:26:55.633119 Training: [12 epoch,  60 batch] loss: 10.11749, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:27:42.833436 Training: [12 epoch,  70 batch] loss: 10.10201, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:28:30.940249 Training: [12 epoch,  80 batch] loss: 10.03549, the best RMSE/MAE: 1.60344 / 1.24584
2021-01-03 00:29:18.201150 Training: [12 epoch,  90 batch] loss: 9.96255, the best RMSE/MAE: 1.60344 / 1.24584
<Test> RMSE：1.08959,MAE：0.83430
2021-01-03 00:31:35.682664 Training: [13 epoch,  10 batch] loss: 9.88434, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:32:22.472750 Training: [13 epoch,  20 batch] loss: 9.84122, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:33:08.854896 Training: [13 epoch,  30 batch] loss: 9.82220, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:33:55.655147 Training: [13 epoch,  40 batch] loss: 9.74952, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:34:43.503292 Training: [13 epoch,  50 batch] loss: 9.71924, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:35:31.161250 Training: [13 epoch,  60 batch] loss: 9.69939, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:36:19.515808 Training: [13 epoch,  70 batch] loss: 9.63735, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:37:07.274226 Training: [13 epoch,  80 batch] loss: 9.60875, the best RMSE/MAE: 1.08959 / 0.83430
2021-01-03 00:37:55.550461 Training: [13 epoch,  90 batch] loss: 9.53474, the best RMSE/MAE: 1.08959 / 0.83430
<Test> RMSE：0.79338,MAE：0.61046
2021-01-03 00:40:12.677137 Training: [14 epoch,  10 batch] loss: 9.44332, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:40:59.872711 Training: [14 epoch,  20 batch] loss: 9.38241, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:41:46.415807 Training: [14 epoch,  30 batch] loss: 9.33829, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:42:33.559510 Training: [14 epoch,  40 batch] loss: 9.31647, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:43:21.714905 Training: [14 epoch,  50 batch] loss: 9.27788, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:44:09.681866 Training: [14 epoch,  60 batch] loss: 9.25314, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:44:57.347689 Training: [14 epoch,  70 batch] loss: 9.15162, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:45:45.628674 Training: [14 epoch,  80 batch] loss: 9.15177, the best RMSE/MAE: 0.79338 / 0.61046
2021-01-03 00:46:33.407967 Training: [14 epoch,  90 batch] loss: 9.06087, the best RMSE/MAE: 0.79338 / 0.61046
<Test> RMSE：0.58802,MAE：0.43532
2021-01-03 00:48:50.492091 Training: [15 epoch,  10 batch] loss: 8.97705, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:49:36.772832 Training: [15 epoch,  20 batch] loss: 8.96417, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:50:23.517693 Training: [15 epoch,  30 batch] loss: 8.89536, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:51:10.968988 Training: [15 epoch,  40 batch] loss: 8.84857, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:51:59.256316 Training: [15 epoch,  50 batch] loss: 8.77604, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:52:46.952460 Training: [15 epoch,  60 batch] loss: 8.74932, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:53:35.492354 Training: [15 epoch,  70 batch] loss: 8.69011, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:54:23.125074 Training: [15 epoch,  80 batch] loss: 8.69198, the best RMSE/MAE: 0.58802 / 0.43532
2021-01-03 00:55:11.684720 Training: [15 epoch,  90 batch] loss: 8.61533, the best RMSE/MAE: 0.58802 / 0.43532
<Test> RMSE：0.53603,MAE：0.36466
2021-01-03 00:57:28.090021 Training: [16 epoch,  10 batch] loss: 8.53625, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 00:58:14.322611 Training: [16 epoch,  20 batch] loss: 8.50134, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 00:59:01.398665 Training: [16 epoch,  30 batch] loss: 8.44465, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 00:59:48.641655 Training: [16 epoch,  40 batch] loss: 8.40026, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 01:00:36.745454 Training: [16 epoch,  50 batch] loss: 8.35254, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 01:01:24.215708 Training: [16 epoch,  60 batch] loss: 8.32430, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 01:02:12.467143 Training: [16 epoch,  70 batch] loss: 8.23868, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 01:02:59.892731 Training: [16 epoch,  80 batch] loss: 8.19883, the best RMSE/MAE: 0.53603 / 0.36466
2021-01-03 01:03:48.163457 Training: [16 epoch,  90 batch] loss: 8.16650, the best RMSE/MAE: 0.53603 / 0.36466
<Test> RMSE：0.46543,MAE：0.29915
2021-01-03 01:06:03.821162 Training: [17 epoch,  10 batch] loss: 8.08634, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:06:49.455452 Training: [17 epoch,  20 batch] loss: 8.04280, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:07:36.571181 Training: [17 epoch,  30 batch] loss: 7.97230, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:08:23.721446 Training: [17 epoch,  40 batch] loss: 7.99526, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:09:10.408010 Training: [17 epoch,  50 batch] loss: 7.90463, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:09:56.861059 Training: [17 epoch,  60 batch] loss: 7.83416, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:10:43.531874 Training: [17 epoch,  70 batch] loss: 7.80966, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:11:30.694629 Training: [17 epoch,  80 batch] loss: 7.79452, the best RMSE/MAE: 0.46543 / 0.29915
2021-01-03 01:12:18.350342 Training: [17 epoch,  90 batch] loss: 7.70894, the best RMSE/MAE: 0.46543 / 0.29915
<Test> RMSE：0.40524,MAE：0.20678
2021-01-03 01:14:33.536174 Training: [18 epoch,  10 batch] loss: 7.63288, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:15:20.074717 Training: [18 epoch,  20 batch] loss: 7.60856, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:16:08.559313 Training: [18 epoch,  30 batch] loss: 7.53578, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:16:56.694175 Training: [18 epoch,  40 batch] loss: 7.49986, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:17:45.176107 Training: [18 epoch,  50 batch] loss: 7.51612, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:18:33.657025 Training: [18 epoch,  60 batch] loss: 7.42427, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:19:21.948076 Training: [18 epoch,  70 batch] loss: 7.37588, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:20:10.607630 Training: [18 epoch,  80 batch] loss: 7.32354, the best RMSE/MAE: 0.40524 / 0.20678
2021-01-03 01:20:58.639430 Training: [18 epoch,  90 batch] loss: 7.28560, the best RMSE/MAE: 0.40524 / 0.20678
<Test> RMSE：0.40015,MAE：0.20417
2021-01-03 01:23:13.175468 Training: [19 epoch,  10 batch] loss: 7.23862, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:24:01.031650 Training: [19 epoch,  20 batch] loss: 7.14970, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:24:49.603322 Training: [19 epoch,  30 batch] loss: 7.13110, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:25:37.842071 Training: [19 epoch,  40 batch] loss: 7.08794, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:26:26.841883 Training: [19 epoch,  50 batch] loss: 7.06517, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:27:15.260923 Training: [19 epoch,  60 batch] loss: 6.99870, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:28:03.676364 Training: [19 epoch,  70 batch] loss: 6.95703, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:28:52.664033 Training: [19 epoch,  80 batch] loss: 6.88769, the best RMSE/MAE: 0.40015 / 0.20417
2021-01-03 01:29:41.159877 Training: [19 epoch,  90 batch] loss: 6.90335, the best RMSE/MAE: 0.40015 / 0.20417
<Test> RMSE：0.39772,MAE：0.18848
2021-01-03 01:31:56.330133 Training: [20 epoch,  10 batch] loss: 6.78971, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:32:44.256624 Training: [20 epoch,  20 batch] loss: 6.75511, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:33:32.752155 Training: [20 epoch,  30 batch] loss: 6.74532, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:34:21.291515 Training: [20 epoch,  40 batch] loss: 6.67458, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:35:09.341249 Training: [20 epoch,  50 batch] loss: 6.65786, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:35:57.891274 Training: [20 epoch,  60 batch] loss: 6.58407, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:36:46.585442 Training: [20 epoch,  70 batch] loss: 6.55676, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:37:34.979868 Training: [20 epoch,  80 batch] loss: 6.51901, the best RMSE/MAE: 0.39772 / 0.18848
2021-01-03 01:38:23.329411 Training: [20 epoch,  90 batch] loss: 6.47462, the best RMSE/MAE: 0.39772 / 0.18848
<Test> RMSE：0.39188,MAE：0.17044
2021-01-03 01:40:38.203596 Training: [21 epoch,  10 batch] loss: 6.44926, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:41:26.441440 Training: [21 epoch,  20 batch] loss: 6.36557, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:42:14.437543 Training: [21 epoch,  30 batch] loss: 6.33229, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:43:02.548264 Training: [21 epoch,  40 batch] loss: 6.27878, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:43:51.027158 Training: [21 epoch,  50 batch] loss: 6.23380, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:44:39.231828 Training: [21 epoch,  60 batch] loss: 6.21738, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:45:27.559792 Training: [21 epoch,  70 batch] loss: 6.15562, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:46:16.308329 Training: [21 epoch,  80 batch] loss: 6.17401, the best RMSE/MAE: 0.39188 / 0.17044
2021-01-03 01:47:04.551286 Training: [21 epoch,  90 batch] loss: 6.12665, the best RMSE/MAE: 0.39188 / 0.17044
<Test> RMSE：0.39025,MAE：0.15987
2021-01-03 01:49:20.408726 Training: [22 epoch,  10 batch] loss: 6.01863, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:50:08.663624 Training: [22 epoch,  20 batch] loss: 6.02955, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:50:56.796209 Training: [22 epoch,  30 batch] loss: 5.95086, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:51:45.419581 Training: [22 epoch,  40 batch] loss: 5.92741, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:52:33.963456 Training: [22 epoch,  50 batch] loss: 5.91347, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:53:22.351341 Training: [22 epoch,  60 batch] loss: 5.87813, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:54:11.138427 Training: [22 epoch,  70 batch] loss: 5.79944, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:54:59.985092 Training: [22 epoch,  80 batch] loss: 5.75132, the best RMSE/MAE: 0.39025 / 0.15987
2021-01-03 01:55:47.667217 Training: [22 epoch,  90 batch] loss: 5.76415, the best RMSE/MAE: 0.39025 / 0.15987
<Test> RMSE：0.39005,MAE：0.13884
2021-01-03 01:58:05.456071 Training: [23 epoch,  10 batch] loss: 5.67865, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 01:58:53.804383 Training: [23 epoch,  20 batch] loss: 5.62683, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 01:59:41.795807 Training: [23 epoch,  30 batch] loss: 5.58757, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:00:30.452781 Training: [23 epoch,  40 batch] loss: 5.57076, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:01:19.024308 Training: [23 epoch,  50 batch] loss: 5.56486, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:02:07.375865 Training: [23 epoch,  60 batch] loss: 5.49739, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:02:56.244219 Training: [23 epoch,  70 batch] loss: 5.49686, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:03:44.523244 Training: [23 epoch,  80 batch] loss: 5.41844, the best RMSE/MAE: 0.39005 / 0.13884
2021-01-03 02:04:32.135218 Training: [23 epoch,  90 batch] loss: 5.41498, the best RMSE/MAE: 0.39005 / 0.13884
<Test> RMSE：0.38934,MAE：0.13204
2021-01-03 02:06:50.642032 Training: [24 epoch,  10 batch] loss: 5.34301, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:07:39.068538 Training: [24 epoch,  20 batch] loss: 5.29228, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:08:26.823123 Training: [24 epoch,  30 batch] loss: 5.28380, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:09:15.344322 Training: [24 epoch,  40 batch] loss: 5.24496, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:10:03.541060 Training: [24 epoch,  50 batch] loss: 5.23625, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:10:51.733076 Training: [24 epoch,  60 batch] loss: 5.19870, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:11:40.476627 Training: [24 epoch,  70 batch] loss: 5.14729, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:12:28.311432 Training: [24 epoch,  80 batch] loss: 5.10429, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:13:16.553321 Training: [24 epoch,  90 batch] loss: 5.07375, the best RMSE/MAE: 0.38934 / 0.13204
<Test> RMSE：0.39651,MAE：0.14096
2021-01-03 02:15:35.294905 Training: [25 epoch,  10 batch] loss: 5.03826, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:16:23.147131 Training: [25 epoch,  20 batch] loss: 4.98894, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:17:11.391730 Training: [25 epoch,  30 batch] loss: 4.98518, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:17:59.939765 Training: [25 epoch,  40 batch] loss: 4.91665, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:18:48.007172 Training: [25 epoch,  50 batch] loss: 4.89114, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:19:36.544385 Training: [25 epoch,  60 batch] loss: 4.85311, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:20:24.116631 Training: [25 epoch,  70 batch] loss: 4.81851, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:21:11.753226 Training: [25 epoch,  80 batch] loss: 4.86940, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:22:00.465585 Training: [25 epoch,  90 batch] loss: 4.78500, the best RMSE/MAE: 0.38934 / 0.13204
<Test> RMSE：0.39109,MAE：0.12703
2021-01-03 02:24:19.639239 Training: [26 epoch,  10 batch] loss: 4.71737, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:25:07.387354 Training: [26 epoch,  20 batch] loss: 4.69086, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:25:55.631057 Training: [26 epoch,  30 batch] loss: 4.66819, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:26:44.083724 Training: [26 epoch,  40 batch] loss: 4.65259, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:27:32.148305 Training: [26 epoch,  50 batch] loss: 4.63035, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:28:20.327461 Training: [26 epoch,  60 batch] loss: 4.58702, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:29:07.222513 Training: [26 epoch,  70 batch] loss: 4.53194, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:29:55.763050 Training: [26 epoch,  80 batch] loss: 4.55435, the best RMSE/MAE: 0.38934 / 0.13204
2021-01-03 02:30:44.695793 Training: [26 epoch,  90 batch] loss: 4.48693, the best RMSE/MAE: 0.38934 / 0.13204
<Test> RMSE：0.38919,MAE：0.13019
2021-01-03 02:33:02.763532 Training: [27 epoch,  10 batch] loss: 4.45058, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:33:51.084119 Training: [27 epoch,  20 batch] loss: 4.44801, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:34:39.018999 Training: [27 epoch,  30 batch] loss: 4.40982, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:35:27.258266 Training: [27 epoch,  40 batch] loss: 4.34580, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:36:15.859907 Training: [27 epoch,  50 batch] loss: 4.34864, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:37:03.578814 Training: [27 epoch,  60 batch] loss: 4.31420, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:37:52.006245 Training: [27 epoch,  70 batch] loss: 4.32152, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:38:40.673668 Training: [27 epoch,  80 batch] loss: 4.26069, the best RMSE/MAE: 0.38919 / 0.13019
2021-01-03 02:39:29.323717 Training: [27 epoch,  90 batch] loss: 4.22232, the best RMSE/MAE: 0.38919 / 0.13019
<Test> RMSE：0.38711,MAE：0.13676
2021-01-03 02:41:47.459335 Training: [28 epoch,  10 batch] loss: 4.17990, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:42:35.850782 Training: [28 epoch,  20 batch] loss: 4.15079, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:43:23.644944 Training: [28 epoch,  30 batch] loss: 4.15434, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:44:11.242897 Training: [28 epoch,  40 batch] loss: 4.12214, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:44:58.443495 Training: [28 epoch,  50 batch] loss: 4.09150, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:45:45.324438 Training: [28 epoch,  60 batch] loss: 4.03754, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:46:33.028305 Training: [28 epoch,  70 batch] loss: 4.05346, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:47:21.192703 Training: [28 epoch,  80 batch] loss: 4.04730, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:48:08.844677 Training: [28 epoch,  90 batch] loss: 4.01711, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39193,MAE：0.12109
2021-01-03 02:50:25.995581 Training: [29 epoch,  10 batch] loss: 3.94697, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:51:13.718411 Training: [29 epoch,  20 batch] loss: 3.92632, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:52:00.812300 Training: [29 epoch,  30 batch] loss: 3.89683, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:52:48.137942 Training: [29 epoch,  40 batch] loss: 3.87796, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:53:34.719496 Training: [29 epoch,  50 batch] loss: 3.83179, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:54:22.156285 Training: [29 epoch,  60 batch] loss: 3.83865, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:55:09.945416 Training: [29 epoch,  70 batch] loss: 3.80971, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:55:58.199807 Training: [29 epoch,  80 batch] loss: 3.80462, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:56:45.821345 Training: [29 epoch,  90 batch] loss: 3.77300, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.38846,MAE：0.13207
2021-01-03 02:59:02.501449 Training: [30 epoch,  10 batch] loss: 3.71687, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 02:59:49.527203 Training: [30 epoch,  20 batch] loss: 3.68555, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:00:36.349345 Training: [30 epoch,  30 batch] loss: 3.68031, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:01:23.175810 Training: [30 epoch,  40 batch] loss: 3.66004, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:02:09.852953 Training: [30 epoch,  50 batch] loss: 3.64047, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:02:58.364071 Training: [30 epoch,  60 batch] loss: 3.64687, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:03:47.182231 Training: [30 epoch,  70 batch] loss: 3.58582, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:04:36.039596 Training: [30 epoch,  80 batch] loss: 3.57236, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:05:24.980861 Training: [30 epoch,  90 batch] loss: 3.55198, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39253,MAE：0.11839
2021-01-03 03:07:43.651800 Training: [31 epoch,  10 batch] loss: 3.50374, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:08:31.539762 Training: [31 epoch,  20 batch] loss: 3.49725, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:09:20.024834 Training: [31 epoch,  30 batch] loss: 3.46158, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:10:07.880975 Training: [31 epoch,  40 batch] loss: 3.42294, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:10:55.820274 Training: [31 epoch,  50 batch] loss: 3.44437, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:11:44.406169 Training: [31 epoch,  60 batch] loss: 3.43835, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:12:33.352229 Training: [31 epoch,  70 batch] loss: 3.35937, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:13:22.200104 Training: [31 epoch,  80 batch] loss: 3.41846, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:14:10.635914 Training: [31 epoch,  90 batch] loss: 3.34742, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39053,MAE：0.15560
2021-01-03 03:16:28.400395 Training: [32 epoch,  10 batch] loss: 3.30447, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:17:15.368951 Training: [32 epoch,  20 batch] loss: 3.28548, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:18:01.715548 Training: [32 epoch,  30 batch] loss: 3.26820, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:18:48.312076 Training: [32 epoch,  40 batch] loss: 3.27836, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:19:35.277862 Training: [32 epoch,  50 batch] loss: 3.24980, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:20:23.179749 Training: [32 epoch,  60 batch] loss: 3.18636, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:21:11.143999 Training: [32 epoch,  70 batch] loss: 3.24929, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:21:58.817013 Training: [32 epoch,  80 batch] loss: 3.17165, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:22:46.605224 Training: [32 epoch,  90 batch] loss: 3.16768, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39154,MAE：0.12654
2021-01-03 03:25:03.724016 Training: [33 epoch,  10 batch] loss: 3.15259, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:25:50.374869 Training: [33 epoch,  20 batch] loss: 3.11724, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:26:36.503077 Training: [33 epoch,  30 batch] loss: 3.07252, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:27:23.222510 Training: [33 epoch,  40 batch] loss: 3.05407, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:28:11.527264 Training: [33 epoch,  50 batch] loss: 3.06836, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:28:59.187908 Training: [33 epoch,  60 batch] loss: 3.06446, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:29:47.701235 Training: [33 epoch,  70 batch] loss: 3.02132, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:30:35.325303 Training: [33 epoch,  80 batch] loss: 2.99693, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:31:23.871235 Training: [33 epoch,  90 batch] loss: 2.99989, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39015,MAE：0.14133
2021-01-03 03:33:40.717148 Training: [34 epoch,  10 batch] loss: 2.93442, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:34:27.469847 Training: [34 epoch,  20 batch] loss: 2.95047, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:35:13.986770 Training: [34 epoch,  30 batch] loss: 2.91985, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:36:01.023731 Training: [34 epoch,  40 batch] loss: 2.89979, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:36:49.156005 Training: [34 epoch,  50 batch] loss: 2.89382, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:37:36.945082 Training: [34 epoch,  60 batch] loss: 2.88699, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:38:25.379249 Training: [34 epoch,  70 batch] loss: 2.86446, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:39:13.145157 Training: [34 epoch,  80 batch] loss: 2.80792, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:40:01.711990 Training: [34 epoch,  90 batch] loss: 2.86985, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39630,MAE：0.10299
2021-01-03 03:42:18.487580 Training: [35 epoch,  10 batch] loss: 2.78567, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:43:04.849485 Training: [35 epoch,  20 batch] loss: 2.77727, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:43:51.254365 Training: [35 epoch,  30 batch] loss: 2.76193, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:44:38.860830 Training: [35 epoch,  40 batch] loss: 2.75366, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:45:26.945160 Training: [35 epoch,  50 batch] loss: 2.78450, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:46:14.852140 Training: [35 epoch,  60 batch] loss: 2.71997, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:47:02.825492 Training: [35 epoch,  70 batch] loss: 2.69817, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:47:51.268584 Training: [35 epoch,  80 batch] loss: 2.66459, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:48:38.994661 Training: [35 epoch,  90 batch] loss: 2.66357, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39386,MAE：0.20485
2021-01-03 03:50:55.928755 Training: [36 epoch,  10 batch] loss: 2.63035, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:51:42.121690 Training: [36 epoch,  20 batch] loss: 2.62780, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:52:15.391780 Training: [36 epoch,  30 batch] loss: 2.60587, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:52:44.841476 Training: [36 epoch,  40 batch] loss: 2.62522, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:53:14.332760 Training: [36 epoch,  50 batch] loss: 2.59204, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:53:43.918255 Training: [36 epoch,  60 batch] loss: 2.56630, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:54:13.620059 Training: [36 epoch,  70 batch] loss: 2.54158, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:54:43.332942 Training: [36 epoch,  80 batch] loss: 2.57588, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:55:13.108456 Training: [36 epoch,  90 batch] loss: 2.52594, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39224,MAE：0.19812
2021-01-03 03:56:30.288439 Training: [37 epoch,  10 batch] loss: 2.49958, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:56:59.142031 Training: [37 epoch,  20 batch] loss: 2.49797, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:57:28.253686 Training: [37 epoch,  30 batch] loss: 2.47291, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:57:57.629740 Training: [37 epoch,  40 batch] loss: 2.45124, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:58:27.162580 Training: [37 epoch,  50 batch] loss: 2.43925, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:58:56.717250 Training: [37 epoch,  60 batch] loss: 2.43487, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:59:26.465083 Training: [37 epoch,  70 batch] loss: 2.44998, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 03:59:56.213742 Training: [37 epoch,  80 batch] loss: 2.40061, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:00:26.045068 Training: [37 epoch,  90 batch] loss: 2.42076, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39086,MAE：0.11588
2021-01-03 04:01:45.496534 Training: [38 epoch,  10 batch] loss: 2.38058, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:02:15.320045 Training: [38 epoch,  20 batch] loss: 2.36185, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:02:45.204563 Training: [38 epoch,  30 batch] loss: 2.33059, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:03:15.121266 Training: [38 epoch,  40 batch] loss: 2.37098, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:03:45.076735 Training: [38 epoch,  50 batch] loss: 2.30680, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:04:15.130898 Training: [38 epoch,  60 batch] loss: 2.31932, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:04:45.317808 Training: [38 epoch,  70 batch] loss: 2.29127, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:05:15.504457 Training: [38 epoch,  80 batch] loss: 2.29174, the best RMSE/MAE: 0.38711 / 0.13676
2021-01-03 04:05:45.726047 Training: [38 epoch,  90 batch] loss: 2.28477, the best RMSE/MAE: 0.38711 / 0.13676
<Test> RMSE：0.39141,MAE：0.11058
The best RMSE/MAE：0.38711/0.13676
